<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.9.17">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-09-17">

<title>If Anyone Builds It, Does Everyone Die? – johnowhitaker.dev</title>
<style>
/* Default styles provided by pandoc.
** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
*/
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark-dcd9e8c573452b099f942d625f6f4e16.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-7808d31791051cc8da4c7717a821f40e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="If Anyone Builds It, Does Everyone Die? – johnowhitaker.dev">
<meta name="twitter:description" content="">
<meta name="twitter:creator" content="@johnowhitaker">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../all.html"> 
<span class="menu-text">Everything Feed</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-more" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">More</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-more">    
        <li>
    <a class="dropdown-item" href="../tsbabn.html">
 <span class="dropdown-text">TSBABN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../resume.html">
 <span class="dropdown-text">Resume: Jonathan Whitaker</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../books.html">
 <span class="dropdown-text">Book Recommendations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../blogs.html">
 <span class="dropdown-text">Blog Recommendations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../yt.html">
 <span class="dropdown-text">YouTube Channel Recommendations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../appearances.html">
 <span class="dropdown-text">Appearances</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../things.html">
 <span class="dropdown-text">Thing Recommendations</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://johnowhitaker.dev/index.xml"><i class="bi bi-rss" role="img">
</i> 
 <span class="dropdown-text">RSS Feed</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../now.html">
 <span class="dropdown-text">Now</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#original-essay" id="toc-original-essay" class="nav-link active" data-scroll-target="#original-essay">Original Essay</a>
  <ul class="collapse">
  <li><a href="#a-quick-summary-of-the-book" id="toc-a-quick-summary-of-the-book" class="nav-link" data-scroll-target="#a-quick-summary-of-the-book">A Quick Summary of the Book</a></li>
  <li><a href="#so-are-we-doomed" id="toc-so-are-we-doomed" class="nav-link" data-scroll-target="#so-are-we-doomed">So Are We Doomed?</a></li>
  <li><a href="#what-are-they-thinking-interpretability" id="toc-what-are-they-thinking-interpretability" class="nav-link" data-scroll-target="#what-are-they-thinking-interpretability">What Are They Thinking? (Interpretability)</a></li>
  <li><a href="#what-do-they-want-alignment" id="toc-what-do-they-want-alignment" class="nav-link" data-scroll-target="#what-do-they-want-alignment">What Do They Want (Alignment)</a></li>
  <li><a href="#what-are-we-building-technology-now" id="toc-what-are-we-building-technology-now" class="nav-link" data-scroll-target="#what-are-we-building-technology-now">What Are We Building? (Technology now)</a></li>
  <li><a href="#what-will-we-build-next" id="toc-what-will-we-build-next" class="nav-link" data-scroll-target="#what-will-we-build-next">What Will We Build Next</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a>
  <ul class="collapse">
  <li><a href="#ps-other-worries" id="toc-ps-other-worries" class="nav-link" data-scroll-target="#ps-other-worries">PS: Other Worries</a></li>
  </ul></li>
  <li><a href="#other-takes" id="toc-other-takes" class="nav-link" data-scroll-target="#other-takes">Other Takes</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">If Anyone Builds It, Does Everyone Die?</h1>
  <div class="quarto-categories">
    <div class="quarto-category">essays</div>
  </div>
  </div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 17, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Update 2025-09-23: I wrote out some quick thoughts right after reading the book, mostly to share with a few specific people. That original post is included below, but something about it feels lacking. There are so many takes in this space, many vastly incompatible with eachother. I’m adding a section at the end with reactions/takes from others I found interesting, and I encourage you to form your own opinions. ¯_(ツ)_/¯</p>
<section id="original-essay" class="level1">
<h1>Original Essay</h1>
<p>I’ve just finished IABIED. My father-in-law David also pre-ordered it, and his reaction is, predictably, a fair bit of hopelessness. Corporations are racing ahead chasing the money, that doesn’t seem like it’s going to stop. The book’s only recommendations are to shut down, with threat of nuclear bombs, any un-monitored GPU cluster with &gt;8 GPUs and ban new AI research. This seems unlikely to happen. What can we do? David’s youngest son is expecting, and talked yesterday about a 529 college savings plan. “What can I say to him? Like you can’t say this to a young father-to-be but there’s no way his kid is going to go to college!”. This essay is my attempt at some sense-making.</p>
<section id="a-quick-summary-of-the-book" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-summary-of-the-book">A Quick Summary of the Book</h2>
<p>Part 1:</p>
<ul>
<li>Humans do well at intelligence (steering and predicting) which gives us immense power</li>
<li>LLMs and other AIs are ‘grown not crafted’</li>
<li>Training for an objective leads to acting like you ‘want’ something - e.g.&nbsp;chess AI ‘wants’ to protect its queen</li>
<li>Optimizing to an objective can lead to weird ‘wants’ - e.g.&nbsp;peacock feathers, non-reproductive sex in the case of evolution.</li>
<li>An ASI’s “wants” are likely to be extremely weird and not good for humans</li>
<li>We’d lose in a fight against ASI that wanted something different to us</li>
</ul>
<p>Part 2: One specific, fictional example, to help people visualize how a bad scenario could go down, without just waving hands at “some magic-like super-smart phenomenon we don’t understand hits us out of left field”. (review: There are other, better tales in this genre.)</p>
<p>Part 3: There are lots of pieces that make this problem very hard: we only get one try, takeoff might happen fast, this is a relatively new science (he compares it to alchemy), people are racing towards it anyway, believing it’s better they try first, people are afraid to be alarmist, it would take pretty drastic measures to stop it.</p>
<p>I think it’s well-written, and will reach lots of people with the ideas that Eliezer and co have been worried about for decades, ideas which are suddenly feeling a lot more pressing in light of the rapid AI advances of today. A lot of the steps in the argument are hard to argue with. They clearly took great care to communicate technical ideas like reasoning model training in a way that is mostly accessible to non-tech folks.</p>
</section>
<section id="so-are-we-doomed" class="level2">
<h2 class="anchored" data-anchor-id="so-are-we-doomed">So Are We Doomed?</h2>
<p>There are a few key claims in the book where I find room for more hope than the authors:</p>
<ol type="1">
<li>Interpretability: We don’t know anything about what is happening inside these networks</li>
<li>Alignment: The ‘wants’ that result from AI training will almost certainly be completely alien and extremely unlikely to be good for humans</li>
<li>Technology: We are building towards ASI of the kind he describes</li>
</ol>
<p>Let’s dig into each of those one by one</p>
</section>
<section id="what-are-they-thinking-interpretability" class="level2">
<h2 class="anchored" data-anchor-id="what-are-they-thinking-interpretability">What Are They Thinking? (Interpretability)</h2>
<p>Clearly we’re a ways from full understanding, but I think they undersell the interpretability field - we’re making great strides, and so far seem to be quite lucky in terms of how far even ‘simple’ techniques like linear probes can go in terms of understanding behaviours or finding worrying patterns. There are some deep laws of intelligence lurking - pockets of computational reducibility that we can exploit for understanding, as Wolfram might say.</p>
<p>As a bonus, reasoning models currently output their ‘chain of thought’ in ~English, and the labs are very aware that this is a fragile and beneficial state we shouldn’t throw away. There’s a <a href="https://arxiv.org/pdf/2507.11473">lot of agreement that this is worth preserving</a>, and a lot of work being done to ensure this remains a useful way to keep a window on these ‘thinking’ models to catch early signs of unwanted behaviour.</p>
<p>AI-assisted interpretability is also not nearly as doomed as they make out. The authors tend to treat intelligence as one singular, scalar value - a dumber model cannot hope to outwit a smarter model, and smarter models will be more dangerous. I disagree, and think (for example) we could build models focused on understanding what other models are ‘reasoning’ about, without needing to give the interpreter model any tools or agency.</p>
</section>
<section id="what-do-they-want-alignment" class="level2">
<h2 class="anchored" data-anchor-id="what-do-they-want-alignment">What Do They Want (Alignment)</h2>
<p>Now onto alignment, which the authors frame as this near-impossible task. After all, we can’t write down exactly what the ‘human values’ we want are, and even if we did, optimization towards those might instead lead to all sorts of warped and crazy actual ‘wants’, like a desire for smiles, which in turn might lead to engineering a smiling virus… If we were building these AIs from scratch, from some sort of self-assembling game-playing logic machines, which followed our specified rules like a malicious djinn following the letter of the wish in an old story, I’d agree we’re in trouble.</p>
<p>But we’re actually in a super lucky spot, thanks to how the tech is developing in practice. See, to get around the difficulty of carefully writing out the specific objective we want, it would be great if we could start the models out with some incredible, general distillation of human behaviours and values, some trove of interactions and information that gave them a more robust understanding of humans… something like the internet?</p>
<p>If you ask an LLM today “should I go camping in this park where there’s a wildfire warning”, the LLM will advise against it. This is not because the maker trained it to advise against hiking in wildfires, or because they carefully told it what to do in that situation. Instead, they trained it to advise against drinking bleach, and to recommend healthy habits, and to help out with programming and math problems. To get better at doing so, the LLM didn’t have to start from scratch. Instead, it built on the rich features and concepts it learnt during pre-training, including the convenient shortcut ideas that make all the specific examples more likely: be helpful, be good. These bleed into other cases, giving us AI’s that (in general) act ‘good’ in a huge number of situations.</p>
<p>This is great news! The model doesn’t need to learn what good is from a list of rules, it can build on the priors introduced by trillions of tokens of human-written text. This is a far nicer situation than we could have hoped for! And not one the authors envisioned when they were dreaming of uncaring evil optimizers back in the 2000s.</p>
<p>This isn’t to say that we get off easy. The trend is away from chatbots and more and more towards AI ‘agents’ that learn from reinforcement learning, not from human text. It’s possible that doing more and more RL will push the models further from the base they learn in pre-training, and could lead to weirder and more scary objectives - indeed, we already see a little bit of this with so-called “reward hacking”, where models are instructed to solve challenges fairly but learn over time that they can be rewarded for finding clever hacks to fool the tests they’re being trained on. This is an active area of research.</p>
<p>There’s also a worry here in terms of intentional mis-use. If a model understands good and bad, could someone train it to be bad intentionally? Alas, it seems so. In fact, training it to be bad in one area (e.g.&nbsp;writing insecure, hackable code) can <a href="https://arxiv.org/abs/2502.17424">lead to a model that also acts racist and mean</a>! It might be almost as simple as flipping a plus sign to a minus sign deep in the model internals - something that will get more worrying as capabilities continue to advance.</p>
</section>
<section id="what-are-we-building-technology-now" class="level2">
<h2 class="anchored" data-anchor-id="what-are-we-building-technology-now">What Are We Building? (Technology now)</h2>
<p>Building on the past two sections, the situation at present is nicer than the book would have us believe. Current systems are trained on human data, and have a more robust understanding of ‘helpful’, ‘honest’ and ‘harmless’ than early theorists could have hoped for.</p>
<p>Furthermore, they’re extremely incentivised to keep getting better in this regard. AI companies make money if their AI behaves well! Training involves taking in text and producing more text, and deployment looks the same. While some models have some ‘situational awareness’ to know when they’re deployed, we can still:</p>
<ul>
<li>inspect the text they’re producing, including with other ‘monitor’ models that can detect early signs of bad behaviour</li>
<li>pause the process at any time (the book uses lots of rhetoric about 10000X faster thinking to show danger, but current models have no way to know what speed they’re going or if we’re carefully inspecting their inner activations)</li>
<li>catch signs of unintended behaviour in testing or early deployment, and roll back to earlier models if these are too bad (e.g.&nbsp;Grok’s “MechaHitler” arc was rapidly cut short)</li>
</ul>
<p>We’re a far cry from the picture in the book’s Part 2, where an AI entity schemes to itself for tens of thousands of GPU hours in inscrutable self-invented language, and then plans and executes it’s evil plan over many instances of itself over many months.</p>
<p>People are actively working on safety in many different ways, and again labs have incentives to keep building systems that reliably do what we want. There’s a lot of pressure against anything that develops ‘misaligned’ wants.</p>
</section>
<section id="what-will-we-build-next" class="level2">
<h2 class="anchored" data-anchor-id="what-will-we-build-next">What Will We Build Next</h2>
<p>AI is being built by people, in corporations. Their motivations vary, but most are genuinely pursuing ways to add value: helping people do more, solving scientific challenges, creating more and more value for other corporations and society.</p>
<p>In one sense, there’s a single direction we go from here: better. AI models will continue to do better and better on various objectives we set. But when you look closer, there are different paths we could take, determined by economic incentives and the choices of those leading the charge.</p>
<p>One dimension I wish we could tweak is the move towards more and more ‘agentic’ AI. This is something we care a lot about at my company - AI as human augmentation, with human driving, is an immensely useful and empowering tool. But too many view the real use as human replacement - a virtual employee you can delegate work to. Besides de-valuing humans, this is a direction that doesn’t help the safety side! Yoshua Bengio prominently believes that pushing for non-agentic AI is key to maintaining safety as we push capabilities forward, and I agree.</p>
</section>
<section id="conclusions" class="level2">
<h2 class="anchored" data-anchor-id="conclusions">Conclusions</h2>
<p>I am much more hopeful than the authors that we will be able to keep solving problems and moving forward, building AI that helps humans do more, pushing back the frontiers of knowledge without destroying the world.</p>
<p>It’s worth thinking carefully about the risks. It’s good that people are thinking about policy that could slow down the chance of bad outcomes. I hope that concensus shifts away from the mad rush into agentic human replacements, and that we collectively spend some time dreaming about what types of future we’d like to usher in. I hope the ‘race’ dynamic doesn’t cause megacorps to throw out some of their ideals.</p>
<p>I can’t claim certainty. But I don’t think everyone is going to die. Especially if we proceed carefully and push towards human-centered AI tools, and away from unchecked agency.</p>
<p>I have no idea what the future looks like. It probably involves a lot of change. But I hope this little brain-dump helps you keep a spark of hope that, just maybe, that kiddo will have a cooler college experience than any of us old cynics could dream of :)</p>
<section id="ps-other-worries" class="level3">
<h3 class="anchored" data-anchor-id="ps-other-worries">PS: Other Worries</h3>
<p>This post focused on ‘ASI’ specifically, the focus of IABIED. There are other reasons to be concerned, some that come up if we do build powerful AI and some that stem from attempts to avoid that. The book mostly ignores these - fair, considering that if you believe the books titular conclusion then all of these are, by comparison, small fry. Still, including them here for completeness and reference until I decide to edit this out for brevity :)</p>
<p>If we try not to build it:</p>
<ul>
<li><p>Authoritarian surveillance state: The book advocates for extreme oversight measures, to ensure that nobody builds ASI. Implementing these would require extreme surveilance and power in the hands of the governments, which could lead to it’s own bad outcomes.</p></li>
<li><p>Concentration of Power: Making (incredibly valuable and useful) AI something that only a few have direct control over is a recipe for concentration of power and wealth, leading to bad outcomes of a different kind</p></li>
</ul>
<p>If we build it safely:</p>
<ul>
<li><p>Misuse: someone might still be able to transform safe, powerful AI into something harmful and unsafe, e.g.&nbsp;by re-training it to agentically pursue damaging cyberattacks or using it to design bioweapons</p></li>
<li><p>Gradual disempowerment: if we become too dependant on AI, we might slowly lose our skills and bargaining power, leaving the bulk of the population with less hold on how they are treated and what their lives look like going forward.</p></li>
<li><p>Misaligned incentives between corporations and humans, as is already happening with addictive scrolling content like tiktok, and which could get far worse as companies push ‘relational’ AI further while optimizing for engagement or profit.</p></li>
</ul>
</section>
</section>
<section id="other-takes" class="level2">
<h2 class="anchored" data-anchor-id="other-takes">Other Takes</h2>
<p>I have been keeping an eye out for other takes on this book. Here are some:</p>
<p>Jeremy Howard: Calling for extreme measures to suppress research will concentrate power in the hands of a few is a very bad idea. <a href="https://x.com/jeremyphoward/status/1969541019437056362">Who is suggesting this</a>? <a href="https://x.com/jeremyphoward/status/1968887354413748270">This could lock in the status quo of power</a>. See also <a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">AI Safety and the Age of Dislightenment</a>.</p>
<p><a href="https://x.com/mimi10v3/status/1968731768212881707">A summary of the book on Twitter that seems accurate and a good way to get the jist without reading it</a>, posted after their <a href="https://x.com/mimi10v3/status/1967839789375426876">original tweet</a> containing just the first few words of each chapter (usually part of a parable) was interpreted as a hilarious diss on Yud</p>
<p><a href="https://www.vox.com/future-perfect/461680/if-anyone-builds-it-yudkowsky-soares-ai-risk">A decent vox article</a> on the book, capturing the conflicting ideals and that this is a clash of worldviews.</p>
<p><a href="https://x.com/jd_pressman/status/1968847166316625989">JDP’s review</a></p>
<p><a href="https://x.com/willmacaskill/status/1968759901620146427">William MacAskill’s review</a> - who found the book disappointing</p>
<p><a href="https://x.com/teortaxesTex/status/1968856896107888656">Teortaxes’ review draft</a> - a critical take that this book is counterproductive and will damage reasonable AI risk discourse</p>
<p>Lots of other immediate dismissals, lots of other ‘this is the most important book ever’ takes. Is it good that this will get everyone talking, or bad that it will get everyone taking based on flawed arguments, or good because we’re all going to have to keep repeating the title which states the premise which might skew public policy, or bad because actually we’re not on track to build ASI at all and this might make it harder to make nice medical AI and stuff, or good because… like I said - I encourage you to form your own opinions ¯_(ツ)_/¯</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/johnowhitaker\.dev");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>