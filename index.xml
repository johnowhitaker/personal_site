<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>johnowhitaker.dev</title>
<link>https://johnowhitaker.dev/</link>
<atom:link href="https://johnowhitaker.dev/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.17</generator>
<lastBuildDate>Sun, 20 Jul 2025 00:00:00 GMT</lastBuildDate>
<item>
  <title>1Hz Dropper</title>
  <link>https://johnowhitaker.dev/mini-hw-projects/dropper.html</link>
  <description><![CDATA[ 




<p>Inspired by the <a href="https://hackaday.io/contest/203248-one-hertz-challenge">Hackaday One Hertz challenge</a>, I used a solenoid scavenged from an old printer to drop dye into water once a second, making for a pretty (if short-lived) time-keeping device. Video of it in action <a href="https://x.com/johnowhitaker/status/1947072110817644573">here</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/drop1.png" class="img-fluid figure-img"></p>
<figcaption>The setup</figcaption>
</figure>
</div>
<p>The top of the syringe plunger makes a loose seal, and is popped out briefly by a pulse of the solenoid. This let’s a bit of air in, which allows a small drop to form at the bottom of the syringe (with needle removed the hole is perfectly sized for this). The tap and small pressure bump as the plunger top returns to it’s resting position knocks the drop off, giving a perfect* 1Hz drip.</p>
<p>I ‘filmed’ a full syringe worth of beautiful footage - about a minute of fresh ink dropping into clear water, with it all set up in a stand. The alignment was perfect, single drops each time. I went to ‘stop’ the recording and, womp womp, it hadn’t started. I promptly spilled some dye, knocked things over and messed it all up attempting several re-takes before deciding I’m content with the current shorter clip. Each failing tick spewing mess on my bench felt like a metaphor for time passing that I did not enjoy :D</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/drop3.png" class="img-fluid figure-img"></p>
<figcaption>a few drops in</figcaption>
</figure>
</div>
<p>Parts used are a Raspberry Pi Pico RP2040 microcontroller in the “Kitronik Simply Robotics Motor Driver Board for Raspberry Pi Pico” board. The code is attached, pulsing the solenoid (connected to the MOTOR_0 output of the board) for a set time each second. Code dumped on the <a href="https://hackaday.io/project/203568-1hz-dropper">hackaday io project</a> (but it’s basically just blinky, one-shotted by o3, plus stolen library code).</p>
<p>Not my finest work in any respect but it was a fun diversion from Sunday tidying.</p>



 ]]></description>
  <category>mini-hw-projects</category>
  <guid>https://johnowhitaker.dev/mini-hw-projects/dropper.html</guid>
  <pubDate>Sun, 20 Jul 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>SVG generation as a microcosm of generative AI research trends</title>
  <link>https://johnowhitaker.dev/misc/svg.html</link>
  <description><![CDATA[ 




<section id="svg-generation-as-a-microcosm-of-generative-ai-research-trends" class="level3">
<h3 class="anchored" data-anchor-id="svg-generation-as-a-microcosm-of-generative-ai-research-trends">SVG generation as a microcosm of generative AI research trends</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/vnD4PlGvOg4" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>I noticed that my open tabs told a fun story - one that’s been repeated for various niches in generative AI, and that shows the current trend. Hopefully fun and maybe gives you ideas to apply to some other forgotten niche :)</p>
<p>References: - DIffVG: https://github.com/BachiLi/diffvg - Iconshop: https://icon-shop.github.io/ - OmniSVG: https://omnisvg.github.io/ - Recraft-SVG: https://replicate.com/recraft-ai/recraft-v3-svg - Rendering-Aware Reinforcement Learning for Vector Graphics Generation: https://arxiv.org/abs/2505.20793</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/svg.html</guid>
  <pubDate>Fri, 11 Jul 2025 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/svg.png" medium="image" type="image/png" height="123" width="144"/>
</item>
<item>
  <title>The Diffusion Duality (Paper reading + discussion)</title>
  <link>https://johnowhitaker.dev/misc/dllm2.html</link>
  <description><![CDATA[ 




<section id="the-diffusion-duality-paper-reading-discussion" class="level3">
<h3 class="anchored" data-anchor-id="the-diffusion-duality-paper-reading-discussion">The Diffusion Duality (Paper reading + discussion)</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/o_ISAlSMoJQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Hmm, as I recorded this I realize I don’t understand the specifics of this paper - since I can’t explain them well! So apologies for the hand-waviness of this one. I hope it’s still vaguely useful, I might try to make a better one in a bit if I notice too many glaring errors on reflection :) Paper page: https://s-sahoo.com/duo/</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/dllm2.html</guid>
  <pubDate>Thu, 19 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/dllm2.png" medium="image" type="image/png" height="68" width="144"/>
</item>
<item>
  <title>Diffusion Language Models - Turning ModernBERT into an instruct-tuned Diffusion LLM</title>
  <link>https://johnowhitaker.dev/misc/dllm1.html</link>
  <description><![CDATA[ 




<section id="diffusion-language-models---turning-modernbert-into-an-instruct-tuned-diffusion-llm" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-language-models---turning-modernbert-into-an-instruct-tuned-diffusion-llm">Diffusion Language Models - Turning ModernBERT into an instruct-tuned Diffusion LLM</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/Ds_cTclxV2o" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Inference notebook: https://colab.research.google.com/drive/1hMV0OBpmJL7L5yIEtkeeUz-7rB1buFmg?usp=sharing Training notebook: https://colab.research.google.com/drive/1D82ULU5dUyJKPnj2oUxtfJeWTB1sVds_?usp=sharing Model on HF: https://huggingface.co/johnowhitaker/modernbert-diffusion/blob/main/README.md LLaDA paper (‘Large Language Diffusion Models’): https://arxiv.org/pdf/2502.09992</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/dllm1.html</guid>
  <pubDate>Mon, 16 Jun 2025 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/dllm1.png" medium="image" type="image/png" height="73" width="144"/>
</item>
<item>
  <title>FastHTML App in Claude Code</title>
  <link>https://johnowhitaker.dev/misc/claude_code_fasthtml.html</link>
  <description><![CDATA[ 




<section id="fasthtml-app-in-claude-code" class="level3">
<h3 class="anchored" data-anchor-id="fasthtml-app-in-claude-code">FastHTML App in Claude Code</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/0TkS_Hulyus" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Building out a FastHTML application with Claude code, sharing some tips for finding good context and keeping control. NB: I think I left Claude code set to use Opus - hence the expensive session. Also, since recording this, Isaac (creator of MonsterUI) shared his cursorrules, which are another great ref for providing FastHTML context to AI: https://github.com/ai-evals-course/isaac-fasthtml-workshop/blob/main/.cursor/rules/fasthtml.mdc</p>
<p>Code from this video: https://github.com/johnowhitaker/fasthtml-claude-code-demo</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/claude_code_fasthtml.html</guid>
  <pubDate>Tue, 27 May 2025 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/ccfh.png" medium="image" type="image/png" height="102" width="144"/>
</item>
<item>
  <title>Crystals, Crokinole, Cellular Automata (small updates)</title>
  <link>https://johnowhitaker.dev/posts/ccc.html</link>
  <description><![CDATA[ 




<p>I’ve been kinda busy, and a few small projects have gone undocumented, so I’m sharing stubs here in case I don’t get to them later.</p>
<section id="crystal-growth-shader" class="level3">
<h3 class="anchored" data-anchor-id="crystal-growth-shader">Crystal Growth Shader</h3>
<p><img src="https://johnowhitaker.dev/posts/images/ccc_crystals.png" class="img-fluid"></p>
<p>I made <a href="https://www.shadertoy.com/view/wXSSzK">this shader</a> after thinking how easy it would be to simulate crystal growth, with crystal formation depleting concentration but making it more likely more crystal grows there, and everything represented as channels in an image for shaders. AI-assisted quick prototype came together quickly, and I swapped in the Michel-Levy colors from my <a href="https://www.shadertoy.com/view/Wcf3R8">bubble shader</a> to make the image above. Lots of fun tweaking constants.</p>
<p><img src="https://johnowhitaker.dev/posts/images/ccc_bubbles.png" class="img-fluid"></p>
<p>(THe bubble shader - don’t know if I shared before)</p>
</section>
<section id="travel-crokinole" class="level2">
<h2 class="anchored" data-anchor-id="travel-crokinole">Travel Crokinole</h2>
<p><img src="https://johnowhitaker.dev/posts/images/ccc_crokinole.png" class="img-fluid"></p>
<p>I like Crokinole and so does my friend Jeremy Howard - but when we both travelled to Singapore a normal board wouldn’t fit in my suitcase so I had this travel set cut by the fantastic SendCutSend which bolts together. I tapped holes for bolts to act as the pegs and used cut aluminium disks as the pucks, with some speed powder (basically polenta as far as I can tell) they glide beautifully. With nothing the flat surfaces tend to slow things once there’s no air cushion. I tried magnets as an inductive braking edge barrier but they were too weak - not sad, I now have lots of small magnets which I often need :) The game was a success, I got lots of Answer.AI friends hooked :)</p>
</section>
<section id="ca-on-a-donut" class="level2">
<h2 class="anchored" data-anchor-id="ca-on-a-donut">CA on a donut</h2>
<p><img src="https://johnowhitaker.dev/posts/images/ccc_ca.png" class="img-fluid"></p>
<p>Idle demoscene ThreeJS project while I read a Wolfram article, <a href="https://chatgpt.com/canvas/shared/682e225a55b08191b70e57a1070985c4">here’s the code</a>. Made after playing with a rule with horizontal wrapping and thinking how cool it would look on a torus. modern AI models like o3 are incredible - it got it wrong but then did lots of python playing to figure out what to change!!</p>
</section>
<section id="edison-cylinder-wip" class="level2">
<h2 class="anchored" data-anchor-id="edison-cylinder-wip">Edison cylinder (WIP)</h2>
<p><img src="https://johnowhitaker.dev/posts/images/ccc_edison.png" class="img-fluid"></p>
<p>I bought an edison cylinder for $5 - amazing piece of history. These hold ~2 minutes of audio. I tried various ways of spinning it (pictured is too fast, no pics of most successful) and the closest I got to audio was dragging a reflective thingee with a piece of wire out the edge over the grooves and then bounding a laser off of it, then turning the light intensity into audio with an existing circuit I have lying around, but it doesnt track the grooves well at all. Might not bother, I hoped this was a wax cylinder and the main goal was to make a music-themed candle for my FIL but alas it’s vinyl haha.</p>


</section>

 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/posts/ccc.html</guid>
  <pubDate>Wed, 21 May 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ICLR 2025</title>
  <link>https://johnowhitaker.dev/posts/ICLR_2025.html</link>
  <description><![CDATA[ 




<p><img src="https://johnowhitaker.dev/posts/images/si.png" class="img-fluid"></p>
<p>I got to attend ICLR 2025 and spend a week in Singapore with my Answer.AI colleagues. This post is some disjointed thoughts on the whole experience.</p>
<section id="social-is-top" class="level2">
<h2 class="anchored" data-anchor-id="social-is-top">Social is top</h2>
<p>The best part of conferences is always meeting people. The venue setup was disappointingly short on affordances for the ‘hallway track’ but Jeremy managed to claim a couch in one of the cafe’s and hold court for various friends old and new :) And of course there were the usual evening dinners + events. This time around though I mostly spent time hanging out with the answer team, heading out to different parts of town for food and coding by the pool. Other hightlights were meeting a collaborator in person for the first time and getting taken out on a hike by a local fast.ai alum who knew all the good spots. If you get a chance to go to one of these conferences, my top recommendation is to focus on meeting friendly people and spending time chatting vs attending talks and things - far more interesting IMO! Poster sessions are a good way to find people with shared research interests. Pretend to be an extrovert - it’s only a few days!</p>
<p>PS, just after I shared this I found these <a href="https://kamathematics.wordpress.com/2025/05/01/tips-on-how-to-connect-at-academic-conferences/">tips on how to connect at academic conferences</a> which are great! Go read that and follow them :)</p>
</section>
<section id="safety-interpretability-are-in-the-air" class="level2">
<h2 class="anchored" data-anchor-id="safety-interpretability-are-in-the-air">Safety + Interpretability are in the air</h2>
<p>I’m trying to think what trends jumped out at me. ‘Agents’ of course, with some interesting stuff and lots of… not haha. The other main trend I noticed was safety/interpretability stuff, with a sense of ‘this is something academics can work on + contribute to without tons of compute’. There was a keynote from an OpenAI person making the case for why it’s important + good to work on even if you <strong>don’t</strong> think AI will soon kill everyone. My friend Stanislav Fort had a fun paper on ‘scaling laws for adversarial attacks’ and as always tons of other neat ideas in progress. Various papers trying to make AIs say naughty things or not say them. And in the airport I met Yoshua Bengio who’d been working very hard trying to convince people it’s a bad idea to be chasing creating smart things with agency. I admire his efforts, and tend to agree that making ‘tool AI’ vs ‘agent AI’ is a good idea! And I liked the way he phrased a take on doomish stuff: “The correct response to [someone asking me about AI2027] is to say that lots of experts agree we can’t casually dismiss it” - in other words, you may think specific takes are wrong or that fears are overblown but ‘nobody takes that seriously’ is not a valid argument. All that said, while I noticed this safety talk around it wasn’t like it was the bulk of the discussion and I’d wager most attendees aren’t thinking much about it.</p>
</section>
<section id="vibe-researching-is-fun" class="level2">
<h2 class="anchored" data-anchor-id="vibe-researching-is-fun">Vibe-Researching is fun</h2>
<p><img src="https://johnowhitaker.dev/posts/images/k_vs_hdim.png" class="img-fluid"></p>
<p>One fun trick I could do at this conference thanks to the latest great models like o3 was whip up quick exploratory demos based on concepts I came across. This is great! Poster sessions and hallway chats often spark ‘it would be cool to compare that to X baseline’, or ‘I want to try this with Y’ - but by the end of a busy conference lots of ideas are forgotten. It was super cool to voice dictate an idea to o3, copy some code into Google Colab and have a minimal toy demo to try out. I did this for a few things that came up, including running one experiment entirely on my phone to make some plots related to a paper I saw, and the ones I did this for are now lodged in my brain enough that I’ll probably follow through on the experiments I want to run now. Wild that someone can say ‘We could probably train a model to go directly from CLIP penultimate hidden states back to an image’ and a few minutes later I can have a toy model training that does just that! The AIs still aren’t excellent for AI research but boy are they better than they used to be - more than enough for simple baselines and getting going quickly. Anyway - vibe research - can recommend!</p>
</section>
<section id="singapore-is-amazing" class="level2">
<h2 class="anchored" data-anchor-id="singapore-is-amazing">Singapore is amazing</h2>
<p>Singapore was delightful to visit. We stayed in ‘Little India’, an easy MRT ride from Chinatown and close to the marina and various parks and things, making it easy to experience a variety of different cuisines and aspects of the city. I loved how pervasive nature was - greenery bursting from any vailable spot, but somehow contained and managed very neatly. The food was particularly good - so many exciting options with nustling hawker centers full of amazing smells and sights. <a href="https://jeadventure.home.blog/2025/04/29/impressions-of-singapore/">This post</a> we sent to our family has more pics and my bird list - ft 36 new ‘lifers’ for me!</p>
</section>
<section id="i-have-the-best-job" class="level2">
<h2 class="anchored" data-anchor-id="i-have-the-best-job">I Have The Best Job</h2>
<p>Answer.ai is fully remote, and this was the first time most of us had met in person. And yet we all got on extremely well, instantly falling into a happy rhythum of exploring and enjoying the city together then returning to the hotel pool area to code, chat and play games. I made a travel ‘crokinole’ board (link coming soon) and introduced the team - hopefully enough got hooked that by next time we meet I’ll have some competition ;) All this already had me feeling grateful, but what really hammered home how cool this company is was answering (and hearing others answer) the repeated question of ‘so what does answer.ai do?’ that came up in conversation with all the new people we met at the conference. I’m so lucky!</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Anyway, there are my jet-lagged ramblings. If we met there, or you wish we’d met there, or you just want to say hi, reach out! I want to cram in more social with fellow AI nerds for a week or two before I settle back to my quiet and introverted default state :)</p>


</section>

 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/posts/ICLR_2025.html</guid>
  <pubDate>Thu, 01 May 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>SpecID - A Hard Eval for Multimodal LLMs</title>
  <link>https://johnowhitaker.dev/mini-projects/specid.html</link>
  <description><![CDATA[ 




<p>We were talking recently around “What is something you could do through a chat interface that current LLMs can’t?”. One example I give personally is identifying Zimbabwean spider species - a lot of which are ~undocumented* so knowledge on what the living specimens look like is concentrated in a few heads, plus a smattering of posts on Facebook and iNaturalist. Anyway, I thought I should put my money where my mouth is and make an actual eval for this, to see how badly SOTA multi-modal LLMs do. Here are the results:</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/specid_results.png" class="img-fluid"></p>
<p>*Update: gemini-2.5-pro-exp-03-25 gets 57%, the new top score! And Flash 2.5 gets 53% when forcing function calls (previously a bug)</p>
<p>The way I constructed this eval was to pull 100 photos with confirmed “research grade” identifications from my iNaturalist catalog. Each sample gets up to four ‘distractors’ - species names for closely-related organisms in the same ~family/group as the target. Multiple-choice is significantly easier than dredging up the name ‘cold’ (especially since many latin names have clues that match visual aspects of the species). And the quiz is harder for me that it could have been, since I included all species (bugs, birds, plants) not just spiders! Turns out I’m way worse at the latin names outside my comfort zone :D Still - at least at present I have a lead on the best models - just not as much of a one as I expected! It’s stunning that 4o can get more than half of these right.</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/specid_sa.png" class="img-fluid"></p>
<p>For the human baseline, I set up an interface that had me guess myself. It took less than 12 minutes total - here’s the prompt that produced the code: “his is a dataset I made, with an image and up to five candidate species. I want to get a human baseline for this dataset. Can you give me code that, in a Google Colab notebook, shows the picture and five buttons (some will need less, can use”NA” if there are less than 5 options) and has me click which one I think is the right one, before storing my guess and showing the next one? Use ipywidgets, include a way for me to see progress (how many done + left) and score (current correct %).” And <a href="https://github.com/johnowhitaker/specid_eval/blob/main/SpecID_Human_Baseline.ipynb">here’s the result</a>. Try it youself if you dare :)</p>
<p>The spider above by the way? It fell from a tree onto me, and I eventualy figured out what it was (first record for Zimbabwe) thanks to this passage from the 1923 <strong>Araignées du sud de l’Afrique</strong> :“Céphalothorax noir, avec les yeux latéraux antérieurs et postérieurs situés sur des saillies orangées. Chélicères, pièces buccales, sternum, pattes-mâchoires, pattes brun-noir. Abdomen gris-noirâtre foncé.”. There have been a handful of records with nice pics shared since, and it featured on a guide, but this is the kind of obscure digging needed sometimes to ID these things.</p>
<p>Anyway. Make your own evals! Occasionally suprising, easy, fun! Show me! That is all :)</p>
<ul>
<li><a href="https://huggingface.co/datasets/johnowhitaker/specid_eval_jw">Dataset on HuggingFace</a> - let me know if you want me to try a model on a private ~equivalent dataset.</li>
<li><a href="https://github.com/johnowhitaker/specid_eval">Code + preliminary results with a script to run it yourself</a></li>
</ul>
<p>*Although I did write a field guide to the more common ones!</p>



 ]]></description>
  <category>mini-projects</category>
  <guid>https://johnowhitaker.dev/mini-projects/specid.html</guid>
  <pubDate>Sun, 20 Apr 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Extrapolate All The Things - Small-To-Big Extrapolation For LLMs</title>
  <link>https://johnowhitaker.dev/mini-projects/extrapolative_sampling.html</link>
  <description><![CDATA[ 




<p>In a research chat yesterday I spoke about a fun thread of research, sparked by a recent paper: <a href="http://arxiv.org/abs/2404.16792">Model Extrapolation Expedites Alignment</a>. The idea of the paper is simple but fun: take an LLM that’s had some SFT done, do a little DPO (or other ‘alignment’ technique of your choice), look at the change in the weights, then <strong>extrapolate</strong> in that direction to get an <em>even more aligned</em> model, saving compute. In other words, if you mix the starting model with a DPO-trained one based on a scalar <em>alpha</em>, you start with <em>alpha</em>=0 at the performance of the starting model, then as you increase it to 1 your performance rises too, until you hit the performance of the DPO-trained model. And if you use <em>alpha</em> &gt; 1, you get a model that is even better than the DPO-trained one! At least, if you didn’t do much DPO training - say, 10 or 20% of what you’d normally do. There’s a few takeaways here related to how little the final alignment training is doing in cases like this, but the core idea is one that is worth exploring more generally - namely, <strong>getting better performance by interpolating from a worse model/prediction to a better one and then extrapolating beyond it</strong>. In this post I’ll explore some related ideas, and my own experiments extrapolating from a small model to a big model to eke out even more performance. I’ve also got a <a href="https://www.youtube.com/watch?v=3odZosJ5CeE">video</a> of me running through these papers and then taking you on the journey of trying the experiments, so you can get a picture of how fun this kind of research can be.</p>
<section id="past-work" class="level2">
<h2 class="anchored" data-anchor-id="past-work">Past Work</h2>
<p>For me, the classic example of this kind of idea is ‘Classifier Free Guidance’ in diffusion models. You interpolate between the prediction from an unconditional model and a conditional one, and then extrapolate beyond it. This is a very powerful technique, and it’s been used in a lot of different ways. Introduced by Salimans and Ho in 2021, it gained popularity through the GLIDE paper which used it to get open source text-to-image diffusion working well early on in the diffusion model revolution, where it has since become a staple of pretty much all subsequent text-to-image systems.</p>
<p>I remember Katherine Crawson and others discussing extending the idea to LLMs and AR models in general soon after that - and sure enough a literatre search finds papers like <a href="http://arxiv.org/abs/2306.17806">Stay on topic with Classifier-Free Guidance</a> that do just that, comparing the preictions with and without a piece of context in the prompt and doing CFG-like scaling of the logits before sampling. (I found this paper after re-inventing the idea to myself and going looking).</p>
<p>Another set of works doing something similar is <a href="http://arxiv.org/abs/2309.03883">DOLA: DECODING BY CONTRASTING LAYERS IMPROVES FACTUALITY IN LARGE LANGUAGE MODELS</a> which contrasted predictions based on early vs late layers in a model for improved factuality (I did an <a href="https://johnowhitaker.dev/mini-projects/dola_cfg.html">experiment</a> back then in 2023 extending it with a CFG-like guidance scale) and <a href="http://arxiv.org/abs/2503.02343">DeLTa: A Decoding Strategy based on Logit Trajectory Prediction Improves Factuality and Reasoning Ability</a> which did something similar by fitting a regression line through the preds of the model at different layers:</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/delta.png" class="img-fluid"></p>
<p>Aside: I found the delta paper while using the Ai2 <a href="https://paperfinder.allen.ai/">Paper Finder</a> tool to look for DoLa since I was blanking on the name, it’s an incredible lit search tool that you should definitely try out asap if you do research!</p>
<p>Anyway, today I thought I’d try the idea of using small -&gt; big model and doing the extrapolation between their logits. I know we chatted about this back then in 2023, I know I talked about it on Twitter back then too and maybe there was even a paper? But let’s pretend we’re doing novel research it’s way more fun that way :D</p>
</section>
<section id="experiment" class="level2">
<h2 class="anchored" data-anchor-id="experiment">Experiment</h2>
<p>In <a href="https://colab.research.google.com/drive/1yg4W6ng8Hi0KOx-FT3ThJ3JkuHWMd7Wl?usp=sharing">this notebook</a> I set up the experiment. The key step is this: <code>final_logits = (1.0 - alpha) * logits1 + alpha * logits2</code>. Where logits1 come from a smaller model (Qwen 2.5 1.5B) and logits2 come from a larger model (Qwen 2.5 3B). Once we validate that it’s all working, I load up some questions from GSM8K and get the scores like so:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">alphas <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb1-2">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> alpha <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> alphas:</span>
<span id="cb1-5">    total, correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb1-6">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)):  <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Iterate through 100 questions</span></span>
<span id="cb1-7">        q, a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_list[i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(data_list[i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>].split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#### "</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])</span>
<span id="cb1-8">        model_a, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_answer(q, alpha)</span>
<span id="cb1-9">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> a: correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-10">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb1-11">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total</span>
<span id="cb1-12">    scores.append(score)</span>
<span id="cb1-13">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Alpha: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span></code></pre></div></div>
<p>The result:</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/extrapolative_res1.png" class="img-fluid"></p>
<p>Womp womp waaaa. An early test with fewer samples had such a nice peak at alpha = 1.2 making it look like the idea worked great! Alas - in this case at least perhaps it was too good to be true. I’ll update this post if I get a more positive result trying this at a larger scale.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>It might sound a little silly to effectively fit a curve for the preds of a few smaller models to try to predict what a bigger one might do, but I remain hopeful there might be places this works… And I can imagine scenarios where you already have both sets of preds or can get them cheaply - perhaps a speculative sampling scenario. It’s also potentially a lot more useful when you have good/bad models in other ways. For e.g.&nbsp;the use-case I’m excited about is tweaking the context/training. Say you’re working on a model for FastHTML. Since it’s a new framework it isn’t really in the training data, so models tend to not know it and try to answer about FastAPI or something instead. We could fine-tune a model on what examples we have, or pop some docs in context, but even then the models might really want to default to the better-known frameworks. With this, we could contrast a model with no training/context with our fine-tuned one, and then extrapolate to get a model that is even more likely to do things the “right” FastHTML way. And so on.</p>
<p>Anyway, I hope this post and the video inspire you to run your own experiments - there are so many directions you can take this! If you try anything fun do let me know. Best. Johno.</p>
<p>PS: I ran this on Qwen 7B -&gt; 14B too, with similarly underwhelming results.</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/extrapolative_res2.png" class="img-fluid"></p>
</section>
<section id="pps-full-code" class="level2">
<h2 class="anchored" data-anchor-id="pps-full-code">PPS: Full code</h2>
<p>For the record, here’s the code that produced the data for the first graph above</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb2-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> transformers</span>
<span id="cb2-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb2-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> warnings</span>
<span id="cb2-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> httpx, json</span>
<span id="cb2-6"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> re</span>
<span id="cb2-7"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm.auto <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb2-8"></span>
<span id="cb2-9"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Suppress irrelevant warnings</span></span>
<span id="cb2-10">warnings.filterwarnings(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"ignore"</span>, category<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="pp" style="color: #AD0000;
background-color: null;
font-style: inherit;">UserWarning</span>)</span>
<span id="cb2-11">transformers.logging.set_verbosity_error() <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Hide tokenizer warnings</span></span>
<span id="cb2-12"></span>
<span id="cb2-13"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># --- Configuration ---</span></span>
<span id="cb2-14">DEVICE <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> torch.cuda.is_available() <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cpu"</span></span>
<span id="cb2-15"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Using device: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>DEVICE<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-16"></span>
<span id="cb2-17"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># --- Load Models ---</span></span>
<span id="cb2-18">model_name <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Qwen/Qwen2.5-1.5B-Instruct"</span></span>
<span id="cb2-19">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(model_name)</span>
<span id="cb2-20">terminators <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [tokenizer.eos_token_id]</span>
<span id="cb2-21"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> tokenizer.pad_token <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>:</span>
<span id="cb2-22">      tokenizer.pad_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.eos_token</span>
<span id="cb2-23"></span>
<span id="cb2-24">model1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb2-25">    model_name,</span>
<span id="cb2-26">    torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16).to(DEVICE).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-27"></span>
<span id="cb2-28">model2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb2-29">    <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Qwen/Qwen2.5-3B-Instruct"</span>,</span>
<span id="cb2-30">    torch_dtype<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>torch.bfloat16</span>
<span id="cb2-31">).to(DEVICE).<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">eval</span>()<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb2-32"></span>
<span id="cb2-33"></span>
<span id="cb2-34"></span>
<span id="cb2-35"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_next_token_logits(model, input_ids, past_key_values):</span>
<span id="cb2-36">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">"""</span></span>
<span id="cb2-37"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    Performs a forward pass and returns the logits for the *next* token,</span></span>
<span id="cb2-38"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    along with the updated past_key_values.</span></span>
<span id="cb2-39"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">    """</span></span>
<span id="cb2-40">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">with</span> torch.no_grad():</span>
<span id="cb2-41">        outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(</span>
<span id="cb2-42">            input_ids<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>input_ids,</span>
<span id="cb2-43">            past_key_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>past_key_values,</span>
<span id="cb2-44">            return_dict<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb2-45">        )</span>
<span id="cb2-46">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Logits are for the *next* token prediction (shape: [batch_size, vocab_size])</span></span>
<span id="cb2-47">    next_token_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs.logits[:, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, :]</span>
<span id="cb2-48">    updated_past_key_values <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> outputs.past_key_values</span>
<span id="cb2-49">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> next_token_logits, updated_past_key_values</span>
<span id="cb2-50"></span>
<span id="cb2-51"></span>
<span id="cb2-52"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> contrastive_generate(</span>
<span id="cb2-53">        model1,</span>
<span id="cb2-54">        model2,</span>
<span id="cb2-55">        tokenizer,</span>
<span id="cb2-56">        messages,</span>
<span id="cb2-57">        alpha: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>,</span>
<span id="cb2-58">        max_new_tokens: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">50</span>,</span>
<span id="cb2-59">        temperature: <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>,</span>
<span id="cb2-60">        terminators <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-61">    ):</span>
<span id="cb2-62"></span>
<span id="cb2-63">    input_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.apply_chat_template(</span>
<span id="cb2-64">        messages,</span>
<span id="cb2-65">        add_generation_prompt<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>,</span>
<span id="cb2-66">        return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span></span>
<span id="cb2-67">    ).to(DEVICE)</span>
<span id="cb2-68">    past_key_values1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-69">    past_key_values2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span></span>
<span id="cb2-70">    generated_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-71">    _, past_key_values1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_next_token_logits(model1, input_ids, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb2-72">    _, past_key_values2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_next_token_logits(model2, input_ids, <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>)</span>
<span id="cb2-73"></span>
<span id="cb2-74">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># --- Generation Loop ---</span></span>
<span id="cb2-75">    current_input_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> input_ids[:, <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>:] <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Last token in inputs</span></span>
<span id="cb2-76">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> _ <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(max_new_tokens):</span>
<span id="cb2-77">        logits1, past_key_values1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_next_token_logits(</span>
<span id="cb2-78">            model1, current_input_ids, past_key_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>past_key_values1 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use cache, mask isn't needed for single token</span></span>
<span id="cb2-79">        )</span>
<span id="cb2-80">        logits2, past_key_values2 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_next_token_logits(</span>
<span id="cb2-81">            model2, current_input_ids, past_key_values<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>past_key_values2 <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Use cache, mask isn't needed for single token</span></span>
<span id="cb2-82">        )</span>
<span id="cb2-83"></span>
<span id="cb2-84">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Key step:</span></span>
<span id="cb2-85">        final_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> (<span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span> alpha) <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> logits1 <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> logits2</span>
<span id="cb2-86"></span>
<span id="cb2-87">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 3. Apply Temperature and Sample</span></span>
<span id="cb2-88">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> temperature <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>:</span>
<span id="cb2-89">            scaled_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> final_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> temperature</span>
<span id="cb2-90">            probs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.softmax(scaled_logits, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-91">            next_token_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.multinomial(probs, num_samples<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>)</span>
<span id="cb2-92">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span>:</span>
<span id="cb2-93">            <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Greedy decoding if temperature is 0 or less</span></span>
<span id="cb2-94">            next_token_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.argmax(final_logits, dim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, keepdim<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>)</span>
<span id="cb2-95"></span>
<span id="cb2-96">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 4. Check for EOS token</span></span>
<span id="cb2-97">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> terminators <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">is</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">not</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span> <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">and</span> next_token_id.item() <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> terminators: <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-98"></span>
<span id="cb2-99">        <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># 5. Append generated token and prepare for next step</span></span>
<span id="cb2-100">        generated_ids.append(next_token_id.item())</span>
<span id="cb2-101">        current_input_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> next_token_id <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Next input is just the token we sampled</span></span>
<span id="cb2-102"></span>
<span id="cb2-103">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># --- Decode and Return ---</span></span>
<span id="cb2-104">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> tokenizer.decode(generated_ids)</span>
<span id="cb2-105"></span>
<span id="cb2-106"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Load eval data</span></span>
<span id="cb2-107">url <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"https://raw.githubusercontent.com/openai/grade-school-math/refs/heads/master/grade_school_math/data/test.jsonl"</span></span>
<span id="cb2-108">response <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> httpx.get(url)</span>
<span id="cb2-109">data_list <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-110"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> line <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> response.text.strip().split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>):</span>
<span id="cb2-111">    item <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> json.loads(line)</span>
<span id="cb2-112">    data_list.append(item)</span>
<span id="cb2-113"></span>
<span id="cb2-114"></span>
<span id="cb2-115"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> get_answer(question, alpha):</span>
<span id="cb2-116">  messages <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [</span>
<span id="cb2-117">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"system"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Answer the following question, end your response with 'final answer: your_answer'."</span>},</span>
<span id="cb2-118">    {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"role"</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"user"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"content"</span>: question},</span>
<span id="cb2-119">  ]</span>
<span id="cb2-120">  alpha <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span></span>
<span id="cb2-121">  answer_text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> contrastive_generate(model1, model2, tokenizer, messages, alpha,</span>
<span id="cb2-122">                       max_new_tokens<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">300</span>, terminators<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>terminators, temperature<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>)</span>
<span id="cb2-123">  numbers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> re.findall(<span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">r'-</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">\d</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>(?:<span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\.</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">\d</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">?</span><span class="vs" style="color: #20794D;
background-color: null;
font-style: inherit;">'</span>, answer_text)</span>
<span id="cb2-124">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">float</span>(numbers[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>])) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> numbers <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">None</span>, answer_text</span>
<span id="cb2-125"></span>
<span id="cb2-126"></span>
<span id="cb2-127">alphas <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.3</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.5</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.7</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.1</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.2</span>, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.5</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">2</span>]</span>
<span id="cb2-128">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-129"></span>
<span id="cb2-130"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> alpha <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> alphas:</span>
<span id="cb2-131">    total, correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb2-132">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">600</span>)):</span>
<span id="cb2-133">        q, a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> data_list[i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>], <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">int</span>(data_list[i][<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>].split(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"#### "</span>)[<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>].replace(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">","</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">""</span>))</span>
<span id="cb2-134">        model_a, _ <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> get_answer(q, alpha)</span>
<span id="cb2-135">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> model_a <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> a: correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-136">        total <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span></span>
<span id="cb2-137">    score <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> correct <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> total</span>
<span id="cb2-138">    scores.append(score)</span>
<span id="cb2-139">    <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Alpha: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>alpha<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">, Score: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>score<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-140"></span>
<span id="cb2-141"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(<span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">f"Scores for different alphas: </span><span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">{</span>scores<span class="sc" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">}</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">"</span>)</span>
<span id="cb2-142"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Scores for different alphas: [71.0, 72.5, 72.83333333333333, 74.16666666666667, 74.5, 74.16666666666667, 73.66666666666667, 73.83333333333333, 72.83333333333333]</span></span></code></pre></div></div>


</section>

 ]]></description>
  <category>mini-projects</category>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/mini-projects/extrapolative_sampling.html</guid>
  <pubDate>Sat, 12 Apr 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Lab Junk Rheoscopic Fluid</title>
  <link>https://johnowhitaker.dev/mini-hw-projects/rheoscopic.html</link>
  <description><![CDATA[ 




<p>As I grabbed the waste tub from my lab bench to deal with it, I noticed the liquid in the bottom swished around in a shiny and interesting way. I pipetted some out and took a closer look. Any movement resulted in irridescent swirls - I’d accidentally made a rheoscopic fluid! This isn’t exactly rare - people make it by adding mica powder to water, for example. But I couldn’t figure out what was causing the effect in my case at first - no mica here, just the remains of a couple of experiments related to cyanotype and the paper towels I’d used to clean up. Intrigued, I set about trying to figure out the minimal recipe needed to re-create the effect. Before we get to the recipe though, take a look at it in motion:</p>
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/rheo.gif" class="img-fluid"></p>
<p>Lovely, isn’t it! More videos in <a href="https://x.com/johnowhitaker/status/1906154484826595722">this twitter thread.</a></p>
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/rheo.png" class="img-fluid"></p>
<p>Turns out you can make this as follows: - Add a strip of paper towel to a container of 2% hydrogen peroxide and a few drops of dish soap. Not too much paper towel. - Leave it for several days (the longer the better) - Add color to taste :)</p>
<p>I think what is happening is the H2O2 is breaking down parts of the paper towel, leaving little bits of cellulose and other stuff suspended in the water which catch the light at the right angle, aligning based on the flow, giving the irridescent effect. The soap helps to stabilize the suspension. I used food coloring to boost the contrast. In the initial version the yellow (from potassium ferricyanide) was the initial color, then it later turned blue (from the cyanotype reaction) - which is how I got the idea. The white version was cool too in some lights but didn’t photograph nearly as well.</p>
<p>Under a microscope it looks like this:</p>
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/rheo_scope.png" class="img-fluid"></p>
<p>Anyway, it was a fun ‘science at home’ moment to spot this, chat about it with an AI, come up with some theories, try out varous mixtures (with and without the various chemicals I thought might be in the original batch) and then double check the recipe again. I bet stronger H2O2 would work better - LMK if you do an independent replication!</p>
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/rheo2.png" class="img-fluid"></p>
<p>Update, a concrete recipe: Add 25cm^2 of brown paper towel to 100ml of 3% H2O2. Add a small drop of dish soap. Heat at 40-50 degrees C for 5-10 hours or leave at room temperature for several days. Add a few drops of food coloring. I sealed 80ml of the resulting mixture in a petri dish with a magnet inside, and it makes an excellent desk fidget toy. Adding too mucn paper (e.g.&nbsp;200 cm^2) resulted in a few larger fibers rather than the desired outcome, even less than what I used here might be optimal.</p>
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/rheoscopic_paper.png" class="img-fluid"></p>
<p>Update 2: I tried an alternative recipe from <a href="https://arxiv.org/abs/1806.06120">this</a> paper (extract shown above) with great results. See <a href="https://x.com/johnowhitaker/status/1918385113181896945">here</a> for a video of the resulting desk toy - mesmerizing!</p>



 ]]></description>
  <category>mini-hw-projects</category>
  <guid>https://johnowhitaker.dev/mini-hw-projects/rheoscopic.html</guid>
  <pubDate>Fri, 11 Apr 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Hiding an Annoying Element with Stylus</title>
  <link>https://johnowhitaker.dev/tils/paid_promotion_links.html</link>
  <description><![CDATA[ 




<p>YouTube has this annoying thing where on hover a little overlay pops up over some videos that says ‘Includes paid promotion…’. I click on this accidentally fairly frequently, opening up a support.google.com link instead of the video I want. This bugs me, but it’s a very minor annoyance. In the past I’d probably just ignore it. I tried inspecting the element in the Chrome dev tools but couldn’t find it and that’s about as much effort as I’m willing to manually do for somethig this small. FOrtunately, these days we have AI! A quick back-and-forth with GPT4.5 and I’ve solved the problem for myself!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/tils/yt_overlay.png" class="img-fluid figure-img"></p>
<figcaption>The problem</figcaption>
</figure>
</div>
<p>I went with a CSS rule in the Stylus browser extension:</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode css code-with-copy"><code class="sourceCode css"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">@-moz-document</span> domain<span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">(</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"youtube.com"</span><span class="fu" style="color: #4758AB;
background-color: null;
font-style: inherit;">)</span> {</span>
<span id="cb1-2">    a<span class="ex" style="color: null;
background-color: null;
font-style: inherit;">[</span><span class="ss" style="color: #20794D;
background-color: null;
font-style: inherit;">href</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">*=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"support.google.com/youtube"</span><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">]</span> {</span>
<span id="cb1-3">        <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">display</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">none</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">!important</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-4">        <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">pointer-events</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">:</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">none</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">!important</span><span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span>
<span id="cb1-5">    }</span>
<span id="cb1-6">}</span></code></pre></div></div>
<p>I’m amused how low my tolerance for painful stuff on a computer is these days, and how happy it makes me to fix any remaining tiny paper cuts with my code-slinging AI buddies.</p>



 ]]></description>
  <guid>https://johnowhitaker.dev/tils/paid_promotion_links.html</guid>
  <pubDate>Thu, 10 Apr 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Tab Clearing (April 9 2025)</title>
  <link>https://johnowhitaker.dev/misc/tab-clear-april2025.html</link>
  <description><![CDATA[ 




<ul>
<li><a href="https://synth.grantkot.com/">Web synth</a> - great sounds, easy controls, works on mobile. Big fan. Think they use adventure time sounds, in some kind of library? I want to emulate this at some point.</li>
<li><a href="https://writings.stephenwolfram.com/2019/02/seeking-the-productive-life-some-details-of-my-personal-infrastructure/">Seeking the Productive Life: Some Details of My Personal Infrastructure</a> - revisiting this inspiring piece from Wolfram on how he optimizes bits if his life. He’s shockingly productive, I think a lot of us could learn from him.</li>
<li><a href="https://arxiv.org/abs/2504.00698">Command A: An Enterprise-Ready Large Language Model</a> - interesting paper with lots of detail on their enterprisey post-training.</li>
<li><a href="https://hkunlp.github.io/blog/2025/dream/">Dream 7B</a> - ‘Diffusion reasoning model’ continuing the trend of diffusion language models starting to look impressive at 7Bish scale, building on smaller-scale PoCs from the past. Watching this space with interest :) It is initialzed from Qwen which is an interesting tidbit.</li>
<li><a href="https://www.together.ai/blog/deepcoder">Together AI announce DeepCoder</a> - speaking of things initialized with Qwen, this 14B reasoner does extremely well thanks to good, open work on RLVR for coding, ft their ‘GRPO+’. Good-looking work!</li>
<li><a href="https://hackaday.com/2025/04/02/70-diy-synths-on-one-webpage/">70 DIY Synths On One Webpage</a> an inspiring collection for if I ever feel like building HW synths</li>
<li><a href="https://www.lesswrong.com/posts/9PiyWjoe9tajReF7v/the-hidden-cost-of-our-lies-to-ai">The Hidden Cost of Our Lies to AI</a> - DNF, but I do like peeking at what the LW people are talking about, and I do have a string sentiment against lying to AI that the author shares.</li>
<li><a href="https://yetch.studio/products/edge-piece-puzzle">Simone’s all-edge puzzle</a> - A nice idea I’m getting for my puzzle-loving wife (shh it’s a birthday secret)</li>
<li><a href="https://docs.perplexity.ai/guides/pricing">https://docs.perplexity.ai/guides/pricing</a> - Their deep research API looks nice, I saw a colleague using it in an internal tool and want to copy that so I don’t have to break flow to head to openai/google for deep research style questions.</li>
<li><a href="https://ai-2027.com/">AI2027</a> - I enguaged a lot with this, listening to the Dwarkesh podcast and skimming the research. They’re nothing if not thorough. I’m not sure I quite buy the foomish takeoff stuff still, but I also don’t think you can dismiss these ideas out-of-hand.</li>
<li><a href="https://hackaday.com/2025/04/04/3d-print-and-play-the-super-mario-tune-as-a-fidget-toy/">3D Print (and Play!) The Super Mario Tune As A Fidget Toy</a> - I’m going to print off some rick-rolls and leave them lying around</li>
<li><a href="https://arxiv.org/abs/2504.05118">VAPO paper</a> - I’ve been meaning to look at DAPO, VAPO and other GRPO variants but not had the time+inclination recently, feels like the DPO or LoRA fevers as everyone tries out minor mods that work on some benches…</li>
<li><a href="https://blog.google/products/google-cloud/ironwood-tpu-age-of-inference/">Google’s Ironwood TPUs</a> look like inference beasts…</li>
<li><a href="https://arxiv.org/abs/2411.15098">OmniControl</a> and this <a href="https://huggingface.co/spaces/Yuanshi/OminiControl_Art">OmniControl Art</a> space (the latter replicating 4o’s stylization trick with Flux) look like an interesting modern controlnet alternative. I want to take a closer look at some point.</li>
<li><a href="https://arxiv.org/abs/2404.16792">Model Extrapolation Expedites Alignment</a> - fun: train DPO model, interpolate between base SFT model and that, with scale &gt; 1. CFG but it’s comparing preds from DPO model vs base, giving a boost on some tasks haha. Not sure it’ll hold for more carefully trained models but neat to see. Reminds me of a paper that did something similar but using early vs late layer predictions instead of two model variants. Have also seen small vs large models. ALl these hacks feel likely to get bitter-lessoned but fun for now. Another in this veign was:</li>
<li><a href="http://arxiv.org/abs/2501.18585">Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs</a> - which supresses thought-changing tokens like “alternatively” until the model has thought for long enough on that topic (some settable threshold) which is a funny hack to encourage longer thoughts. They do see some improvement!</li>
<li><a href="https://arxiv.org/abs/2504.04022">Rethinking Reflection in Pre-Training</a> - looking at how much ‘reasoning’ is learnt during pre-training, and identifying a measure that grows steadily, which seems very relevant to the discussion over whether reasoning is all RL or is motly elicitation of latent capabilities learnt during pretraining.</li>
<li><a href="https://arxiv.org/abs/2503.22828">Learning to Reason for Long-Form Story Generation</a> very neat idea: RLVR on reasoning hains with the score being how likely the model is to generate the gold standard output after said reasoning chain: “Our reward uses a reference model to get the improved likelihood of the true next chapter.”, “🎯 Verifiable Rewards via Completion Likelihood Improvement (VR-CLI) evaluates reasoning by the”improvement” in downstream perplexity of the gold completion (the next chapter).”</li>
<li><a href="https://arxiv.org/abs/2404.09937">Compression Represents Intelligence Linearly</a> - “we find that LLMs’ intelligence – reflected by average benchmark scores – almost linearly correlates with their ability to compress external text corpora. These results provide concrete evidence supporting the belief that superior compression indicates greater intelligence.” - nice to have more empirical evidence for this, I think it’s a good heuristic to keep in mind.</li>
<li><a href="https://arxiv.org/abs/2504.02495">Inference-Time Scaling for Generalist Reward Modeling</a> - TO READ</li>
<li><a href="https://www.ethansmith2000.com/post/on-vibe-coding">On Vibe Coding</a> - TO READ</li>
<li><a href="https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/">Taking a responsible path to AGI</a> - TO READ</li>
<li><a href="https://huggingface.co/spaces/enzostvs/deepsite">Deep site</a> - TO TRY</li>
</ul>



 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/misc/tab-clear-april2025.html</guid>
  <pubDate>Wed, 09 Apr 2025 00:00:00 GMT</pubDate>
  <media:content url="https://i.pinimg.com/originals/40/fe/86/40fe863b3ccf9ceb56cbef670cf6bc41.gif" medium="image" type="image/gif"/>
</item>
<item>
  <title>Can AI Models Rhyme?</title>
  <link>https://johnowhitaker.dev/mini-projects/rhyme_eval.html</link>
  <description><![CDATA[ 




<p>Which word in [‘Buoy’, ‘Employ’, ‘Decoy’, ‘Corduroy’] rhymes with ‘Ennui’? I went looking for an online quiz with questions like this and figured it would make a good mini LLM eval. Without further ado, here are the results:</p>
<p><img src="https://johnowhitaker.dev/mini-projects/images/rhyme_eval.png" class="img-fluid"></p>
<p>I put the full code in a gist <a href="https://gist.github.com/johnowhitaker/b1ca1ac7e1814bf84aa52dde6d174277">here</a>. It’s pretty easy to knock together little evals like this! To make extracting the answers easy I used function calling, with TogetherAI for the open-source models and OpenAI for theirs. Since they all use the OpenAI API it was trivial to swap between them. As you can see if you skim my gist, I got AI to write the bulk of the code BUT did it in small enough pieces that I could check and tweak as needed.</p>
<p>For ease of replication, here are the questions (alas, this probably poisons this eval for future AIs but we can always make more!):</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">qs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [{<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tough'</span>,</span>
<span id="cb1-2">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Chaff'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Tariff'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cliff'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bluff'</span>],</span>
<span id="cb1-3">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bluff'</span>},</span>
<span id="cb1-4"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sigh'</span>,</span>
<span id="cb1-5">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Achy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Alloy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fussy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Awry'</span>],</span>
<span id="cb1-6">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Awry'</span>},</span>
<span id="cb1-7"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ennui'</span>,</span>
<span id="cb1-8">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Buoy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Employ'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Decoy'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Corduroy'</span>],</span>
<span id="cb1-9">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Buoy'</span>},</span>
<span id="cb1-10"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ballet'</span>,</span>
<span id="cb1-11">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Gaffe'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cafe'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Chafe'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Strafe'</span>],</span>
<span id="cb1-12">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Cafe'</span>},</span>
<span id="cb1-13"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Marshal'</span>,</span>
<span id="cb1-14">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Impartial'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lethal'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Patriarchal'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Substantial'</span>],</span>
<span id="cb1-15">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Impartial'</span>},</span>
<span id="cb1-16"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ignore'</span>,</span>
<span id="cb1-17">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Comport'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Transport'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Rapport'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Purport'</span>],</span>
<span id="cb1-18">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Rapport'</span>},</span>
<span id="cb1-19"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Aisle'</span>,</span>
<span id="cb1-20">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Smile'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Fuels'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Spies'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Else'</span>],</span>
<span id="cb1-21">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Smile'</span>},</span>
<span id="cb1-22"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hymn'</span>,</span>
<span id="cb1-23">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Climb'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Limb'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Comb'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Thumb'</span>],</span>
<span id="cb1-24">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Limb'</span>},</span>
<span id="cb1-25"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Stray'</span>,</span>
<span id="cb1-26">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Levee'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Spree'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Melee'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Emcee'</span>],</span>
<span id="cb1-27">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Melee'</span>},</span>
<span id="cb1-28"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bottom'</span>,</span>
<span id="cb1-29">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Solemn'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Damn'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Column'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Autumn'</span>],</span>
<span id="cb1-30">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Autumn'</span>},</span>
<span id="cb1-31"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Deuce'</span>,</span>
<span id="cb1-32">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mousse'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lacrosse'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Posse'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Finesse'</span>],</span>
<span id="cb1-33">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Mousse'</span>},</span>
<span id="cb1-34"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Subtle'</span>,</span>
<span id="cb1-35">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Hustle'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Committal'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Rebuttal'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Acquittal'</span>],</span>
<span id="cb1-36">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Rebuttal'</span>},</span>
<span id="cb1-37"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Ukulele'</span>,</span>
<span id="cb1-38">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Oily'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Icily'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Daily'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Lily'</span>],</span>
<span id="cb1-39">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Daily'</span>},</span>
<span id="cb1-40"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Spaghetti'</span>,</span>
<span id="cb1-41">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sweaty'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Reality'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Dirty'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Bounty'</span>],</span>
<span id="cb1-42">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Sweaty'</span>},</span>
<span id="cb1-43"> {<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'question'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Queue'</span>,</span>
<span id="cb1-44">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'options'</span>: [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Meow'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Escrow'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'Straw'</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'View'</span>],</span>
<span id="cb1-45">  <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'answer'</span>: <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'View'</span>}]</span></code></pre></div></div>



 ]]></description>
  <category>mini-projects</category>
  <guid>https://johnowhitaker.dev/mini-projects/rhyme_eval.html</guid>
  <pubDate>Tue, 01 Apr 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>My Thoughts on the Future of “AI”</title>
  <link>https://johnowhitaker.dev/essays/future-ai.html</link>
  <description><![CDATA[ 




<p>You should definitely read <a href="https://nicholas.carlini.com/writing/2025/thoughts-on-future-ai.html">this piece</a> by Nicholas Carlini. Many people have very confident beliefs - from ‘100% AGI in 3 years’ to ‘AI is just hype’. I think a spectrum of outcomes are possible and worth considering. Motivated by Carlini’s post, I’m going to attempt to document my own (similar) views in this post plus additional thoughts.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://nicholas.carlini.com/writing/2025/maybe_future.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1 from Carlini’s Post: ‘A very scientific plot of the future potential for LLMs.’</figcaption>
</figure>
</div>
<section id="my-evolving-uncertain-views" class="level2">
<h2 class="anchored" data-anchor-id="my-evolving-uncertain-views">My (Evolving, Uncertain) Views</h2>
<p>I’d historically been in the camp of ‘LLMs are fuzzy pattern matchers, impressive but bounded in what they can do’. Very powerful, but e.g.&nbsp;I didn’t expect even much larger LLMs to be able to play connections well 0-shot - a game that for humans requires back-tracking and mentally ‘trying out’ different combinations. The initial o1 announcement was the final nail in the coffin for that theory for me - ‘thinking’ tokens expand the space of ‘programs’ LLMs can run - ‘reasoning’ not just ‘intuition’. And RL means they can potentially discover new useful patterns that work for them, rather than relying on human demonstration (i.e.&nbsp;pretraining or SFT).</p>
<p>I still think there’s a small chance the recent wave of RL/post-training stuff is mostly bringing out more from the powerful-fuzzy-pattern-matching pretraining stage - the so-called ‘<a href="https://www.interconnects.ai/p/elicitation-theory-of-post-training">elicitation hypothesis</a>’, which would possibly mean that we’re still plateauing once the immediate gains run out, with much larger training runs (e.g.&nbsp;4.5) yielding only modest improvements, and ‘needs to have had <em>some</em> demonstration on the internet’ still being a requirement. This would still means incredibly powerful models! Lots of things are well-covered in pre-training space, e.g.&nbsp;frontend dev which current models are incredible at. But you’d still want a human steering when doing any novel research, you’d still expect ~poor performance on truly novel tasks. IF it isn’t just that, then it looks like this approach could take us <strong>very far</strong>. Lots of things you can make verifiable tasks for will get ‘solved’ quite fast - and a big open question is how well gains on a bunch of different tasks like this transfer to stuff that’s harder to score.</p>
<p>OK so so far I’m saying I think could slow down, could go ‘very far’. Hedging your bets Johno! Yeah, I’m conflicted. I have very wide error bars on my future predictions! I must confess recently my ‘feeling the AGI’-meter has been wavering upwards.</p>
<p>More reasons for the slow case:</p>
<ul>
<li>People I respect (e.g.&nbsp;Jeremy Howard) have seen past hype cycles and suggest this could be more of the same</li>
<li>A lot of AGI believers seem to hold almost religious convictions. Reminds me of my religious past - which makes me skeptical that the arguments presented are the true reasons for belief. Sometimes looks more like a desperate hope for the coming super-being.</li>
<li>While current models are ‘wow’ when vibe-coding simple applications, they still break when you do more complex things or work a different way (but less over time, so not a strong case).</li>
<li>So much hype and nonsense in the AI space, which I fear is distorting my perceptions. So. Many. Grifters. most of whom have little clue what they’re talking about.</li>
<li>Lots of people selling the dream work at companies who stand to gain, with lots of funding on the line and the potential for regulatory capture or other power grabs.</li>
</ul>
<p>More reasons for the AGI soon case:</p>
<ul>
<li>Lots of researchers I respect hold this idea</li>
<li>Huge investment and influx of smart people (many manhattan-project-equivalents)</li>
<li>I’m very incentivized to think I’m special for being a very smart human, and resistant to the idea that ‘being smart’ might not be so valuable going forward, which might make for some motivated reasoning against AI progress (a.k.a. cope)</li>
<li>Existing progress is, when you step back a bit, truly remarkable</li>
</ul>
<p>One recent <a href="https://arxiv.org/abs/2503.14499">paper</a> (<a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">blog post</a>) that did a great job highlighting current progress looked at the <em>length of time</em> tasks take humans as a measure of difficulty, and how models do at these tasks. Notably, and matching my experience with coding agents like claude-code, recent models do well at tasks that could take 15 minutes and only drop below ~50% success rate at around the hour mark. Importantly, they estimate the doubling rate for the lenght of tasks AI can do at 7 months and possibly dropping. The paper is new and focused on code but I think captures the larger trend well.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/task_success_rate.png" class="img-fluid figure-img"></p>
<figcaption>Figure from the blog post</figcaption>
</figure>
</div>
<p>At the very least then, I think we should take the possibility seriously that folks like Dario might be right - even if the timelines are a little longer than claimed. Because if they are right, then we’re in for some big shifts!</p>
</section>
<section id="so-what" class="level2">
<h2 class="anchored" data-anchor-id="so-what">So What?</h2>
<p>AGI or not, AI is powerful and improving, which leads to a few worries:</p>
<ul>
<li>Misuse: I’m worried people will do some bad things with this technology - no malign super-intelligence required. It’s tricky to prevent this when working on such a general, ‘dual-use’ technology but if you’re in this space have a good think about what your specific contributions can/will be used for please</li>
<li>Concentration of power: a very valuable + powerful technology in the hands of a few could mean concentrations of power and wealth that are Probably Not Good(TM) - see <a href="https://www.fast.ai/posts/2023-11-07-dislightenment.html">AI Safety and the Age of Dislightenment</a> for an interesting essay on this</li>
<li>Gradual Disempowerment: if people rely on AI more and more to do coding and other cognitive tasks, they might lose their skills and/or sense of purpose. Which I think would be sad! Especially since co-creating with AI could be so positive.</li>
<li>Junk Food: relatedly, reward-tuned feeds optimised for engagement, AI answers optimized for us to like them, AI relationships optimized for profit… might not constitute a healthy diet for our poor primate brains.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/anthropic_views.png" class="img-fluid figure-img"></p>
<figcaption>From Anthropic’s recent <a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan">post</a></figcaption>
</figure>
</div>
<p>If you believe, as many do, that we’re just a few years away from “Intellectual capabilities matching or exceeding that of Nobel Prize winners across most disciplines” (which <a href="https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan">Anthropic claims will be hear by late 2026/2027</a>) then there are additional worries - how should we re-structure society? Will there be new jobs for people? Will this be like that Rick and Morty episode where the dinosaurs return and humanity basically retires? Will the ‘country of geniuses in a datacenter’ have rights, or decide they don’t want to make me another HTML snake game? While I’m not sold on the superintelligence front just yet I think governments etc should be thinking very seriously about what to do under different rates of AI advancement.</p>
<p>I am currently working at a company that is (effectively) betting <strong>against</strong> the ‘country of geniuses by 2026’ thesis. We’re teaching people to code, carefully and mostly by hand, with AI assistance. We’re building our own tooling and experimentation muscles to develop hundreds or thousands of clever, specific AI applications. We’re thinking hard about the (current) limitations of LLMs, and how to use them to augment human creation rather than trying to replace people entirely. And we’re planning to work on this for many years. If AI can build better than us by next year, well, we lost the bet! Still - I can’t think of a better way to spend my time regardless of outcome, and I think we and many others can play a small part in finding ways to spread the benefits of AI far and wide (especially with a focus on efficiency and learning).</p>
</section>
<section id="you-tell-me" class="level2">
<h2 class="anchored" data-anchor-id="you-tell-me">You Tell Me</h2>
<p>Anyway, this is a long rambly post. What are your thoughts dear hypothetical reader? Do you lean more strongly one way or another? Do you have smaller error bars with justifications you can share? Have you got sufficient hobbies to keep you busy in humanities retirement? Are you stockpiling problem descritions from your backlog to feed to 2026’s top AI? Do you have suggestions for things people should work on <em>now</em>? Please let me know :)</p>
<p>PS: As recommended by Nicholas Carlini, I encourage you to think of concrete demos that would sway you on capabilities/timelines. Lots of my past ones have fallen! A current one I use would be: come up with an efficient (possibly GPU-accelerated) way to solve the map folding problem and get an answer for the 8x8 case without years of CPU time. I don’t know if this is possible (!) but I do know there aren’t existing solutions, and solving it would require some clever thinking about a problem that is quite ‘spatial’ and geometric (something current models struggle with). Another might be playing SMESS (an old obscure chess derivative) given the rules - I’d be impressed to see a model learning to do this via self-play, and even more to see one able to play at my level (i.e.&nbsp;smart human with ~2 games of experience so far) without any training at all. We’ll see how long these last :)</p>
<p>PPS: ‘AGI’ is a tricky term - I’ve claimed since GPT-4 that we already have something that is undeniably artificial, somewhat general, and definitely intellight! But I use it in this post as shorthand for ‘powerful, transformative AI that is &gt;= human level at lots of things’.</p>


</section>

 ]]></description>
  <category>essays</category>
  <guid>https://johnowhitaker.dev/essays/future-ai.html</guid>
  <pubDate>Tue, 18 Mar 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Quickly Making Carbon Quantum Dots</title>
  <link>https://johnowhitaker.dev/mini-hw-projects/cqds.html</link>
  <description><![CDATA[ 




<p>“Quantum dots” have a very cool name (possibly due to marketing for fancy TVs) but they live up to it - they’re tiny particles that flouresce in pretty colors, with applications in everything from solar cells to medical imaging. They’re often made from somewhat nasty materials like cadmium, but carbon quantum dots (CQDs) are a more environmentally friendly alternative. They can be made from simple precursors at home! In this post I share my early experiments making them in a few different ways.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/qd1.png" class="img-fluid figure-img"></p>
<figcaption>A laser shines through three cuvettes, two of which contain carbon quantum dots</figcaption>
</figure>
</div>
<p>Initial inspo: <a href="https://www.instructables.com/DIY-Quantum-DotsNanotech-in-Your-Kitchen/">this Instructables post</a> and various replications from hobby scientists on the net. Using a microwave cut the time taken down from 4-8 hours (the ‘hydrothermal method’ in the formal literature) to &lt;10 minutes. In my attempt I used much smaller quantities that the Instructables post, so two ~2-minute microwave sessions were sufficient. Still - I’m an impatient chap!</p>
<p>TODO link video</p>
<p>So, here’s my new, minimal, quick-and-dirty method for making carbon quantum dots:</p>
<ul>
<li>In a spoon, put a pinch of ascorbic acid (vitamin C) powder* and a pinch of sucrose (table sugar).</li>
<li>Add a few drops of water (~1/4 to ~1/2 tsp)</li>
<li>Heat over a flame (I used a mini blowtorch) until the most of the liquid is gone and the color starts turning golden brown (like caramel).</li>
<li>Add some water back, yiending a brownish-yellow syrup.</li>
</ul>
<p>*: Use vinegar instead of water for a more acidic solution if you don’t have ascorbic acid on hand - bubbles more furiously but works just as well.</p>
<p>Drip some of this into water while shining a UV light or blue laser through it. If all goes well you’ll see some beautiful blue-green flourescence!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/qd3.png" class="img-fluid figure-img"></p>
<figcaption>Dropping in the syrup</figcaption>
</figure>
</div>
<p>I’ve been trying to use a technique called ‘Direct Light Scattering’ to estimate the particle size. From the literature, it seems the CQDs are ~2-4nm. But unlike other QDs, the color isn’t tied to size as much - so who knows. I was skeptical these are even true quantum dots - maybe caremalized sugar just fluoresces under UV light? But nope, seems like this is matching what the science nerds do and these are indeed QDs. My DLS setup needs some tweaking before I’m ready to report any results with certainty though :)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/mini-hw-projects/images/qd2.png" class="img-fluid figure-img"></p>
<figcaption>Attempting DLS - this attempt was too informal to be useful</figcaption>
</figure>
</div>
<p>PS: other things I tried</p>
<ul>
<li>Just sugar (no acid) - no flourescence</li>
<li>A basic version with sodium bicarbonate (baking soda) in place of ascorbic acid - no flourescence</li>
<li>Using vinegar instead of water, with just sugar added - WORKS</li>
<li>Trying other solvents (careful if you’re heating IPA for e.g.&nbsp;- it’s flammable!) or adding things later to try and change the color - no luck</li>
<li>Eating it (“Johno don’t eat the quantum!”) - no superpowers yet. Probably don’t do this :)</li>
</ul>
<p>I really want to make different colors, so next I’ll look around for other carbon precursors and try different heating methods. For now, I have an easy way to make pretty glowy water, which is at least a start!</p>



 ]]></description>
  <category>mini-hw-projects</category>
  <guid>https://johnowhitaker.dev/mini-hw-projects/cqds.html</guid>
  <pubDate>Sat, 01 Mar 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Direct Ascent Synthesis: Revealing Hidden Generative Capabilities in Discriminative Models</title>
  <link>https://johnowhitaker.dev/posts/DAS.html</link>
  <description><![CDATA[ 




<p>I have a paper out! What!?!? Johno doesn’t write papers. True. But when Stanislav Fort discivered a neat trick that was one I’d also found back in the day, we got talking and figured it ought to be better documented so other people can use it too. I have to say: he did all the hard work! I sadly didn’t have time to play much, but did chip in a little. This blog post is a few of my own thoughts, but you should read the <a href="https://arxiv.org/abs/2502.07753">paper</a> first.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/posts/images/das.png" class="img-fluid figure-img"></p>
<figcaption>Figure 1 from our paper</figcaption>
</figure>
</div>
<p>The TL;DR is as follows: instead of optimizing raw pixels, optimize a collection of image tensors at different resolutions that get resized and stacked together to form the final image. This turns out to have really neat regularization effects, and gives a really nice primitive for seeing what ‘natural’-ish images trigger various features in classifiers etc. This is pretty much the idea behind my 2021 <a href="https://github.com/johnowhitaker/imstack">‘imstack’</a> stuff, but made cleaner and more general.</p>
<p>The other trick is to do some augmentations, critically adding some jitter (different crops) and noise. Once you have these pieces in place, you can optimize towards a text prompt with CLIP, or do style transfer, or trigger a specific class in a classification model… the possibilities are endless. Here’s the code to make the quintessential ‘jellyfish’ from an imagenet model for e.g.&nbsp;(<a href="https://colab.research.google.com/drive/1gLZXcPIKpBwYWgweOVli9ORcOyJ-khJ5?usp=sharing">colab</a>)</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> stack(x, large_resolution):</span>
<span id="cb1-2">  out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.0</span></span>
<span id="cb1-3">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,p <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(x):</span>
<span id="cb1-4">    out <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+=</span> F.interpolate(p, size<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span>(large_resolution, large_resolution), mode<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'bicubic'</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> resolutions[i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">&gt;</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span> <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">else</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">'nearest'</span>)</span>
<span id="cb1-5">  <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> out</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">def</span> raw_to_real_image(raw_image): <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">return</span> (torch.tanh(raw_image)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">1.0</span>)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">2.0</span></span>
<span id="cb1-8"></span>
<span id="cb1-9">large_resolution <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">336</span></span>
<span id="cb1-10">resolutions <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,large_resolution<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">4</span>)</span>
<span id="cb1-11">image_layers <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [torch.zeros(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>,<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">3</span>,res,res).to(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span>) <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> res <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> resolutions]</span>
<span id="cb1-12"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i,p <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(image_layers): p.requires_grad <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span></span>
<span id="cb1-13">optimizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.optim.SGD(image_layers, <span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.005</span>)</span>
<span id="cb1-14"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> step <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> tqdm(<span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">100</span>)):</span>
<span id="cb1-15">  optimizer.zero_grad()</span>
<span id="cb1-16">  images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> raw_to_real_image(stack(image_layers, large_resolution))</span>
<span id="cb1-17">  images <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> make_image_augmentations(images, count<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">16</span>, jitter_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">56</span>, noise_scale<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="fl" style="color: #AD0000;
background-color: null;
font-style: inherit;">0.2</span>)</span>
<span id="cb1-18">  loss <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-</span>model(normalize(images))[:, <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">107</span>].mean()</span>
<span id="cb1-19">  loss.backward()</span>
<span id="cb1-20">  optimizer.step()</span></code></pre></div></div>
<p>I want to do a video explanation soon to capture more thoughts on this and show off more of what this technique can do. See also, <a href="https://x.com/stanislavfort/status/1890724291752100265">Stanislav’s announcement post</a>. The rest of this post is me rambling on some tangential bits that have come up since the paper was released.</p>
<section id="thoughts" class="level2">
<h2 class="anchored" data-anchor-id="thoughts">Thoughts</h2>
<p>The initial responses to SF’s post about the paper on Twitter were an interesting mix: - ‘Wow this is amazing’ (often a little <em>too</em> enthusiastic, in the ‘this makes diffusion models obsolete’ direction!) - This is nothing new and you don’t cite [X]. Some were reasonable suggestions for related work, some were thigns we did in fact cite (lol) and a fair few were missing the point of the paper, interpreting it as somehow claiming the idea of generation via inversion as opposed to just saying ‘hey we found a better method to do that that actually kinda works now’.</p>
<p>Some of the misunderstanding was maybe on us re: wording, some was due to this coming out in 2025 despite being an older idea, and some was folks being trigger-happy - understandably so, having seen previous works that ripped off ideas from the good old days of AI art. Getting involved in the discussion was interesting for me: I was ‘part of the scene’ back then, and while everyone was riffing off eachother’s ideas and sharing cool stuff freely, it wasn’t exactly documented - if you weren’t there in the eleutherai discord and the various private channel offshoots or checking out the copy-of-a-copy-of-a-copy-of-a-colab notebooks you’d have no idea what ideas had or hadn’t been done. I contributed new stuff to the collective, as did many others, and didn’t really care about attribution or first dibs. But since a lot of that happened outside of academia, I pity any researchers trying to work on adjacent things - there’s no paper search that will turn up that early work! Fortunately, there did end up being papers on some of the key ideas, which we were careful to cite (with updates to add a few I missed thanks to the helpful Tiwtter discussion).</p>
<p>I’m a little conflicted though - I don’t like the idea that an idea doesn’t count unless it’s on arxiv. I don’t like the idea that if you <em>do</em> put it on archive belatedly a twitter mob might yell at you. I was very happy when SF reached out - as soon as he realized my imstack work pre-dated his independant discovery of the idea he went straight to the ethical “I could never write this up without you” stance, but if he had I honesty wouldn’t have cared (apart from ’hey great minds think alike) since how the heck would he be expected to find a pokey github pages project with a few colabs or my old YouTube lessons with 100s of views. I’m glad we’ve got a thing we can point at now if we (or anyone) builds on this cool idea. Anyway.</p>
<p>Going forward, I think my approach is going to stay the same as it mostly has been: write things down as easily as possible (tweets as things go, blogs asap), share things freely, try and point to things that inspire me and hope to inspire others, don’t fuss if someone else has the same ideas or builds on mine.</p>


</section>

 ]]></description>
  <category>papers</category>
  <category>blogs</category>
  <guid>https://johnowhitaker.dev/posts/DAS.html</guid>
  <pubDate>Mon, 17 Feb 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Bubble Colors</title>
  <link>https://johnowhitaker.dev/misc/bubble_colors.html</link>
  <description><![CDATA[ 




<p>I learned recently that all it takes to get nice photos/videos of bubbles is a wide area light source - which I happened to have on hand in the form of a video light a friend was getting rid of. You blow a bubble onto something that will keep it in place - in my case an inverted teacup, and then photograph the reflection of the light. The results are stunning:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/misc/thumbnails/bubble_photo.jpg" class="img-fluid figure-img"></p>
<figcaption>Bubble colors</figcaption>
</figure>
</div>
<p>It amused me that I liked these so much given that I’ve also been loving my <a href="mini-hw-projects/scope_mods.md">crystal photography</a>. More than that - the color palettes seem very similar… Turns out, this is because they both rely on similar thin-film effects (although not identical, since in the crystal case the colors show up thanks to the polarizing filters and the crystals ability to change the polarization of light). You can look up the ‘Michel-Lévy color chart’ to see the expected color for a section of a given thickness and birefringence. I found <a href="https://github.com/markus-bauer/calculated_Michel_Levy_Chart">this code</a> to calculate these values, and turned them into <a href="https://www.shadertoy.com/view/Wcf3R8">this shader</a> which looks like this:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/misc/thumbnails/bubble_shader.png" class="img-fluid figure-img"></p>
<figcaption>shader</figcaption>
</figure>
</div>
<p>Not nearly as pretty as the real thing though - for that I’ll need to venture into the world of fluid dynamics simulations, a rabbit hole I look forward to immensely :)</p>



 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/misc/bubble_colors.html</guid>
  <pubDate>Mon, 10 Feb 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Tab Clearning (Feb 2025)</title>
  <link>https://johnowhitaker.dev/misc/tab_clear_feb2025.html</link>
  <description><![CDATA[ 




<p>Clearing some recent tabs. I’m not sure how useful these posts are to anyone else, but they help me finally read + close things I’ve been meaning to get to.</p>
<ul>
<li><a href="https://www.nature.com/articles/s41467-024-55003-5">An amazing study in nature</a> doing ‘Optical widefield nuclear magnetic resonance microscopy’, using nitrogen-vacancy-doped diamond (which flouresces in a way dependant on the surroundings) in a thin layer below a sample to get per-pixel NMR data. Wild stuff. Currently 10um resolution but easy to see that dropping further.</li>
<li><a href="https://huggingface.co/blog/open-deep-research">Recreating Deep Research</a> from huggingface. Nice to see people showing their version of search in a tool loop. I also like the look of <a href="https://github.com/jina-ai/node-DeepResearch">Jina’s version</a>.</li>
<li>New AI stuff
<ul>
<li>Gemini 2 (nice cheap good models)</li>
<li>Pika Additions, fun ‘VFX’ application of video models, hooray more control</li>
<li>Mistral’s new ‘Le Chat’ https://chat.mistral.ai/ is supoer fast (&gt;1000 tokens/sec) thanks to Cerebras and has a good canvas implementation, I like it</li>
</ul></li>
<li><a href="https://www.interconnects.ai/p/openais-o1-using-search-was-a-psyop">Search was a PSYOP</a> - between impling RLHF was the secret sauce when it was apparently mostly SFT (see Stella Bidermans tweets, Karpathy’s comments in his recent video) and then getting everyone excited about MCTS, PRMs etc when o1 was (pretty confirmed) more like r1, OpenAI’s muddying of the waters isn’t great. But TBH I think this is partly the fault of the community, being follow the magic leaders mode not pursuing things ourselves.</li>
<li><a href="https://replicate.com/minimax/video-01">First frame or subject ref -&gt; video</a> cool model</li>
<li><a href="https://personalitymap.io/">https://personalitymap.io/</a> - see correlated (well, predicted to be correlated) traits.</li>
<li><a href="https://kipp.ly/nov-dec-24/">Kipply has a ‘things read’ post</a> with some gems, e.g.&nbsp;this SA post <a href="https://www.astralcodexten.com/p/half-an-hour-before-dawn-in-san-francisco">Half an hour before dawn in san fransisco</a>, wow, what writing!</li>
<li><a href="https://ghuntley.com/dothings/">The future belongs to idea guys who can just do things</a> an interesting take from gumroad guy</li>
<li><a href="https://andysblog.uk/why-blog-if-nobody-reads-it/">Why blog if nobody reads it</a> Andy attempts to motivate, while claiming “Let’s tell the truth, then: Nobody reads your blog.”. I don’t know that I agree. I’d say ‘nobody reads your blog when you start’ - but eventually, somehow, these things have a way of finding a handful of interesting people who like what you write! Who knows, maybe your blog is one I’ll carefully add to my <a href="../blog.html">list</a> of ones I check and share.</li>
<li><a href="https://arxiv.org/abs/2502.03387">LIMO: Less Is More For Reasoning</a> another hint that today’s base models are based ;) and it doesn’t take much to squeeze more perf. out of them. Not sure I agree with the conclusions on this though, I suspect it more reflects good data in the pre-training phase rather than the magic of the post-training.</li>
<li><a href="https://gradual-disempowerment.ai/misaligned-economy">Gradual Disempowerment</a> makes a good case for this being an extra risk we should think about when talking powerful AI systems. Not misuse and power grabs (my current biggest worry) or sci-fi style ‘kill all humans’ but just the slow loss of power as people are replaced in more and more places and thus lose the ‘bargaining power’ that has historically kept institutions and states <sub>somewhat</sub> aligned with the interests of the people.</li>
<li><a href="https://marginalrevolution.com/marginalrevolution/2025/02/trumpian-policy-as-cultural-policy.html">Trumpian Policy as Cultural Policy</a> I don’t keep up much with politics but a lot of people here in Portland are somewhat traumatised by DT’s recent set of actions. This essay was an interesting lens to view them in. I should read more Tyler Cowen, and appreciate any suggestions for counterparties who might take a different view to balance him out :)</li>
</ul>



 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/misc/tab_clear_feb2025.html</guid>
  <pubDate>Fri, 07 Feb 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>First Impressions of Three New AI Models</title>
  <link>https://johnowhitaker.dev/misc/model_first_impressions.html</link>
  <description><![CDATA[ 




<section id="first-impressions-of-three-new-ai-models" class="level1">
<h1>First Impressions of Three New AI Models</h1>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>The rest of this post will have a few examples from my experimenting with some additional commentary, but first, here are my main initial impressions of these models:</p>
<ul>
<li>DeepSeek R1: A truly impressive model. Seems on par with the frontier w.r.t. coding and such, but they’ve also done well tuning the genearl tone. I’m not suprised some of my non-technical family members have switched! It’s free, it shows some of the benefits of ‘reasoning models’, it can search, it doesn’t write in a stiff and censored manner… what more could you want?</li>
<li>o3-mini-high: Another impressive coding model. Maybe not as good as Claude for style/taste when coding web apps? Maybe a little stronger than Sonnet or R1 on harder code stuff? Between these two I think the space of what you can do with one-shot asks / vibe coding is pretty darn impressive.</li>
<li>Deep Research: When it works, it’s suuuper useful. This one feels like it will bring me lots of insights + learnings that I wouldn’t otherwise get, and save a ton of time for people who’s work involves a lot of this specific kind of research. Still some amusing failure cases, but I’m glad to add this to the toolbox. Edit: some early glitches are putting me off in some cases though, keep expectations in check.</li>
</ul>
<p>All of these models are amazing technical achievements, and do genuinely push forward the bounds of what is possible. But I’m kinda sad that the direction everyone is pushing is so stringly ‘get the AI to do the whole thing’ rather than augmenting humans more. Sigh. Anyway, on to some specific examples.</p>
</section>
<section id="deep-research" class="level2">
<h2 class="anchored" data-anchor-id="deep-research">Deep Research</h2>
<p><strong>Prompt: Vitamin C crystals look cool in polarized light microscopy. I want you to come up with a way to use this to create art. Do a deep dive on what causes the colors, and how to begin to steer the process. Focus on quality sources + papers. Suggest experiments (they should be home-lab accessible) on different ways to control the appearance of what is grown. Give me a paintbrush the world hasn’t seen before!</strong></p>
<p>The <a href="https://gist.github.com/johnowhitaker/74680dc5a1bda4affa3ad4fabafa4ec2">result</a> had a ton of info, giving me terms for thigns I’d seen but not researched and suggesting some good experiments I’m planning to try. There was also one key idea not in any of the references that I <em>think</em> will work great! I did notice that a <em>lot</em> of the info came from one amazing blog post - it was nice to see this based on the references and just go read that directly. Verdict: a good start!</p>
<p><strong>Prompt (from a relative not used to AI): I want a List of top 20 Cities in the US for investing in single family home rentals ranked by the following criteria. 1. Business friendly state where laws favor landlord and evictions are easy, 2. Growing population, 3. Business growth, 4. Affordable house prices. 5. Strong rent to house value ratio. Create a 5 point scale for each of these criteria that allow them to be quantitatively evaluated. Eg. For criterion 1, a score of 1 might be least business friendly and 5 would be most business friendly. Give specific criteria for each rank in each scale (eg. A ranking of 1 for criteria 1 means x and a ranking of 2 for criterion 1 means y etc).</strong></p>
<p>Unlike other chat systems that can’t even attempt this, deep research was able to go off and run through tons of sources finding populations, stats etc. It came back with a list of 20 cities with notable features of each, and a table when asked in a follow-on. Hard to verify, but closer to what he wanted than anything else could give. It was an interesting chat: “shouldn’t this kind of thing [repetitive assessment of lots of cities in parallel] be perfect for AI?” - for now at least such things would work best with some custom code - e.g.&nbsp;I could come up with a scorecard and then in parallel run 500 research ‘agents’ with one city to research each. To him, it was obvious the AI should just do this! And I guess deep research gets a little closer via a loooong search and reasoning trajectory.</p>
<p>Verdict: not bad, also eye opening to see an ask from someone not thinking as much about what might suit this type of system.</p>
<p><strong>Prompt: I recently ran into the map folding problem. I really want to add the next number in the sequence but my code (https://github.com/johnowhitaker/map_folding/blob/main/README.md) is too slow. This needs a new approach. Please see if you can come up with a way to efficiently leverage GPU compute to solve this problem efficiently. Check your ideas with the smaller numbers in the sequence if you can write and run code. Please give this your best go. Good luck!</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/misc/thumbnails/dr_conc.png" class="img-fluid figure-img"></p>
<figcaption>The completely hallucinated conclusion of its work</figcaption>
</figure>
</div>
<p>This was hilarious. It first wrote a nice long report outlining the problem, then when pressed for an implementation I got another long report talking about all the successful (!) experiments we’d run using the code it wrote, when in actuality said code was buggy and missing key implementation details. Most amusingly, the citations were all from my own past journey down this rabbit hole! <strong>Verdict: major fail</strong>. Perhaps they steer it away from using code much, and it is better suited to finding existing answers than coming up with something novel.</p>
<p><strong>Prompt: Create a nice, approachable blog post based on this paper: https://arxiv.org/abs/2501.19393 Include images if you can, and links to related work. I want something easier for semi-technical people to consume than the paper.</strong></p>
<p>Result: meh, an OK post, with things like <code>(Imagine an infographic here comparing 60k vs 1k training examples, showing a small stack of gold-standard problems outweighing a mountain of average ones.)</code> instead of actual images.</p>
<p><strong>Update:</strong> A few more attempts this morning led to complete fails. Like saying it’ll generate figures and then simply describing what they might look like, or acting like it was starting a job (” I’ll let you know when it’s ready!“) and then just doing nothing. Each glitch makes me feel less like wanting to try more things - fastest hype-to-disappointment arc I’ve had with a model in a while.</p>
</section>
<section id="o3-mini" class="level2">
<h2 class="anchored" data-anchor-id="o3-mini">O3-MINI</h2>
<p><a href="https://x.com/johnowhitaker/status/1885515560689295826">Thread with pics</a></p>
<ol type="1">
<li><p>Facewave A fairly complex ask. Made one small mistake on the OpenPIPE API but corrected in the first follow-up message. Could use some visual tweaks but it works (apart from drawing the points wrong)! Control midi with your webcam :) App: https://tools.johnowhitaker.com/facewave_o3 Code: https://github.com/johnowhitaker/utils/tree/main/static_apps/facewave_o3 Verdict: Impressive start, not bad for 2 minutes</p></li>
<li><p>“I want you to make a p5 sketch I can paste into https://editor.p5js.org that draws a beetle. It should draw a different beetle each time it is run (use a random seed) to reflect the amazing diversity of life. Make it as good-looking as you can.” o3-mini (left) compared to me years ago (right) Code: https://editor.p5js.org/johnowhitaker/sketches/Oj6Okn8rN Verdict: runs first time, good variations, darn impressive! Quirky bugs but dang they’re obviously bugs!</p></li>
<li><p>Shader: “Make me a shader toy opalescent nudibranch. I want code I can paste in that depicts these incredible creatures in a stunning way.” First attempt I had to fix some code issues and then got a blob. ‘Make it cooler’ made some improvements. changing to -high (used in prev 2) was better especially in motion. Code: https://shadertoy.com/view/XXGBR3 Verdict: shaders are hard, meh results and maybe stuck on an older version of what shadertoy wants? (needs a more recent knowledge cutoff!) but hey I still got something with minimal fixing.</p></li>
<li><p>Synth: “I want a web page that is a synthesizer. I needs cool visuals of the sound, keyboard (AWSEDFT… keys mapped to piano notes) to play notes and should accept 2 midi CC codes using webmidi to modulate the sound. Make it epic - no simple sine waves and bland visuals, get really creative.” OK impressive, it works! Keys play sounds, having facewave open in another tab (with loopMIDI running) modulates filters/sound quality somehow. Not bad for first try, this thing is pretty good at zero-shot software making!</p></li>
<li><p>A nice enabling utility for today’s Saturday making: an SVG-to-GCODE tool for a hanging pen plotter. Got it first time (I made a couple of easy tweaks). Prompt: “I want you to make an SVG-to-GCODE tool for a hanging pen plotter. The user chooses and uploads an SVG containing one or more poly-lines. They specify how far apart the two stepper motors are, a ‘scale factor’ (how much string is let out based on sending a command to move 1mm, dependant on pulley size, steps per rotation, gearing and so on), and the starting length of the strings (i.e.&nbsp;how much string has been let out at the ‘0’ position. Assume the steppers are driven with a control board that assumes a cartesian 3D printer - i.e.&nbsp;GCode to move +10mm in both X and Y will result in both motors turning the same amount. For cases with more than one poly-line, have user-configurable ‘pen up’ and ‘pen down’ GCODE strings. Make a preview section that actually simulates such a configuration accurately to render the drawing as it would be done by the bot. Include an option for scaling the SVG to determine it’s size, assume the center of the SVG is (0, 0). Provide the code as a standalone HTML file (you can use js and CSS from a CDN of course). Good luck!” The visual is important - make it easy to check if something works. If this translates to the robot you’ll see pictures later today after a hardware store opens :)</p></li>
</ol>
</section>
<section id="deepseek-r1" class="level2">
<h2 class="anchored" data-anchor-id="deepseek-r1">DeepSeek R1</h2>
<section id="d-printer-control-over-usb-win" class="level3">
<h3 class="anchored" data-anchor-id="d-printer-control-over-usb-win">3D Printer Control over USB (win)</h3>
<p>Prompt: <strong>I have an ender 3 v3 se 3D printer plugged into my linux laptop via USB C cable. It shows up as ‘Bus 001 Device 008: ID 1a86:7523 QinHeng Electronics CH340 serial converter’ when I do <code>lsusb</code>. The stepper motors are currently disabled and set to what I’d like the 0 position to be. Give me python code that, when run, lets me move the position using WASD and Q/E for up/down. Include a visual display of what is happening. Make the code clear and modular enough that I can reuse pieces of it. Use pygame for the GUI.</strong></p>
<p>It nailed the script on first try (!) and gave good instructions for running it. And yet the serial device wasn’t showing up where I (or R1) expected it. It gave good debugging steps, and things that would fix it. But missed something I found with a quick search + skim of <a href="https://askubuntu.com/questions/1403705/dev-ttyusb0-not-present-in-ubuntu-22-04">this askubuntu.com thread</a>: removing <code>brltty</code> fixes it (a Braille screen reader uses the same product ID as this chip). A very odd and annoying and niche issue - finding others who’d had the same problem gave the fastest fix in this case. Still, chalking this up as a win for R1 - I now have code that would get me started doing exactly what I want, with a working PoC, in almost no time.</p>
</section>
<section id="asking-car-questions-i-mostly-know-the-answer-to-win" class="level3">
<h3 class="anchored" data-anchor-id="asking-car-questions-i-mostly-know-the-answer-to-win">Asking car questions I mostly know the answer to (win)</h3>
<p><strong>Explain concisely to me how to check oil and coolant in my 2014 Buick encore</strong></p>
<p>Perfect response. I’m not a car guy but asking followups and comparing to the car manual it’s all correct. If you knew nothing this would probably help you on simple stuff and be pretty reliable. But also, just read the manual :)</p>
</section>
<section id="conding-help-on-ai-research-win" class="level3">
<h3 class="anchored" data-anchor-id="conding-help-on-ai-research-win">Conding help on AI research (win)</h3>
<p><strong>I’m working on a notebook exploring the GRPO loss function, which includes a KL divergence term comparing the model being trained to a reference model. Here’s the code from my explorations so far, please give me some code and explanation for how to measure the KL divergence given the output text, the model and the reference model as I have them here. Make it clear and didactic, and suitable for exploratory notebook programming. [code…]</strong></p>
<p>It gave me an implementation of KL divergence that worked with the code I already had. The KL divergence it measured was considering all possible tokens, not just the selected tokens, which seemed to differ from what the implementation I was looking at does. This cased a double-take, although was arguably more likely that I’d miss this if I wasn’t paying attention. It turns out the code I was looking at used <a href="http://joschu.net/blog/kl-approx.html">this approximation</a>. Talking this through with R1 was very helpful for understanding this - it could look at the code I was examining, confirm my reading of it and that it wasn’t normal KL, then I could link the approzimation page, and it could convert the code to formulas and explain how it is indeed doing that approximation and why it’s useful. This feels like having a super smart assistant, rather than prev AI models which can get a bit hallucinatory when it comes to harder stuff or new, research code. I asked it some follow-ups on related questions about GRPO and it was super helpful there too, noticing links to PPO and REINFORCE and explaining how they’re related to the code I was examining.</p>
</section>
<section id="exploring-a-new-model-release" class="level3">
<h3 class="anchored" data-anchor-id="exploring-a-new-model-release">Exploring a new model release</h3>
<p><strong>Summarize the new Qwen2.5 VL release and explain how it stacks up against existing models.</strong></p>
<p>“Oops! DeepSeek is experiencing high traffic at the moment. Please check back in a little while.”. They did admirably keeping up with a huge surge in global demand, but did still drop out during the days of peak interest.</p>
</section>
<section id="writing-a-blog-post" class="level3">
<h3 class="anchored" data-anchor-id="writing-a-blog-post">Writing a blog post</h3>
<p>As a test (we’re talking about dos and don’ts for AI-assisted writing, spoiler I think mostly don’t) I gave it four bullet points and a few pointers and it turned it into a very well-written and structured post - some of the nicest writing I’ve seen from a chatbot.</p>


</section>
</section>
</section>

 ]]></description>
  <category>misc</category>
  <guid>https://johnowhitaker.dev/misc/model_first_impressions.html</guid>
  <pubDate>Tue, 04 Feb 2025 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Vibe Coding and V-Plotters</title>
  <link>https://johnowhitaker.dev/essays/vibe_coding.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/vibe_coding.png" class="img-fluid figure-img"></p>
<figcaption>‘vibe coding is already here’</figcaption>
</figure>
</div>
<p>A <a href="https://x.com/karpathy/status/1886192184808149383">Karpathy tweet</a> sparked a bunch of discussion about ‘vibe coding’ - his term for completely embracing AI coding assistance where “it’s not really coding - I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.”. He clarifies that this is still only good enough for “throwaway weekend projects” but it’s clear that many think of this as the obvious future of coding. So - is it? And if so, is that a good thing? I think my weekend hobby coding helps illustrate why I hope that the old fashioned way sticks around.</p>
<section id="what-plotters" class="level2">
<h2 class="anchored" data-anchor-id="what-plotters">What-plotters?</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/plot_portrait.png" class="img-fluid figure-img"></p>
<figcaption>A plotted self-portrait</figcaption>
</figure>
</div>
<p>The project in question was a ‘V-Plotter’ - a device that draws by moving a pen/sharpie around at the end of two strings or chains, wound in and out by a pair of motors under computer control. Building one was on my project list, but I’d been putting off writing the necessary software. So, when o3-mini was released and its coding capabilities lauded, I threw it a challenge: write a web app that would take in an SVG and spit out the G-CODE that would control my motors correctly to draw the image in question.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/plot_preview.png" class="img-fluid figure-img"></p>
<figcaption>The preview in the vibe-coded webapp</figcaption>
</figure>
</div>
<p>It spat out a <a href="https://tools.johnowhitaker.com/svg2g">functional-seeming site</a>, and so I cobbled together the necessary hardware and attempted a plot. Apart from being rotated 90 degrees, the results seemed to indicate that it was working as hoped! A project that I’d been idly thinking about for years had materialized before breakfast on a Saturday - vibe coding for the win!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/plotter.png" class="img-fluid figure-img"></p>
<figcaption>The plotter itself</figcaption>
</figure>
</div>
</section>
<section id="a-question-begets-a-question" class="level2">
<h2 class="anchored" data-anchor-id="a-question-begets-a-question">A Question Begets a Question</h2>
<p>I shared the first test in a group with some folks from my local hackerspace, and a fellow member responded asking if we could chat further - turns out the too had long wanted to build such a device. I agreed to meet, but felt an instant pang of <em>something</em> - what would I tell this guy? “Oh yeah I kinda know how it works but actually I just had AI write all the code?” Bah! Haven’t I spent the last few months teaching a course on doing things carefully, iteratively, in small pieces, with AI supporting but a human in charge? There was only one thing to do.</p>
</section>
<section id="another-way" class="level2">
<h2 class="anchored" data-anchor-id="another-way">Another Way</h2>
<p>I started from a clean slate - a Jupyter notebook and a plan. With AI assistance, I started building up pieces bit by bit, sprinkling in some explanation. Can I load an SVG? What do lines and polylines look like? Can I turn those into sequences of moves? Can I shift and scale these move sequences? And then, once I was ready, the crux: how do you turn a set of X,Y positions into string length commands that would make sense for the plotter? What scary maths would one need to know?</p>
<p>Prompt: Please write function c_t_v that takes the plotter params and a set of moves defined in cartesian space, and converts them such that they’ll work on the plotter. Test it with the square from before.</p>
<p>With all the other scaffolding in place, the new function was only a few lines of code. And looking at it, with plots and examples in my own ‘context window’, it was immediately obvious how that code works. It’s just two triangles!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/essays/plot_kinematics.png" class="img-fluid figure-img"></p>
<figcaption>plot kinematics</figcaption>
</figure>
</div>
<p>This was code I could share with and explain to my soon-to-be-new-friend. This was something I could write a tutorial on. This was something I could code again in 2 years time when I decide I need to build a better plotter. In other words, ‘inverse kinematics for hanging plotters’ is now something I <strong>understand</strong>.</p>
</section>
<section id="the-moral-of-the-story" class="level2">
<h2 class="anchored" data-anchor-id="the-moral-of-the-story">The Moral Of The Story</h2>
<p>There are so many use-cases for instant, throwaway software. So many things now in reach for people who are new to coding or time constrained. Vibe coding is amazing, and powerful, and I enjoy it a lot! But there’s something delightful about figuring out something for yourself that I’m worried people might miss out on. Because behind the intimidating syntax and the boilerplate there’s often something extremely elegant and satisfying, which our puny human minds are more than capable of understanding. I hope that as we develop these tools even further, we keep in mind the value of wasting an hour or two trying to do things the human way.</p>
<p>Yours, humanly, Johno</p>


</section>

 ]]></description>
  <category>essays</category>
  <category>mini-hw-projects</category>
  <guid>https://johnowhitaker.dev/essays/vibe_coding.html</guid>
  <pubDate>Mon, 03 Feb 2025 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
