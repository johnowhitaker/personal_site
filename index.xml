<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>johnowhitaker.dev</title>
<link>https://johnowhitaker.dev/</link>
<atom:link href="https://johnowhitaker.dev/index.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.6.11</generator>
<lastBuildDate>Mon, 19 Aug 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Computational Linguistics</title>
  <link>https://johnowhitaker.dev/dsc/computational-linguistics.html</link>
  <description><![CDATA[ 




<p>LLMs are cool - we can make them do all sorts of tricks. But beyond the generative applications there’s a whole bunch of interesting linguistic analysis we can do, especially with base models that are essentially this very compressed representation of the world of text. In this post I wanted to show a few quick experiments I did, in the hope that they might inspire you to try something similar.</p>
<section id="what-time-is-afternoon" class="level2">
<h2 class="anchored" data-anchor-id="what-time-is-afternoon">What time is ‘Afternoon’?</h2>
<p>It occurred to me the other day that I could use LLMs to disambiguate the meaning of ‘afternoon’, and other phrases (‘a dozen’, ‘soon’ etc). For example, we can think up some sentences that both specify that something is happening in the afternoon and then formalize the time. By plotting the likelihood of different times, we can see how the model interprets the word:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/dsc/images/afternoon.png" class="img-fluid figure-img"></p>
<figcaption>what time is afternoon</figcaption>
</figure>
</div>
<p>Pretty neat! It turns out ‘afternoon’ is most likely to be around 3pm.</p>
</section>
<section id="dinner-time" class="level2">
<h2 class="anchored" data-anchor-id="dinner-time">Dinner Time:</h2>
<p>We can do this same thing for meal times:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/dsc/images/meal_times.png" class="img-fluid figure-img"></p>
<figcaption>plot for different meal times</figcaption>
</figure>
</div>
<p>Notice I chose the ranges for the charts carefully. It turns out this method of looking at liklihood gets messed up a little when things tokenize into multiple tokens. For example, for ‘breakfast’ 10am might be unlikely BUT after a 1 is generated the options for the token after that are 0, 1 or 2, with 0 being very likely. So the average liklihood of “10am” is high, even though it’s not a likely time for breakfast. Not to mention the variants like ‘noon’, 13:00, etc. So, take this with a grain of salt!</p>
</section>
<section id="probing-deeper" class="level2">
<h2 class="anchored" data-anchor-id="probing-deeper">Probing deeper</h2>
<p>One alternative is to probe some (fixed) token as our measure, and change other parts of the context. For example, let’s see which accompaniment to “tea” is best by looking at the likelihood of “lovely” at the end of this prompt: “Shall we have some tea and {X}? That would be lovely”.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://johnowhitaker.dev/dsc/images/tea_and_x.png" class="img-fluid figure-img"></p>
<figcaption>plot for different accompaniments to tea</figcaption>
</figure>
</div>
<p>Biscuits are the clear winner! Speaking of, I’m off to make a cuppa and grab a biscuit. Until next time!</p>
</section>
<section id="ps-exploring-this-yourself" class="level2">
<h2 class="anchored" data-anchor-id="ps-exploring-this-yourself">PS: Exploring this yourself</h2>
<p>Here’s some code to get you started:</p>
<p>Load the model:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> torch</span>
<span id="cb1-2"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> transformers <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> AutoModelForCausalLM, AutoTokenizer</span>
<span id="cb1-3"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> datasets <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> load_dataset</span>
<span id="cb1-4"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> torch.nn.functional <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> softmax</span>
<span id="cb1-5"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> tqdm.auto <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> tqdm</span>
<span id="cb1-6"></span>
<span id="cb1-7">device <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"cuda"</span></span>
<span id="cb1-8">model_id <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"meta-llama/Meta-Llama-3-8B"</span></span>
<span id="cb1-9">tokenizer <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoTokenizer.from_pretrained(model_id)</span>
<span id="cb1-10">tokenizer.pad_token <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer.eos_token</span>
<span id="cb1-11">model <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> AutoModelForCausalLM.from_pretrained(</span>
<span id="cb1-12">    model_id,</span>
<span id="cb1-13">    torch_dtype <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.bfloat16</span>
<span id="cb1-14">)</span>
<span id="cb1-15">model.to(device)<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">;</span></span></code></pre></div>
<p>Sample:</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"""Shall we meet this afternoon?</span><span class="ch" style="color: #20794D;
background-color: null;
font-style: inherit;">\n</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">Sure - I'll see you at"""</span></span>
<span id="cb2-2">candidates <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> [<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" 1pm"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" 2pm"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" 3pm"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" 4pm"</span>, <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" 5pm"</span>]</span>
<span id="cb2-3">text_inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer(text, return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).input_ids.to(device)</span>
<span id="cb2-4">candidate_inputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> tokenizer([text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> c <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> candidates], return_tensors<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"pt"</span>, padding<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>, truncation<span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span><span class="va" style="color: #111111;
background-color: null;
font-style: inherit;">True</span>).input_ids.to(device)</span>
<span id="cb2-5"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">print</span>(text_inputs.shape, candidate_inputs.shape)</span>
<span id="cb2-6">model_outputs <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model(candidate_inputs)</span>
<span id="cb2-7">logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> model_outputs.logits</span>
<span id="cb2-8">scores <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> []</span>
<span id="cb2-9"><span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, c <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(candidates):</span>
<span id="cb2-10">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate perplexity of the completion</span></span>
<span id="cb2-11">    completion_ids <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> candidate_inputs[i].unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-12">    completion_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> logits[i].unsqueeze(<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>)</span>
<span id="cb2-13">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Start is start of completion</span></span>
<span id="cb2-14">    start <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> text_inputs.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># End is where padding starts</span></span>
<span id="cb2-16">    end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion_ids.shape[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">1</span>]</span>
<span id="cb2-17">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">range</span>(start, end):</span>
<span id="cb2-18">        <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">if</span> completion_ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, i] <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">==</span> tokenizer.pad_token_id:</span>
<span id="cb2-19">            end <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> i</span>
<span id="cb2-20">            <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">break</span></span>
<span id="cb2-21">    token_indices <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion_ids[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, start:end]</span>
<span id="cb2-22">    token_logits <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> completion_logits[<span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span>, start:end]</span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># Calculate perplexity</span></span>
<span id="cb2-24">    perplexity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> <span class="dv" style="color: #AD0000;
background-color: null;
font-style: inherit;">0</span></span>
<span id="cb2-25">    <span class="cf" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">for</span> i, t <span class="kw" style="color: #003B4F;
background-color: null;
font-weight: bold;
font-style: inherit;">in</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">enumerate</span>(token_indices):</span>
<span id="cb2-26">        perplexity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">-=</span> token_logits[i, t]</span>
<span id="cb2-27">    perplexity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> perplexity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">/</span> <span class="bu" style="color: null;
background-color: null;
font-style: inherit;">len</span>(token_indices)</span>
<span id="cb2-28">    perplexity <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">=</span> torch.exp(perplexity)</span>
<span id="cb2-29">    scores.append(perplexity.item())</span>
<span id="cb2-30"></span>
<span id="cb2-31"><span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">from</span> matplotlib <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">import</span> pyplot <span class="im" style="color: #00769E;
background-color: null;
font-style: inherit;">as</span> plt</span>
<span id="cb2-32">plt.bar(candidates, scores)</span>
<span id="cb2-33">plt.title(text <span class="op" style="color: #5E5E5E;
background-color: null;
font-style: inherit;">+</span> <span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">" ..."</span>)</span>
<span id="cb2-34">plt.xlabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Completion"</span>)</span>
<span id="cb2-35">plt.ylabel(<span class="st" style="color: #20794D;
background-color: null;
font-style: inherit;">"Score"</span>)</span></code></pre></div>


</section>

 ]]></description>
  <category>Blog</category>
  <guid>https://johnowhitaker.dev/dsc/computational-linguistics.html</guid>
  <pubDate>Mon, 19 Aug 2024 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/dsc/images/meal_times.png" medium="image" type="image/png" height="82" width="144"/>
</item>
<item>
  <title>What You Should Read (AI Edition)</title>
  <link>https://johnowhitaker.dev/misc/what_you_should_read.html</link>
  <description><![CDATA[ 




<section id="what-you-should-read-ai-edition" class="level1">
<h1>What You Should Read (AI Edition)</h1>
<p>There have been hundreds of thousands of films made. But if you reveal that you haven’t seen Star Wars, you’re regarded with a mixture of sympathy and scorn. In this listicle, I’m going to attempt to give you a short list of essentials to spare you the same fate in the field of AI research. This is off the top of my head, so I’ll definitely miss things and get some timelines wrong. Let me know if there are things you think I absolutely must add. I’ve split things up into a few different categories to keep the story flowing.</p>
<p>See also: supposedly the list Ilya sent to Carmack: https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE</p>
<p>Most of this list was written ~6 months ago to send to a friend, sharing in case I get asked for it again.</p>
<section id="vision" class="level2">
<h2 class="anchored" data-anchor-id="vision">Vision</h2>
<p>Let’s start with how computers see. Back in the old days, computer vision was a hard task dominated by carefully hand-crafted features in controlled conditions. A forward-thinking Fei-Fei Li set a near-impossible-seeming task: learn to classify ~1M images into ~1000 classes. Some nerds figured out how to build convolutional neural networks in a way that let them use GPUs for training and got a much better score than any prior approach. Soon deep learning was a hot topic and more and more researchers fought for the top spot on the imagenet accuracy leaderboards.</p>
<p>You don’t need to read every paper that claimed a 0.1% improvement. I’d recommend picking any ‘intro to convolutional nns’ tutorial to get the basics then following the main improvements: - The original ResNet paper (https://arxiv.org/abs/1512.03385) showed how using residual connections makes it possible to train much deeper networks</p>
<ul>
<li><p>MobileNets (https://arxiv.org/abs/1704.04861) introduced “depth-wise separable convolutions to build light weight deep neural networks.” which made them popular for deployment on lower-power devices.</p></li>
<li><p>EfficientNet (https://arxiv.org/abs/1905.11946) took this further and was a fan favourite for a while in terms of performance and efficiency</p></li>
<li><p>When transformer models started to get popular (see the LLM section), the VIT paper (https://arxiv.org/abs/2010.11929) fed patches of an image into a transformer and got extremely good results, kicking off a war between the convolutionists and the transformacons that continues to this day.</p></li>
<li><p>ConvNeXt (https://arxiv.org/abs/2201.03545) said ‘hey let’s take some good ideas from ViTs and elsewhere and see if we can make a better convnet for the 2020s.’</p></li>
<li><p>MLP-Mixer (a personal fave, less pivotal) said ’who needs attention or convolutions? Just make sure there’s some way to mix across channels (like the MLPs in a ViT) and some way to mix across space (like the attention in a ViT or the conv kernels in a convnet). I love that it works - hooray scaling and the bitter lesson :)</p></li>
</ul>
<p>ViTs are probably the go-to these days, although there are attempts to fix some of their flaws (fixed size, need lots of compute especially for high-res images, less priors baked in so well-suited to data-rich regimes) - but most of the modifications proposed sort of make sense and also don’t make <em>that</em> big of a difference compared to scaling. If you want more on them maybe read “Scaling Vision Transformers” (https://arxiv.org/abs/2106.04560) and something like the Hierarchical ViT paper (https://arxiv.org/abs/2205.14949).</p>
<p>While people were duking it out for the classification crown, there were a few other things happening - A medical segmentation paper proposed the UNet architecture that turned out to be pretty good for anything that needs an image-shaped output (like segmentation) - https://arxiv.org/abs/1505.04597</p>
<ul>
<li><p>People figured out how to do object detection, although there ended up being tons of different ways to finagle the data and at least 8 papers with different architectures using the name YOLO. If you care about object detection probably just check what the most recent one is that everyone seems to use.</p></li>
<li><p>People found that a model trained on imagenet could then be fine-tuned for some new task using very few images, in a process called “transfer learning”. See the first lesson of fast.ai to get excited about this and to see how easy it can be. You should check out this 2017 work exploring what these models learn: https://distill.pub/2017/feature-visualization/</p></li>
</ul>
<p>There’s also the big question of labels. Imagenet is all well and good, but if we want to scale up more can we find ways to learn without class labels?</p>
<ul>
<li><p>Contrastive learning: two images of the same thing (or, pragmatically, two transforms of the same image) should map to similar features. Unrelated images should map to less-similar features. SimCLR “A Simple Framework for Contrastive Learning of Visual Representations” (https://arxiv.org/abs/2002.05709) is a goodie.</p></li>
<li><p>MAEs “Masked Autoencoders Are Scalable Vision Learners” (https://arxiv.org/abs/2111.06377) - what if we instead learn to predict a masked-out region of an image? Turns out at scale this is enough to learn useful features. Lots of fun overlap between MAEs and generative models too…</p></li>
<li><p>iJEPA “Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture” (https://arxiv.org/abs/2301.08243) Yann thinks there’s a better way, we predict the <em>embedding</em> of the target not the target itself. JEPA is an interesting line of research.</p></li>
<li><p>CLIP (https://arxiv.org/abs/2103.00020) - a contrastive approach that maps images and text to the same space (ish). Let’s you learn from billions of captioned images on the web. Gives an incredibly useful way to get features from images and text that you can use for 0-shot classification, search, conditioning generative models… one of the most impactful vision papers IMO. Lots of derivatives, SigLIP etc improving on the core idea, OpenCLIP project with tons of models… Datacomp is an interesting one, asking ‘what data should you use for a clip-like thing if the model + compute is fixed?’</p></li>
</ul>
<p>Finally, there’s the question of how we generate images. Can we just run a convnet backwards? Not quite, but:</p>
<ul>
<li><p>VAEs: papers can be very math-heavy. https://arxiv.org/abs/1906.02691 is a 2019 paper by D. Kingma and Max Welling who also did an important 2013 paper https://arxiv.org/abs/1312.6114. I think maybe skip both, maybe go for a more accessible intro like https://deeplearning.neuromatch.io/tutorials/W2D4_GenerativeModels</p></li>
<li><p>Generative Adversarial Networks (https://arxiv.org/abs/1406.2661) introduce a great idea: use a second network trying to tell the diff between the output of the first network and real data. GAN literature got full of fiddly tricks and a mythical feeling that these beasts are hard to train.</p></li>
<li><p>BigGAN (https://arxiv.org/abs/1809.11096) scaled up and showed class conditioning. StyleGAN (https://arxiv.org/abs/1812.04948) learned ‘disentangled’ features and gave amazing control and trippy interpolations. light-weight GAN (https://arxiv.org/abs/2101.04775) is my go-to for something you can train on a relatively small dataset with all the modern tricks. And more recently GigaGAN (https://arxiv.org/abs/2303.05511) flexed fast text-to-image (meh) and super-resolution (incredible).</p></li>
<li><p>A Neural Algorithm of Artistic Style (https://arxiv.org/abs/1508.06576) came up with the cool idea of style transfer. My course has some more modern approaches https://johnowhitaker.github.io/tglcourse/representations.html</p></li>
<li><p>Taming Transformers for High-Resolution Image Synthesis (https://arxiv.org/abs/2012.09841) aka the VQGAN paper showed how to tokenize images and also set us up for latent diffusion and the fun we had optimizing VQGAN latents with CLIP (https://johnowhitaker.github.io/tglcourse/generators_and_losses.html)</p></li>
<li><p>Dalle (https://arxiv.org/abs/2102.12092) modelled images and text as sequences - just learn to predict the next token in a sequence that looks like [text caption… image patch tokens]. Parti (https://sites.research.google/parti/) scaled it up and how weird that worked!</p></li>
</ul>
<p>Diffusion models stole the show though</p>
<ul>
<li><p>Imagen and Dalle 2 showed off high-quality (closed)</p></li>
<li><p>Stable Diffusion (https://arxiv.org/abs/2112.10752) gave us open-source stuff, newer versions track trends in what seems to work</p></li>
<li><p>InstructPix2Pix (https://arxiv.org/abs/2211.09800) used synthetic data to get a model that can do image + text -&gt; edited image. Emu Edit did more data.</p></li>
<li><p>Personalization happened (Dreambooth (https://arxiv.org/abs/2208.12242), Textual Inversion (https://arxiv.org/abs/2208.01618), ZipLoRA(https://arxiv.org/abs/2311.13600) are some standouts)</p></li>
<li><p>Controlnet (https://arxiv.org/abs/2302.05543) and IPAdapter (https://arxiv.org/abs/2308.06721) added extra ways to control the generation, as did many others</p></li>
<li><p>Making them fast w/ distillation, score matching, flow, …. It gets crowded and complicated here. Progressive Distillation (https://arxiv.org/abs/2202.00512) was an early big one.</p></li>
</ul>
<p>BTW diffusion models learn useful features for other tasks, there’s a whole bunch of stuff too much to cover here.</p>
</section>
<section id="language-wip" class="level2">
<h2 class="anchored" data-anchor-id="language-wip">Language (WIP)</h2>
<p>TODO: Synthetic data Textbooks are all you need (tinystories) Orca, evol-instruct, restructured Pretraining Optimizers, training dynamics etc Place for adam, layernorm, grad clipping, LR scheduling, EMA, …</p>
<section id="the-early-days" class="level3">
<h3 class="anchored" data-anchor-id="the-early-days">The Early Days</h3>
<ol type="1">
<li><p><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> (2015) - Karpathy’s great blog post on character-level RNNs.</p></li>
<li><p><a href="https://arxiv.org/abs/1801.06146">ULMFiT: Universal Language Model Fine-tuning for Text Classification</a> (2018) - Demonstrated transfer learning for text. “Pretraining” becomes a thing.</p></li>
</ol>
</section>
<section id="the-rise-of-transformer-based-models" class="level3">
<h3 class="anchored" data-anchor-id="the-rise-of-transformer-based-models">The Rise of Transformer-based Models</h3>
<ol start="3" type="1">
<li><p><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a> (2017) - Introduced the Transformer architecture for translation.</p></li>
<li><p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> (2018) - Bidirectional training of Transformers.</p></li>
<li><p><a href="https://arxiv.org/abs/1910.10683">T5: Exploring the Limits of Transfer Learning</a> (2020) - Unified text-to-text framework.</p></li>
<li><p><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a> (2018) - Introduced GPT. “We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task”</p></li>
<li><p><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners</a> (2019) - Introduced GPT-2. “We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.”</p></li>
<li><p><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a> (2020) - Introduced GPT-3. “Here we show that scaling up language models greatly improves task-agnostic, few-shot performance”</p></li>
<li><p><a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> (2020) - Empirical analysis of scaling relationships.</p></li>
<li><p><a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> (2022) - Chinchilla gave better scaling laws for how to get best performance at different scales (without considering inference costs).</p></li>
</ol>
<p>I like how the GPT series of papers show the progression from unsupervised pretraining to few-shot learning, as we realize how much this paradigm can do.</p>
</section>
<section id="efficient-fine-tuning-and-adaptation" class="level3">
<h3 class="anchored" data-anchor-id="efficient-fine-tuning-and-adaptation">Efficient Fine-tuning and Adaptation</h3>
<ol start="11" type="1">
<li><p><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a> (2021) - Efficient fine-tuning method. After this there was a flurry of LoRA variants since it is something people can research on an academic budget. Most can safely be ignored. I like ‘DoRA’ as a better-performing version and LoftQ for quantization-aware LoRA stuff (see also FA-LoRA I think it’s called).</p></li>
<li><p><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a> - Efficient fine-tuning with quantization. By the kegend Tim Dettmers, made fine-tuning with quantization practical for so many more people. Check out the answerai posts on this topic for more on scaling and quantization.</p></li>
</ol>
</section>
<section id="instruction-tuning-and-alignment" class="level3">
<h3 class="anchored" data-anchor-id="instruction-tuning-and-alignment">Instruction Tuning and Alignment</h3>
<ol start="13" type="1">
<li><p><a href="https://arxiv.org/abs/2203.02155">InstructGPT: Training language models to follow instructions</a> (2022) - Instruction-following using human feedback.</p></li>
<li><p><a href="https://arxiv.org/abs/2212.08073">Constitutional AI: Harmlessness from AI Feedback</a> (2022) - AI-assisted approach to alignment.</p></li>
<li><p><a href="https://arxiv.org/abs/2305.18290">Direct Preference Optimization: Your Language Model is Secretly a Reward Model</a> (2023) - Simplified approach to RLHF.</p></li>
<li><p><a href="https://arxiv.org/abs/2310.16944">Zephyr: Direct Distillation of LM Alignment</a> (2023) - A good recipe for making ‘aligned’ models based on synthetic data from big models.</p></li>
<li><p>Tulu and <a href="https://arxiv.org/abs/2311.10702">Tulu 2</a> applying this recipe and exploring what data works well. Decent emperical papers, lots of insights about things like length skewing LLM-judged scores.</p></li>
</ol>
<p>As with LoRA, there was a flurry of DPO variants. See <a href="https://www.youtube.com/watch?v=YNOIyvUCpAs&amp;t=14290s">this video of mine</a> for a chat about some of the modifications and why some at leat are useful.</p>
</section>
<section id="multi-modal" class="level3">
<h3 class="anchored" data-anchor-id="multi-modal">Multi-Modal</h3>
<ol start="18" type="1">
<li><p><a href="https://arxiv.org/abs/2204.14198">Flamingo: a Visual Language Model for Few-Shot Learning</a> - early good VLM. Idefics is open source version.</p></li>
<li><p><a href="https://arxiv.org/abs/2407.07726">PaliGemma</a> - shows the approach of glueing a pretrained vision encoder (siglip in this case) to an existing language model (gemma in this case) to get a multi-modal model. Not the first to do it but a nice decent recent paper.</p></li>
<li><p><a href="https://arxiv.org/abs/2405.09818">Chameleon</a> (and <a href="https://arxiv.org/abs/2407.21770">MoMa</a> - the efficiency upgrade with MoE of Chameleon). From Meta, good look at how early fusion models might end up looking.</p></li>
</ol>


</section>
</section>
</section>

 ]]></description>
  <category>Blog</category>
  <guid>https://johnowhitaker.dev/misc/what_you_should_read.html</guid>
  <pubDate>Wed, 14 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Tab Clearing</title>
  <link>https://johnowhitaker.dev/misc/tab-clearing.html</link>
  <description><![CDATA[ 




<section id="notes-as-i-clear-my-browser-tabs-reading-list" class="level1">
<h1>Notes as I clear my browser tabs / reading list</h1>
<p>Writing down quick takeaways/impressions as I clear up some of my reading backlog</p>
<p><img src="https://j.gifs.com/mLlYoY.gif" class="img-fluid"></p>
<section id="diffusionimage-stuff" class="level2">
<h2 class="anchored" data-anchor-id="diffusionimage-stuff">Diffusion/Image Stuff</h2>
<p><a href="https://blog.fal.ai/introducing-aurasr-an-open-reproduction-of-the-gigagan-upscaler-2/">AuraSR</a> is a nice, GAN-based super resolution model that does 4x upscaling. Based on Adobe’s GigaGAN but released under a open source license. This is a really nice image upscaling model.</p>
<p><a href="https://sander.ai/2024/06/14/noise-schedules.html">Noise schedules considered harmful</a>: Fantastic posts from Sander. Diving into noise schedules and why it might make more sense to instead look at relative noise. Waiting during training and choice of timestep/ noise level during sampling as two independent things and stopped focusing on this. Needless abstraction of time steps or noise schedules. Fantastically written clare lines up with exactly how I view this. No City math. Definitely a recommended read.</p>
<p><a href="https://srush.github.io/DiffRast/">DiffRast</a>: A fun expiration of some of the things you can do with differentiable graphics, by Sasha Rush. This is exactly the type of thing that I would spend weeks playing with. If I was in my tinkering with AI art face. It is using Jax. I want to come back and give this a proper look at some point.</p>
<p><a href="https://xavierchen34.github.io/MimicBrush-Page/">MimicBrush</a>: They have nailed reference-based in painting. Amazing results. The trick is to construct training data from pairs of video frames. Learn to fill a masked region of one frame with info from another -&gt; learn to use a reference image to fill gaps, without copying its structure.</p>
<p><a href="https://fudan-generative-vision.github.io/hallo/#/">Hallo</a>: yet another audio-driven face animation thing, ok I guess. Controlnet-style way to feed in the reference image, “temporal attention”, bla bla bla</p>
<p><a href="https://gojasper.github.io/flash-diffusion-project/">Flash Diffusion</a>: A diffusion distillation method that has the students predicting One step what the teacher predicts in multiple and also uses an adversarial loss. Seems like a decent diffusion distillation paper, but nothing particularly novel just skimmed</p>
<p><a href="https://rb-modulation.github.io/">RB-Modulation</a>: Impressive results on custom content and style via reference images with diffusion models. Nathaniel Ruiz of dream Booth and ziplora among many others supervised. The results are amazing but the paper is almost impossible to understand. Far too much technical jargon and acronyms. I did not bother trying to go deep on it. The tldr is something like ‘we mess with the attention to include features from the style or content images and we have a way of disentangling the two. The features are somehow persist separately and then combined allowing us to reweight things or adjust how and where the influence from content or style applies’. But there is also lots of nonsense about optimal control and stochastic bloody blast.</p>
</section>
<section id="llm-stuff" class="level2">
<h2 class="anchored" data-anchor-id="llm-stuff">LLM stuff</h2>
<p><a href="https://thesephist.com/posts/prism/">Prism</a>: very fun work that is similar in some ways to anthropics sparse Auto encoders work. It identifies vectors or directions in feature space or in embedding space. If you will that are that that represents atomic concepts around language and then explores using these to edit text. So for example, they identify some semantic directions like casual/formal or becoming a question or whatever, and then have a auto encoder style thing that can take in some text and produce a vector and then produce text from that vector and then they find ways to edit the embedding based on these identified features. So you could for example, make some text more formal or make it a question instead of a statement. There’s a lot in the post that I didn’t go into too deeply, but it seems like a very nice exploration of practical applications for the kind of mechanistic interpretability stuff we looked at with andthics paper. I look forward to his future work</p>
<p><a href="https://blog.google/technology/developers/google-gemma-2/">Gemma 2</a>. <a href="https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf">Technical report</a> the Gemma 2 model is a sizable improvement over the original Gemma model. The 9B now seems like the best in its weight class beating out Llama 3 8b. The 27b version is almost as good as llama 70B on some measures the best in its size class, but also unclear if it is with it to move from Llama 3 70B for the kinds of applications, this would be used for. Interestingly, they explored distillation as a training process for the smaller models. At least they found a substantial boost compared to normal pre-training. They also did model averaging where they took the SFT version and the RLHF version and averaged the weights. Overall the model looks pretty decent. Nothing too crazy. Architecture wise they used grouped query attention, mix of local and global attention. It seems like a pretty efficient and performant model.</p>
<p><a href="https://sakana.ai/llm-squared/">Sakana’s LLM Squared</a>: Using llms to evolve codes to train. LLMs sounds very fancy and futuristic, but I was a little bit underwhelmed by what they actually did. Even though it was still pretty cool. They are using llms as code mutation to “evolve” the loss function for something like DPO so preference optimization. They generate lots of candidate algorithms and evaluate them and pick the best and then show that sure enough and it also does well on other similar evals. The whole thing is a little iffy since I know these algorithms can be finicky and it’s all preference-based with MT-bench and alpaca eval, but still cool that they were able to improve over DPO. We’ll see how it goes and if they can evolve code for other parts of the stack which seems a lot harder to optimize for and measure, but still cool to see people trying fun things like this. I can never resist evolution or computation.</p>
<p><a href="https://digirl-agent.github.io/">DigiRL-Agent</a>: Learning the hard task of controlling a device via vision. They do offline RL based on annotated actions. This is what many others do and it does pretty poorly on real life. Benchmarks. But then they further train “online” RL with rewards based on vlm scoring. In other words, they have agents actually interact with a virtual device to carry out tasks. This translates to a huge improvement in performance on the “Android in the wild” data set.. they set a new state-of-the-art. I am not sure how well this generalizes outside of this specific task set/ domain. But it is very cool to see agents actually taking action even if it’s in a simulated environment and using that to improve versus just trying to use an off the shelf vision language model that does not have as much understanding as is needed to operate real world devices based on pixels only.</p>
<p><a href="https://research.character.ai/optimizing-inference/">Charachter AI post ‘Optimizing Inference’</a>: Amazing post with a bunch of tricks from Noam Shazeer and crew. Key ideas they use when serving 20k qps: - Multi-Query Attention to reduce KV size - interleave local attention vs global attention (only 1/6 global) to speen things up. - Cross-layer KV cache sharing (between 2 or 3 consecutive layers) to reduce memory usage - Fancy caching to match as much as possible. All the focus is on keeping as much KV cache in mem as possible it seems. - They use int8 quant on weights, activations and KV cache, with fancy kernels. Train in int8 too.</p>
<p><a href="https://moultano.wordpress.com/2023/06/28/the-many-ways-that-digital-minds-can-know/">The Many Ways that Digital Minds Can Know (Moultano)</a>: A fun way to look at some different axes we might care about re: LLMs. Interesting framing!</p>
<p><a href="https://sander.ai/2020/09/01/typicality.html">Musings on typicality</a>: A 2020 post from Sander Dieleman, helping explain why beam search isn’t ideal - linked from the recent review paper “From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models”.</p>
<p><a href="https://openai.com/index/finding-gpt4s-mistakes-with-gpt-4/">Finding GPT-4’s mistakes with GPT-4</a>: Using a model to spot the mistakes of a model. Duuuuude. Cool work, having humans review outputs for RLHF seems like a spot where bugs could be missed, this appears to help a bunch with that (and will likely be a useful model for them to have lying around too!).</p>


</section>
</section>

 ]]></description>
  <category>Blog</category>
  <guid>https://johnowhitaker.dev/misc/tab-clearing.html</guid>
  <pubDate>Thu, 27 Jun 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Paperathon #1</title>
  <link>https://johnowhitaker.dev/misc/2023-11-29-paperathon.html</link>
  <description><![CDATA[ 




<section id="paperathon---4-hours-of-paper-reading-live-on-twitch" class="level3">
<h3 class="anchored" data-anchor-id="paperathon---4-hours-of-paper-reading-live-on-twitch">Paperathon - 4 hours of paper reading live on Twitch</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/YNOIyvUCpAs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>An experimental livestream reading through papers. A tidy version of the notes from the stream can be found <a href="https://docs.google.com/document/d/1weSVlVfVUufOesEmMB2_TlmM_I250ug-WIEZu9xEpVw/edit?usp=sharing">here</a>. Thanks to those who joined the live stream and helped out with the discussion!</p>
<p>Topics w/ timestamps: - 01:00 - Intro + Plan - 06:00 - Orca 2 (and Orca 1) - 20:00 - Emu EDIT - 32:20 - TULU V2 - 49:00 - QLoRA - 1:01:00 - Finding papers - 1:07:10 - System 2 Attention - 1:18:00 - Zephyr - 1:29:20 - DPO - 1:44:30 - cDPO and IPO - 2:00:00 - I-JEPA - 2:33:30 - Luciddreamer (+ dreamfusion, SDS chat, gaussian splitting) - 2:54:40 - MeshGPT - 3:08:20 - Diffusion model roundup - EMU, Dalle-3, Wurstchen, Matryoshka, Pixart, Commoncanvas - 4:00:40 - Recap</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-11-29-paperathon.html</guid>
  <pubDate>Wed, 29 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/paperathon.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>ZipLoRA: Any Subject in Any Style (deep dive and paper explanation)</title>
  <link>https://johnowhitaker.dev/misc/2023-11-26-Ziplora.html</link>
  <description><![CDATA[ 




<section id="ziplora-any-subject-in-any-style-deep-dive-and-paper-explanation" class="level3">
<h3 class="anchored" data-anchor-id="ziplora-any-subject-in-any-style-deep-dive-and-paper-explanation">ZipLoRA: Any Subject in Any Style (deep dive and paper explanation)</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/gJ4bvOX-1CE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>In this video we talk about merging LoRAs - the difficulties with a naive approach and the benefits of the new “ZipLoRA” technique. Paper: https://arxiv.org/abs/2311.13600</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-11-26-Ziplora.html</guid>
  <pubDate>Sun, 26 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/ziplora.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>MLOps Community Mini Summit #3 Talk: What The Kaggle LLM Science Exam Competition Can Teach Us About LLMs</title>
  <link>https://johnowhitaker.dev/misc/2023-11-16-mlops-talk.html</link>
  <description><![CDATA[ 




<section id="what-the-kaggle-llm-science-exam-competition-can-teach-us-about-llms" class="level3">
<h3 class="anchored" data-anchor-id="what-the-kaggle-llm-science-exam-competition-can-teach-us-about-llms">What The Kaggle LLM Science Exam Competition Can Teach Us About LLMs</h3>
<p>Come for the LLM insights, and stay for the Llama-filled slides :)</p>
<p>I enjoyed how the same key things came up in multiple talks: - Inspect your data! - Build good evals - Keep feedback loops short</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/79FOajKg0xI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>My talk went into some themes that crop up in many different LLM projects, and how the Kaggle competition was a great way to explore them.</p>
<p><img src="https://johnowhitaker.dev/misc/thumbnails/mlops_webinar_slide.jpg" class="img-fluid"></p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-11-16-mlops-talk.html</guid>
  <pubDate>Thu, 16 Nov 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/mlops_webinar_slide.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>How I ‘monetized’ an AI demo</title>
  <link>https://johnowhitaker.dev/misc/2023-10-18-How_I_monetized_an_AI_demo.html</link>
  <description><![CDATA[ 




<section id="how-i-monetized-an-ai-demo" class="level3">
<h3 class="anchored" data-anchor-id="how-i-monetized-an-ai-demo">How I ‘monetized’ an AI demo</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/rUDwAEV3JH0" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>See the finished product at https://hallowhatnow.johnowhitaker.repl.co/Template for those wanting to try something like this - https://replit.com/<span class="citation" data-cites="johnowhitaker/AIAppTemplatev">@johnowhitaker/AIAppTemplatev</span>=1In this video, I take you through the process I followed to take a generative AI workflow and turn it into a ‘product’, where users upload a picture and pay to have it transformed into a gallery of themed Halloween costume ideas. It’s pretty cool that anyone with a bit of time and patience can make and share something like this! I hope you’re inspired to build something -)Chapters -00/00 - Introduction and flask app - the frontend01/00 - Stripe checkout03/14 - Replicate for ML model hosting and inference06/12 - Keeping track of what works09/09 - Sending our requests from python11/12 - Testing it out12/30 - Final tips</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-10-18-How_I_monetized_an_AI_demo.html</guid>
  <pubDate>Wed, 18 Oct 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/rUDwAEV3JH0.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>What is Speculative Sampling</title>
  <link>https://johnowhitaker.dev/misc/2023-09-01-What_is_Speculative_Sampling.html</link>
  <description><![CDATA[ 




<section id="what-is-speculative-sampling" class="level3">
<h3 class="anchored" data-anchor-id="what-is-speculative-sampling">What is Speculative Sampling</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/q6oiidmVnwE" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>A quick explainer video for a technique called ‘speculative sampling’ or ‘assisted generation’ which speeds up language model sampling through the use of a smaller ‘draft’ model. On some data types this can give a 2x speedup with no loss in accuracy! Let me know if you have suggestions for other topics you’d like covered.http://jalammar.github.io/illustrated-gpt2/https://huggingface.co/blog/assisted-generationhttps://arxiv.org/abs/2302.01318 (Accelerating Large Language Model Decoding with Speculative Sampling)https://proceedings.neurips.cc/paper/2018/file/c4127b9194fe8562c64dc0f5bf2c93bc-Paper.pdf (Blockwise Parallel Decoding for Deep Autoregressive Models)</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-09-01-What_is_Speculative_Sampling.html</guid>
  <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/q6oiidmVnwE.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>LLM basics #4 with the LLM Science Exam Kaggle Competition - Retrieval</title>
  <link>https://johnowhitaker.dev/misc/2023-09-01-LLM_basics_#4_with_the_LLM_Science_Exam_Kaggle_Competition_-_Retrieval.html</link>
  <description><![CDATA[ 




<section id="llm-basics-4-with-the-llm-science-exam-kaggle-competition---retrieval" class="level3">
<h3 class="anchored" data-anchor-id="llm-basics-4-with-the-llm-science-exam-kaggle-competition---retrieval">LLM basics #4 with the LLM Science Exam Kaggle Competition - Retrieval</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/nvvuTiE4BEk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>In this (delayed) final video of the series, we take a lightning look at one more useful technique to add to your submission arsenal - document retrieval.Finding the closest matches among a collection of documents is an extremely useful tool for all sorts of LLM applications, and this intro shows how easy it can be to get started.Notebook link - https://www.kaggle.com/johnowhitaker/embedding-documents-for-retrieval</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-09-01-LLM_basics_#4_with_the_LLM_Science_Exam_Kaggle_Competition_-_Retrieval.html</guid>
  <pubDate>Fri, 01 Sep 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/nvvuTiE4BEk.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>LLM basics #3 with the LLM Science Exam Kaggle Competition - Training a task-specific model for MCQs</title>
  <link>https://johnowhitaker.dev/misc/2023-08-10-LLM_basics_#3_with_the_LLM_Science_Exam_Kaggle_Competition_-_Training_a_task-specific_model_for_MCQs.html</link>
  <description><![CDATA[ 




<section id="llm-basics-3-with-the-llm-science-exam-kaggle-competition---training-a-task-specific-model-for-mcqs" class="level3">
<h3 class="anchored" data-anchor-id="llm-basics-3-with-the-llm-science-exam-kaggle-competition---training-a-task-specific-model-for-mcqs">LLM basics #3 with the LLM Science Exam Kaggle Competition - Training a task-specific model for MCQs</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/J2dG-Sxv0EI" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>How do we adapt a model to solve multiple-choice questions In this video we dive into how AutoModelForMultipleChoice actually works, exploring the data processing and then training a few models. Also covered – Evaluating as you go- Spotting overfitting- Logging with W&amp;B- Tips for improving the scoreMy colab notebook - https://colab.research.google.com/drive/16HNLUrWuXs32XCh6FTOTZYUo4xNGII15usp=sharingRadek’s post - https://radekosmulski.com/how-to-fine-tune-a-transformer/his notebook (good place to start your own experiments) - https://www.kaggle.com/code/radek1/new-dataset-deberta-v3-large-training/notebookGood luck, and see you on the leaderboard!</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-08-10-LLM_basics_#3_with_the_LLM_Science_Exam_Kaggle_Competition_-_Training_a_task-specific_model_for_MCQs.html</guid>
  <pubDate>Thu, 10 Aug 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/J2dG-Sxv0EI.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>LLM basics #1 with the LLM Science Exam Kaggle Competition - Zero-Shot approaches</title>
  <link>https://johnowhitaker.dev/misc/2023-08-07-LLM_basics_#1_with_the_LLM_Science_Exam_Kaggle_Competition_-_Zero-Shot_approaches.html</link>
  <description><![CDATA[ 




<section id="llm-basics-1-with-the-llm-science-exam-kaggle-competition---zero-shot-approaches" class="level3">
<h3 class="anchored" data-anchor-id="llm-basics-1-with-the-llm-science-exam-kaggle-competition---zero-shot-approaches">LLM basics #1 with the LLM Science Exam Kaggle Competition - Zero-Shot approaches</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/ddCYORu41Xs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Talking about ways to use an off-the-shelf language model to solve a multiple-choice task. Covering – Intro to the Kaggle competition- Benchmarking with GPT3.5- Using the OpenAI function calling API to enforce structure on answers- Using Llama2 as a classifier by examining the logits (next token predictions)- Using perplexity to evaluate question-answer pairsNotebook using the OpenAI API to test GPT3.5 - https://www.kaggle.com/johnowhitaker/benchmark-gpt3-5Llama2 demo notebook - https://colab.research.google.com/drive/1lzfHOqCKg6k7HykHrf4qWJhhEO3I5VXjusp=sharing (quickly made for this video, don’t trust the calculations, rather start with the below notebook) Notebook testing different open models with the perplexity approach - https://www.kaggle.com/code/takamichitoda/llm-perplexity-ranking-ensemble (a good template to start experimenting since it shows how to run as a submission.</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-08-07-LLM_basics_#1_with_the_LLM_Science_Exam_Kaggle_Competition_-_Zero-Shot_approaches.html</guid>
  <pubDate>Mon, 07 Aug 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/ddCYORu41Xs.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Stylizing Video with Diffusion Models</title>
  <link>https://johnowhitaker.dev/misc/2023-02-15-Stylizing_Video_with_Diffusion_Models.html</link>
  <description><![CDATA[ 




<section id="stylizing-video-with-diffusion-models" class="level3">
<h3 class="anchored" data-anchor-id="stylizing-video-with-diffusion-models">Stylizing Video with Diffusion Models</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/3ZqJAM8ZbIk" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Reviewing existing techniques, introducing ‘coherence guidance’ and running through the GEN1 Paper from <span class="citation" data-cites="RunwayML.Notebook">@RunwayML.Notebook</span> link to try my method out - https://colab.research.google.com/drive/1inQJPKLOpjB/Bpo0GmboqJWJ1AxzW5Xausp=sharingTag me on Twitter with your creations - https://twitter.com/johnowhitakerRunway GEN1 - https://research.runwayml.com/gen1Please subscribe, share, give feedback etc as that will help the channel reach more people and get better! I’m experimenting with new video styles so I’d especially like any feedback on the new look/feel vs the usual screen recordings -)</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-02-15-Stylizing_Video_with_Diffusion_Models.html</guid>
  <pubDate>Wed, 15 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/3ZqJAM8ZbIk.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>InstructPix2Pix Explained - Edit Images with Words!</title>
  <link>https://johnowhitaker.dev/misc/2023-02-03-InstructPix2Pix_Explained_-_Edit_Images_with_Words!.html</link>
  <description><![CDATA[ 




<section id="instructpix2pix-explained---edit-images-with-words" class="level3">
<h3 class="anchored" data-anchor-id="instructpix2pix-explained---edit-images-with-words">InstructPix2Pix Explained - Edit Images with Words!</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/-I9-2XK3kOs" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Join me as I explain the core idea behind ‘InstructPix2Pix’ which let’s you edit images using natural language instructions!The thumbnail was generated with ‘Add sunglasses’, ‘make it in a city at night’ and ‘give him a leather jacket’ - far better than messing about with photoshop -)My previous video on ‘Editing Images with Diffusion Models’ - https://www.youtube.com/watchv=zcG7tG3xS3sInstructPix2Pix paper - https://arxiv.org/abs/2211.09800GitHub - https://github.com/timothybrooks/instruct-pix2pixModel - https://huggingface.co/timbrooks/instruct-pix2pixDemo Space - https://huggingface.co/spaces/timbrooks/instruct-pix2pixPlaygroundAI - playgroundai.com</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-02-03-InstructPix2Pix_Explained_-_Edit_Images_with_Words!.html</guid>
  <pubDate>Fri, 03 Feb 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/-I9-2XK3kOs.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Stable Diffusion Deep Dive Notebook Run-through</title>
  <link>https://johnowhitaker.dev/misc/2023-01-30-Stable_Diffusion_Deep_Dive_Notebook_Run-through.html</link>
  <description><![CDATA[ 




<section id="stable-diffusion-deep-dive-notebook-run-through" class="level3">
<h3 class="anchored" data-anchor-id="stable-diffusion-deep-dive-notebook-run-through">Stable Diffusion Deep Dive Notebook Run-through</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/844LY0vYQhc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>In this video/notebook Johno shows us what is happening behind the scenes when we create an image with Stable Diffusion, looking at the different components and processes and how each can be modified for further control over the generation process.The notebook is available in this repository - https://github.com/fastai/diffusion-nbs00/00 - Introduction00/40 - Replicating the sampling loop01/17 - The Auto-Encoder03/55 - Adding Noise and image-to-image08/43 - The Text Encoding Process15/15 - Textual Inversion18/36 - The UNET and classifier free guidance24/41 - Sampling explanation36/30 - Additional guidanceThis was made as a companion to lesson one of the new FastAI 2022 part 2 course (aka Lesson 9) by Jonathan Whitaker (his channel - https://www.youtube.com/channel/UCP6gT9X2oXYcssfZu05RV2g)Errata - there should be some scaling done to the model inputs for the unet demo in cell 49 (19 minutes in) - see scheduler.scale/model/input in all the loops for the code that is missing. And in the autoencoder part the ‘compression’ isn’t exactly 64 times since there are 4 channels in the latent representation and only 3 in the input.</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-01-30-Stable_Diffusion_Deep_Dive_Notebook_Run-through.html</guid>
  <pubDate>Mon, 30 Jan 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/844LY0vYQhc.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Podcast E6 - Wasim Lorgat</title>
  <link>https://johnowhitaker.dev/misc/2023-01-10-Podcast_E6_-_Wasim_Lorgat.html</link>
  <description><![CDATA[ 




<section id="podcast-e6---wasim-lorgat" class="level3">
<h3 class="anchored" data-anchor-id="podcast-e6---wasim-lorgat">Podcast E6 - Wasim Lorgat</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/c3Ct0M63Ryc" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Wasim talks us through his approach to learning and building projects, touching on NBDev, building on big AI models like CLIP, his project ‘Meepo’, plans for building a new editor for Jupyter. This was the first podcast I’ve been able to record in-person, which was super fun! As mentioned in the outro, let me know what you’d like to see from future episodes, which will likely be intermittent rather than regular -)Links -https://wasimlorgat.com/https://nbdev.fast.ai/https://twitter.com/wasimlorgat,meepo.shop, Mentioned - nbterm and jpterm from <span class="citation" data-cites="davidbrochart">@davidbrochart</span>, lexica.art (CLIP-based search over images), FAISS (https://faiss.ai/) for fast vector search,</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2023-01-10-Podcast_E6_-_Wasim_Lorgat.html</guid>
  <pubDate>Tue, 10 Jan 2023 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/c3Ct0M63Ryc.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>HuggingFace Class</title>
  <link>https://johnowhitaker.dev/misc/2022-12-23-HuggingFace_Class.html</link>
  <description><![CDATA[ 




<section id="huggingface-class" class="level3">
<h3 class="anchored" data-anchor-id="huggingface-class">HuggingFace Class</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/mY20iKOQ2zw" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Unit 2 - Fintuning and Guidance (casual notebook walkthough),Looking through the notebooks for the huggingface diffusion models class, unit 2 - https://github.com/huggingface/diffusion-models-class/tree/main/unit2We cover fine-tuning existing diffusion models on new datasets, guiding generation with additional loss functions and creating a class-conditioned diffusion model.I mention a vide for unit 3 but have since remembered that I already link the Stable Diffusion Deep Dive video so that’s probably sufficient!</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2022-12-23-HuggingFace_Class.html</guid>
  <pubDate>Fri, 23 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/mY20iKOQ2zw.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>HuggingFace Diffusion Model Class</title>
  <link>https://johnowhitaker.dev/misc/2022-12-22-HuggingFace_Diffusion_Model_Class.html</link>
  <description><![CDATA[ 




<section id="huggingface-diffusion-model-class" class="level3">
<h3 class="anchored" data-anchor-id="huggingface-diffusion-model-class">HuggingFace Diffusion Model Class</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/09o5cv6u76c" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Unit 1 (casual notebook walkthough),Intro to diffusion models via https://github.com/huggingface/diffusion-models-class</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2022-12-22-HuggingFace_Diffusion_Model_Class.html</guid>
  <pubDate>Thu, 22 Dec 2022 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/09o5cv6u76c.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>Editing Images with Diffusion Models (lit review _ overview of different approaches)</title>
  <link>https://johnowhitaker.dev/misc/2022-11-22-Editing_Images_with_Diffusion_Models_(lit_review___overview_of_different_approaches).html</link>
  <description><![CDATA[ 




<section id="editing-images-with-diffusion-models-lit-review-_-overview-of-different-approaches" class="level3">
<h3 class="anchored" data-anchor-id="editing-images-with-diffusion-models-lit-review-_-overview-of-different-approaches">Editing Images with Diffusion Models (lit review _ overview of different approaches)</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/zcG7tG3xS3s" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>How can we use diffusion models to edit existing images rather than generating completely new images1) Add some noise, denoise with a new prompt- VQGAN + CLIP paper - https://arxiv.org/pdf/2204.08583.pdf (includes editing with masks, prompt changes, lots of other ideas that have parallels in the newer diffusion model works)- SDEdit - https://sde-image-editing.github.io/- MagicMix - https://magicmix.github.io/2) As in (1) but with a mask- Blended Diffusion - https://omriavrahami.com/blended-diffusion-page/- Mask with CLIPSeg - https://huggingface.co/spaces/nielsr/text-based-inpainting- Get the mask from the diffusion model (DiffEdit, great paper) - https://arxiv.org/abs/2210.11427- John’s post on DiffEdit - https://www.storminthecastle.com/posts/diffedit/3) Cross-attention Control- Prompt-to-Prompt Image Editing with Cross Attention Control - https://arxiv.org/abs/2208.01626 (report on this with SD - https://wandb.ai/wandb/cross-attention-control/reports/Improving-Generative-Images-with-Instructions-Prompt-to-Prompt-Image-Editing-with-Cross-Attention-Control–VmlldzoyNjk2MDAy)4) Fine-tune (‘overfit’) on a single image and then generate with the fine-tuned model- Imagic paper - https://arxiv.org/pdf/2210.09276.pdf- UniTune - https://arxiv.org/pdf/2210.09477.pdfI hope you enjoyed this video! Which method do you think will win out Anything you’d like more information on</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2022-11-22-Editing_Images_with_Diffusion_Models_(lit_review___overview_of_different_approaches).html</guid>
  <pubDate>Tue, 22 Nov 2022 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/zcG7tG3xS3s.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>TGL Discussion Series - Hamel Husain</title>
  <link>https://johnowhitaker.dev/misc/2022-11-13-TGL_Discussion_Series_-_Hamel_Husain.html</link>
  <description><![CDATA[ 




<section id="tgl-discussion-series---hamel-husain" class="level3">
<h3 class="anchored" data-anchor-id="tgl-discussion-series---hamel-husain">TGL Discussion Series - Hamel Husain</h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/q8s2egj4RlQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>In this video, I chat with Hamel Husain, a Machine Learning Engineer/Data Scientist passionate about making tools. Hamel is one of the most helpful and interesting people I’ve met. He turned the usual interview process on its head and decided to interview me too, so you get two for the price of one in this episode!</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2022-11-13-TGL_Discussion_Series_-_Hamel_Husain.html</guid>
  <pubDate>Sun, 13 Nov 2022 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/q8s2egj4RlQ.jpg" medium="image" type="image/jpeg"/>
</item>
<item>
  <title>TGL Discussions Series - @EnzymeZoo</title>
  <link>https://johnowhitaker.dev/misc/2022-11-10-TGL_Discussions_Series_-_@EnzymeZoo.html</link>
  <description><![CDATA[ 




<section id="tgl-discussions-series---enzymezoo" class="level3">
<h3 class="anchored" data-anchor-id="tgl-discussions-series---enzymezoo">TGL Discussions Series - <span class="citation" data-cites="EnzymeZoo">@EnzymeZoo</span></h3>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/6tMLIxlE_6I" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p><span class="citation" data-cites="EnzymeZoo">@EnzymeZoo</span> is an artist and developer. In this discussion, we chat about the community development work that goes into creating a tool like the Deforum notebook (https://deforum.github.io/). We explore how that project came together, how you can get involved and <span class="citation" data-cites="EnzymeZoos">@EnzymeZoos</span> general thoughts on AI art and creativity.Links – <a href="https://discord.gg/deforum">Deforum Discord</a> (user discord, but from there you can find the developer group as well)- <a href="https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin/Jax/Diffusion/2/7.ipynb">Jax diffusion</a> notebook- <a href="https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco/Diffusion.ipynb">Disco Diffusion</a> notebook- List of tools by <span class="citation" data-cites="pharmapsychotic">@pharmapsychotic</span> which has many more notebooks and guides - https://pharmapsychotic.com/tools.html- More <a href="https://twitter.com/searchq=%23DeforumDiffusion&amp;src=typeahead/click">creations using deforum on Twitter</a>After we stopped recording <span class="citation" data-cites="EnzymeZoo">@EnzymeZoo</span> also asked that I mention/thank Stability AI. The release of Stable Diffusion was the catalyst for Deforum springing into existence, and they have since started supporting some of the developers behind this and other notebooks.</p>


</section>

 ]]></description>
  <category>Video</category>
  <guid>https://johnowhitaker.dev/misc/2022-11-10-TGL_Discussions_Series_-_@EnzymeZoo.html</guid>
  <pubDate>Thu, 10 Nov 2022 00:00:00 GMT</pubDate>
  <media:content url="https://johnowhitaker.dev/misc/thumbnails/6tMLIxlE_6I.jpg" medium="image" type="image/jpeg"/>
</item>
</channel>
</rss>
