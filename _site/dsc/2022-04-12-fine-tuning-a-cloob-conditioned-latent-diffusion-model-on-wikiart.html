<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-04-12">

<title>johnowhitaker.dev - Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">johnowhitaker.dev</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html">
 <span class="menu-text">Data Science Castnet Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../tils.html">
 <span class="menu-text">TILs</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../about.html">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#what-is-cloob-conditioned-latent-diffusion" id="toc-what-is-cloob-conditioned-latent-diffusion" class="nav-link active" data-scroll-target="#what-is-cloob-conditioned-latent-diffusion">What is CLOOB-Conditioned Latent Diffusion?</a></li>
  <li><a href="#trainingfine-tuning-a-model" id="toc-trainingfine-tuning-a-model" class="nav-link" data-scroll-target="#trainingfine-tuning-a-model">Training/Fine-Tuning a model</a></li>
  <li><a href="#evaluating-the-resulting-model" id="toc-evaluating-the-resulting-model" class="nav-link" data-scroll-target="#evaluating-the-resulting-model">Evaluating The Resulting Model</a></li>
  <li><a href="#closing-thoughts" id="toc-closing-thoughts" class="nav-link" data-scroll-target="#closing-thoughts">Closing Thoughts</a></li>
  <li><a href="#comparison-images" id="toc-comparison-images" class="nav-link" data-scroll-target="#comparison-images">Comparison images</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt</h1>
</div>



<div class="quarto-title-meta">

    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 12, 2022</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-sunset-landscape-painting-oil-on-canvas-edited.png?w=780" class="img-fluid"></p>
<p>Prompt: ‘A sunset landscape painting, oil on canvas’ (fine-tuned Wikiart model)</p>
<p>As part of the Huggingface ‘#huggan’ event, I thought it would be interesting to fine-tune a latent diffusion model on the WikiArt dataset, which (as the name suggests) consists of paintings in various genres and styles.</p>
<section id="what-is-cloob-conditioned-latent-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="what-is-cloob-conditioned-latent-diffusion">What is CLOOB-Conditioned Latent Diffusion?</h2>
<p>Diffusion models are getting a lot of fame at the moment thanks to GLIDE and DALL-E 2 which have recently rocked the internet with their astounding text-to-image capabilities. They are trained by gradually adding noise to an input image over a series of steps, and having the network predict how to ‘undo’ this process. If we start from pure noise and have the network progressively try to ‘fix’ the image we eventually end up with a nice looking output (if all is working well).</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/screenshot-from-2022-04-12-14-55-09.png?w=1024" class="img-fluid"></p>
<p>An illustration of this kind of model from <a href="https://hojonathanho.github.io/diffusion/">the website</a> related to <a href="https://arxiv.org/abs/2006.11239">one of the key papers that first outlined this idea</a>.</p>
<p>To add text-to-image capacity to these models, they are often ‘conditioned’ on some representation of the captions that go along with the images. That is, in addition to seeing a noisy image, they also get an encoding of the text describing the image to help in the de-noising step. Starting from noise again but this time giving a description of the desired output image as the text conditioning ideally steers the network towards generating an image that matches the description.</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/cloob_cropped.png?w=1024" class="img-fluid"></p>
<p>CLOOB architecture diagram (from <a href="https://ml-jku.github.io/cloob/">the project page</a> - which is worth a read!)</p>
<p>Downsides: these diffusion models are computationally intensive to train, and require images with text labels. <strong>Latent diffusion models reduce the computational requirements by doing the denoising in the latent space of an autoencoder rather than on images directly</strong>. And since CLOOB maps both images and text to the same space, <strong>we can substitute the CLOOB encodings of the image itself in place of actual caption encodings</strong> if we want to train with unlabelled images. A neat trick if you ask me!</p>
<p>The best non-closed text-to-image implementation at the moment is probably the latent diffusion model trained by the CompVis team, which you can try out <a href="https://huggingface.co/spaces/multimodalart/latentdiffusion">here</a>.</p>
</section>
<section id="trainingfine-tuning-a-model" class="level2">
<h2 class="anchored" data-anchor-id="trainingfine-tuning-a-model">Training/Fine-Tuning a model</h2>
<p><span class="citation" data-cites="JDP">@JDP</span> provides training code for CLOOB conditioned latent diffusion (<a href="https://github.com/JD-P/cloob-latent-diffusion">https://github.com/JD-P/cloob-latent-diffusion</a>) based on the similar CLIP conditioned diffusion trained by Katherine Crowson (<a href="https://github.com/crowsonkb/v-diffusion-pytorch">https://github.com/crowsonkb/v-diffusion-pytorch</a>). One of my #huggan team members, Théo Gigant, uploaded the WikiArt dataset to the huggingface hub, and the images were downloaded, resized and saved to a directory on a 2xA6000 GPU machine provided by Paperspace.</p>
<p>After a few false starts figuring out model loading and other little quirks, we did a ~12 hour training run and logged the results using Weights and Biases. You can view demo outputs from the model as it trains in <a href="https://wandb.ai/johnowhitaker/jw-ft-cloob-latent-diffusion/reports/Fine-Tuning-CLOOB-latent-diffusion--VmlldzoxNzk5OTgz">the report</a>, which thanks to the W&amp;B magic showed them live as the model was training, making for exciting viewing among our team :)</p>
</section>
<section id="evaluating-the-resulting-model" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-the-resulting-model">Evaluating The Resulting Model</h2>
<p>WikiArt is not a huge dataset relative to the model (which has over a billion parameters). One of the main things we were curious about was how the resulting model would be different from the one we started with, which was trained on a much larger and more diverse set of images. Has it ‘overfit’ to the point of being unuseable? How much more ‘arty’ do the results look when passing descriptions that don’t necessarily suggest fine art? And has fine-tuning on a relatively ‘clean’ dataset lowered the ability of the model to produce disturbing outputs? To answer these questions, we generated hundreds of images with both models.</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/download-10.png?w=768" class="img-fluid"></p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/download-9.png?w=768" class="img-fluid"></p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/spring.png?w=768" class="img-fluid"></p>
<p>Generated images from the prompts ‘winter landscape’, ‘autumn landscape’ and ‘spring landscape’ (WikiArt model). Note: all results are ‘painterly’ despite no allusion to paintings or art in the prompts. Seeds kept consistent for each set - note the slight similarity in overall structure for corresponding images.</p>
<p>I’ve moved the side-by-side comparisons to a gallery at the end of this post. These were the key takeaways for me:</p>
<ul>
<li>Starting from a ‘photorealistic’ autoencoder didn’t stop it from making very painterly outputs. This was useful - we thought we might have to train our own autoencoder first as well.</li>
<li>The type of output definitely shifted, almost everything it makes looks like a painting</li>
<li>It lost a lot of more general concepts but does really well with styles/artists/image types present in the dataset. So landscape paintings are great, but ‘a frog’ is not going to give anything recognizable and ‘an avocado armchair’ is a complete fail :)</li>
<li>It may have over-fit, and this seems to have made it much less likely to generate disturbing content (at the expense of also being bad at a lot of other content types).</li>
</ul>
</section>
<section id="closing-thoughts" class="level2">
<h2 class="anchored" data-anchor-id="closing-thoughts">Closing Thoughts</h2>
<p>Approaches like CLOOB-Conditioned Latent Diffusion are bringing down the barrier to entry and making it possible for individuals or small organisations to have a crack at training diffusion models without $$$ of compute.</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/screenshot-from-2022-04-06-20-27-39.png?w=1024" class="img-fluid"></p>
<p>Our model during training (left) vs OpenAI’s DALL-E 2 (right) which was unveiled during our project and inspired various memes :)</p>
<p>This little experiment of ours has shown that it is possible to train one of these models on a relatively small dataset and end up with something that can create pleasing outputs, even if it can’t quite manage an avocado armchair. And as a bonus, it’s domain-focused enough that I’m happily sharing a live demo that anyone can play with online, without worrying that it’ll be used to generate any highly-realistic fake photographs of celebrity nudity or other such nonsense. What a time to be alive!</p>
</section>
<section id="comparison-images" class="level2">
<h2 class="anchored" data-anchor-id="comparison-images">Comparison images</h2>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-watercolor-painting-of-a-rose-1.png?w=761" class="img-fluid"></p>
<p>A watercolor painting of a rose</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/autumn-watercolor-1.png?w=761" class="img-fluid"></p>
<p>Autumn watercolor</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/autumn-landscape-1.png?w=761" class="img-fluid"></p>
<p>Autumn landscape</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-monet-pond-1.png?w=761" class="img-fluid"></p>
<p>A Monet Pond</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-pink-lilly-1.png?w=761" class="img-fluid"></p>
<p>A pink lilly</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/tarsila-do-amaral-1.png?w=761" class="img-fluid"></p>
<p>Tarsila do Amaral</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/blue-and-pink-hydrangeas-impressionistic-oils-1.png?w=761" class="img-fluid"></p>
<p>Blue and pink hydrangeas, impressionistic oils</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/ney-york-skyline-in-winter.png?w=761" class="img-fluid"></p>
<p>New York skyline in winter</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-face-portrait-in-oils-1.png?w=761" class="img-fluid"></p>
<p>A face, portrait in oils</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/a-female-figure-charcoal-1.png?w=761" class="img-fluid"></p>
<p>A female figure, charcoal</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/the-moon-over-a-landscape-1.png?w=761" class="img-fluid"></p>
<p>The moon over a landscape</p>
<p><img src="https://datasciencecastnethome.files.wordpress.com/2022/04/peaceful-blue-1.png?w=761" class="img-fluid"></p>
<p>Peaceful Blue</p>
<p>Comparison images from our finetuned model (top) and the original model (bottom). Captions are the prompts used.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>