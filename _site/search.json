[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html",
    "href": "dsc/2019-08-27-trying-automated-ml.html",
    "title": "Trying Automated ML",
    "section": "",
    "text": "Some students had asked me for my opinion on automated tools for machine learning. The thought occurred that I hadn’t done much with them recently, and it was about time I gave the much-hyped time savers a go - after all, aren’t they going to make data scientists like me redundant?\nIn today’s post, I’ll be trying out Google’s AutoML tool by throwing various datasets at it and seeing how well it does. To make things interesting, the datasets I’ll be using will be from Zindi competitions, letting us see where AutoML would rank on the player leader-board. I should note that these experiments are a learning exercise, and actually using AutoML to win contests is almost certainly against the rules. But with that caveat out the way, let’s get started!"
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html#how-it-works",
    "href": "dsc/2019-08-27-trying-automated-ml.html#how-it-works",
    "title": "Trying Automated ML",
    "section": "How it works",
    "text": "How it works\nAutoML (and other similar tools) aims to automate one step of the ML pipeline - that of model selection and tuning. You give it a dataset to work on, specify column types, choose an output column and specify how long you’d like it to train for (you pay per hour). Then sit back and wait. Behind the scenes, AutoML tries many different models and slowly optimizes network architecture, parameters, weights… essentially everything one could possibly tweak to improve performance gets tweaked. At the end of it, you get a (very complicated) model that you can then deploy with their services or use to make batch predictions.\n\nThe first step with AutoML tables - Importing the data.\nThe resultant models are fairly complex (mine were ~1GB each fully trained) and are not something you can simply download and use locally - you must deploy them via Google (for an extra fee). This, coupled with the cost of training models, makes it fairly expensive to experiment with if you use up your trial credits - so use them wisely.\nFortunately, there are other ways to achieve broadly the same result. For example, AutoKeras. Read more about that here."
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html#experiment-1-farm-pin-crop-detection",
    "href": "dsc/2019-08-27-trying-automated-ml.html#experiment-1-farm-pin-crop-detection",
    "title": "Trying Automated ML",
    "section": "Experiment 1: Farm Pin Crop Detection",
    "text": "Experiment 1: Farm Pin Crop Detection\nThis competition involves a classification problem, with the goal being to predict which crop is present in a given field. The training data is provided as field outlines and satellite images - not something that can effortlessly slot into AutoML tables. This meant that the first step was to sample the image bands for the different fields, and export the values to a CSV files for later analysis (as described in this post). This done, I uploaded the resultant training file to cloud storage, selected the table, chose my input and output columns and hit go.\n\nAutoML ‘Evaluate’ tab showing model performance.\nThe scoring metric for this competition is log loss. My previous best (using the same training data to train a random forest model) scored around 0.64 (~20th on the leaderboard). So a score of <0.6 looked promising. I uploaded the test set, hit predict and then manually cleaned up the output to match the submission format for Zindi. Score? 0.546, putting me in 12th place. No feature engineering besides sampling some satellite images, no manual tweaking of model parameters…. not bad!\nI was quite pleased with this result. I enjoy the feature engineering side of things, but the tedium of hyper-parameter tuning is less appealing to me. If this tool can magically let me skip that step, it’s a win in my book! I may re-visit this with some added features, information from more images and perhaps a trick or two to enlarge the training set."
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html#experiment-2-traffic-jam",
    "href": "dsc/2019-08-27-trying-automated-ml.html#experiment-2-traffic-jam",
    "title": "Trying Automated ML",
    "section": "Experiment 2: Traffic Jam",
    "text": "Experiment 2: Traffic Jam\nSpurred on by the first success, I turned to the Traffic Jam competition since I still had the dataset on my laptop. This was a regression problem, with the goal being to predict the number of tickets sold for a given trip into Nairobi. The training data was fairly sparse, with only ~2000 rows to work from. Still, I figured it was worth a shot and threw a few node hours worth of Google-managed ML magic at the problem.\n\nAn MAE of 3.4, hypothetically equivalent to ~3rd place!\nThe evaluation results had me excited - and MAE of 3.4 would have placed the model in third place had the competition remained open. I hastily uploaded the predictions to Zindi, to see the score of… 5.3 (160th place). Now, I might be missing some glaring error in the way I formatted predictions for upload, but I suspect that the issue is with AutoML. It’s not really designed for such small datasets. From the website: “Depending on how many features your dataset has, 1,000 rows might not be enough to train a high-performing model.” The impressive MAE shown in the results tab is for one particular test set, and it seems that for the Zindi test set we were simply not as lucky. Another potential factor: The random test set will have sampled from the same date range as the training data, whereas the Zindi test set was for a different time period. In cases like this, a non-shuffled test/train split can be a better indicator of true performance.\nSo, we’ve learnt something new! The magic tool isn’t magic, and just like any other method it needs good training data to make good predictions."
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html#experiment-3-sendy",
    "href": "dsc/2019-08-27-trying-automated-ml.html#experiment-3-sendy",
    "title": "Trying Automated ML",
    "section": "Experiment 3: Sendy",
    "text": "Experiment 3: Sendy\nI couldn’t resist trying it out once more on the newly launched Sendy Competition. I merged the Riders info into the train and test sets, uploaded the data, gave it an hour of training time and set it going. The goal is to minimize RMSE when predicting travel time between two locations (for deliveries). I also did some modelling myself while I waited for the AutoML training to finish.\nScores (RMSE for predicted time in seconds)\nMy first attempt (Catboost on provided data): 734 (7th place when this post was written)\nFirst place: 721\nGoogle AutoML: 724 (4th place until I convince them to remove my latest entry)\nNot too shabby! To me, one of the great uses of a tool like this is to give a ballpark for what a good model looks like. Without the Zindi leaderboard, I wouldn’t have a way to gauge my model performance. Is it good? Could it get better with the same data? Now I can compare to the AutoML, using it as a ‘probably close to best’ measure."
  },
  {
    "objectID": "dsc/2019-08-27-trying-automated-ml.html#where-next",
    "href": "dsc/2019-08-27-trying-automated-ml.html#where-next",
    "title": "Trying Automated ML",
    "section": "Where next?",
    "text": "Where next?\nThese quick tests have convinced me that these automated tools can be a useful part of my workflow, but are not a complete replacement for manual experimentation, exploration, feature engineering and modelling. I intend to play around more with AutoML and other tools in the near future, so stay tuned for a continuation of this series."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "",
    "text": "Welcome to the first in a three-part series on Zindi’s Uber Movement SANRAL Cape Town Challenge. This tutorial will take a look at the challenge, start exploring the data and show how to fit a quick model and get a score on the leaderboard. Part two will add in some extra features and a more complex model, and part 3 will run through some GIS tricks to further augment the data and improve our accuracy.\nFollow along with this post using this notebook."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#the-challenge",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#the-challenge",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "The Challenge",
    "text": "The Challenge\nThis aim of this competition is to predict where road incidents in Cape Town are likely to happen next. It’s interesting for a few different reasons:\n1) Traffic incidents are rare - so rare the odds of one happening on a 500m stretch of road in a given hour (which is how Zindi has framed the problem) are always going to be low enough that ‘no incident’ is the most likely outcome. If the metric was accuracy, predicting all 0s would probably be your best bet. However, incidents do occur! And the chosen metric (F1 score) means that you’d better predict some incidents or you’ll score 0. More on this later.\n2) It’s spatial. We can treat this like any other supervised learning problem (with some data shaping) but these events are all located on a road grid that exists in the real world. Segments have positions, and lead into other segments. There are intersections, corners, different lanes…. Some GIS knowledge could give you an edge here (or you could wait for part 3!)\nSo, we need to create a model that can predict how likely it is that there will be an incident on a given stretch of road at a given time. Then we need to use that likelihood to choose some segments where we thing the chances of an incident are highest. And then we make submissions and hope we get a good score :) Where do we start? Let’s take a look at the data."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#the-data",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#the-data",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "The data",
    "text": "The data\n\nThe different road segments\nThe roads along which events have been recorded have been divined into segments, each roughly 500m long (lengths vary). The events themselves each have a latitude and longitude associated with them, and have been tagged with the segment id of the nearest road segment. Due to map inaccuracies, the events don’t always line up exactly with the road network.\n\nEvents (blue) not quite aligned with road segments.\nThe main input file is ‘train.csv’, which contains the individual events. The submission requires grouping these into segments and making hourly predictions, so some re-shaping is required (see the notebook).\n\ntrain.csv - the base on which we’ll build\nExtra data includes a shapefile of the road segments themselves. This shows the segments but also includes extra info like the umber of lanes, road name etc. There is also Uber Movenet data with travel times between different zones withing the city. In part 3 we’ll look more at this.\n\nUber movement zones (red) with those along the road segments selected (green).\nFinally, there is the data from SANRAL and the option to add weather data. Initially, the SANRAL data was only provided for the training period (since the worry was that it would give too much away). It has since been updated to include all dates covered - making it much more useful."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#adding-some-features",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#adding-some-features",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "Adding some features",
    "text": "Adding some features\nWe’re looking at each segment, for each hour. What kinds of features can we add that could help us create a model? The other data sources contain some useful info (as we’ll see in the following posts) but even with just train.csv we can start building up some info to work with. For example, we can derive day of the week, time, month etc from the datetime - all of which likely influence the incident rate.\n\nAdding some date-related variables\nWe can also get the rough locations of the segments by looking at the locations of the incidents within them:\n\nAdding location columns\nThere’s plenty more, but for now let’s fit a model and make some predictions."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#modelling",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#modelling",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "Modelling",
    "text": "Modelling\nI went with CatBoost as a starting model. Good performance, reasonable handling of imbalanced data and it saves us having to fiddle with categorical columns. We specify the input and output columns, create a CatBoostClassifier and throw our data at it:\n\nFirst model\nIn the notebook, you’ll see me scoring the model with log-loss to see if it’s better than random predictions or predicting the mean. Even though it isn’t the metric Zindi is using, it’ll help us pick the best out of several models. Then I try F1 score, and we see our first little hitch: the model scores 0 (bad) on the test set. What’s up? It’s predicting all 0s, as any good model would."
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#f1-scores-thresholds-and-classification-vs-prediction",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#f1-scores-thresholds-and-classification-vs-prediction",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "F1 scores, thresholds and classification vs prediction",
    "text": "F1 scores, thresholds and classification vs prediction\nLooking at the model’s predicted probabilities, we see the issue - values range from ~0 to ~0.2. If we were gunning for classification accuracy, we’d go with 0 if the probability is this low. BUT, here we’re not going for absolute classifications, we’re aiming for predictions of which segments are most likely. A good article on the difference here. So how do we fix this?\nOne approach is by picking a threshold and predicting 1s where it is exceeded. In the notebook, I show that predicting 1s if the probability is >0.05 gets a better f1 score. Of course, there are experimental or theoretical ways to get this threshold correct (see this paper for eg) but trying a few different values and guessing was my lazy approach :)\nAnother option is to mess about with the class_weights parameter. I followed the advice in the docs, and got roughly the same score as I had with the threshold method.\n\nTip from the CatBoost documentation"
  },
  {
    "objectID": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#making-a-submission",
    "href": "dsc/2019-10-19-zindi-uberct-part-1-getting-started.html#making-a-submission",
    "title": "Zindi UberCT Part 1: Getting started",
    "section": "Making a submission",
    "text": "Making a submission\nSo, we have a model that predicts probabilities, and a threshold above which we’ll predict a one. All that’s left is to transform our sample submission dataframe the same way we did with train - adding time and location columns. Then we feed it through our model, save and submit!\n\nMaking predictions\nThis model scores around 0.036 on the leader-board (10’th place since the contest is still new). At this stage, you could go into Zindi competition mode and start tweaking every possible model parameter to up your score slightly, but the real value will be in getting more than just some date-related columns to work with. We’’l get to that - for now, take a look my starting notebook, play around, get on that leaderboard and stay tuned!"
  },
  {
    "objectID": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html",
    "href": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html",
    "title": "Packaging a classification model as a web app",
    "section": "",
    "text": "My shiny new web app, available here\nIn my previous post I introduced fastai, and used it to identify images with potholes. Since then, I’ve applied the same basic approach to the Standard Bank Tech Impact Challenge: Animal classification with pretty decent results. A first, rough model was able to score 97% accuracy thanks to the magic of transfer learning, and by unfreezing the inner layers and re-training with a lower learning rate I was able to up the accuracy to over 99% for this binary classification problem. It still blows my mind how good these networks are at computer vision.\nZebra or Elephant?\nThis was exciting and fun. But I wanted to share the result, and my peer group aren’t all that familiar with log-loss scores. How could I get the point across and communicate what this means? Time to deploy this model as a web application :)"
  },
  {
    "objectID": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#exporting-the-model-for-later-use",
    "href": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#exporting-the-model-for-later-use",
    "title": "Packaging a classification model as a web app",
    "section": "Exporting the model for later use",
    "text": "Exporting the model for later use\n\nFinal training step, saving weights and exporting to a file in my Google Drive\nI knew it was possible to save some of the model parameters with model.save(‘name’), but wasn’t sure how easy it would be to get a complete model definition. Turns out, enough people want this that you can simply call model.export(‘model_name’). So I set my model training again (I hadn’t saved last time) and started researching my next step while Google did my computing for me."
  },
  {
    "objectID": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#packaging-as-an-app",
    "href": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#packaging-as-an-app",
    "title": "Packaging a classification model as a web app",
    "section": "Packaging as an app",
    "text": "Packaging as an app\nI expected this step to be rather laborious. I’d need to set up a basic app (planned to use Flask), get an environment with pytorch/fastai set up and deploy to a server or, just maybe, get it going on Heroku. But then I came across an exciting page in the fastai docs: ‘Deploying on Render’. There are essentially 3 steps:\n- Fork the example repository\n- Edit the file to add a link to your exported model\n- Sign up with Render and point it at your new GitHub repository.\nThen hit deploy! You can read about the full process in the aforementioned tutorial. Make sure your fastai is a recent version, and that you export the model (not just saving weights).\nThe resultant app is available at zebra-vs-elephant.onrender.com. I used an earlier model with 97% accuracy (since I’m enjoying that top spot on the leaderboard ;)) but it’s still surprisingly accurate. It even get’s cartoons right!\n\n\n\n\n\n\n\n\nPlease try it out and let me know what you think. It makes a best guess - see what it says for non-animals, or flatter your friends by classifying them as pachyderms."
  },
  {
    "objectID": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#conclusion",
    "href": "dsc/2019-09-12-packaging-a-classification-model-as-a-web-app.html#conclusion",
    "title": "Packaging a classification model as a web app",
    "section": "Conclusion",
    "text": "Conclusion\nThere seems to be a theme to my last few posts: “Things that sound hard are now easy!”. It’s an amazing world we live in. You can make something like this! It took 20 minutes, with me doing setup while the model trained! Comment here with links to your sandwich-or-not website, your am-I-awake app, your ‘ask-a-computer-if-this-dolphin-looks-happy’ business idea. Who knows, one of us might even make something useful :)\n\nYes, that is apparently an elephant…\nUPDATE: I’ve suspended the service for now, but can re-start it if you’d like to try it. Reach out if that’s the case :)"
  },
  {
    "objectID": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html",
    "href": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html",
    "title": "Playing with Tweet Sentiment Analysis",
    "section": "",
    "text": "The average sentiment of the most recent 200 tweets from each country’s capital city.\nA mentee of mine has been working on web scraping for NLP projects and her most recent target was Twitter. She’s working on something cool (stay tuned) but in the meantime, I thought I’d share a few of my own experiments. You can follow along and see full code examples in this colab notebook."
  },
  {
    "objectID": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#scraping-tweets-with-twint",
    "href": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#scraping-tweets-with-twint",
    "title": "Playing with Tweet Sentiment Analysis",
    "section": "Scraping Tweets with Twint",
    "text": "Scraping Tweets with Twint\n\nScraping tweets from a specific user\nI used twint - a scraper written in Python which gives a lot of functionality while avoiding the need for API keys, authentication etc. You can target specific users, locations, topics and dates (see their wiki for details) which makes this a powerful tool for finding and downloading tweets. For my tests today, I chose a few well-known Twitter personalities from my feed. I also scraped tweets from capital cities around the world, using the ‘Lang’ configuration option to focus on English tweets to make comparison easier (yes, I know, this is not ideal)."
  },
  {
    "objectID": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#sentiment-score-with-roberta",
    "href": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#sentiment-score-with-roberta",
    "title": "Playing with Tweet Sentiment Analysis",
    "section": "Sentiment Score with roBERTa",
    "text": "Sentiment Score with roBERTa\nNLTK’s SIA can give a quick and easy sentiment score for a piece of text, but many tweets use more obscure language and styles that aren’t well-captured by the default lexicon or the approach as a whole. Luckily, tweet sentiment analysis is a popular task and there are pre-trained deep learning models available that do a pretty good job out-of-the-box. I used a roBERTa model fine-tuned on the TweetEval task. The model card on huggingface had all the code needed to classify a piece of text, making it very simple to get started. I’m so glad this trend of making models accessible with key info is catching on!\n\n\n\n\n\n\nThe model outputs three scores corresponding to the labels ‘negative’, ‘neutral’ and ‘positive’. We can combine the positive and negative scores to get a combined sentiment score running from -1 (very negative) to +1 (very positive). From this, we can get stats like ‘average sentiment’, but I wanted a better way to see at a glance what a user’s tweets look like. Hexbin plots to the rescue :) These show the distribution of tweets in both sentiment and tweet length. You can see that Musk tends to tweet shorter, more neutral tweets while Gates favours mid-length positive ones and Lomborg tends heavily towards grumpy full-length rants 😂"
  },
  {
    "objectID": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#scoring-countries",
    "href": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#scoring-countries",
    "title": "Playing with Tweet Sentiment Analysis",
    "section": "Scoring Countries",
    "text": "Scoring Countries\nI was curious: what would we see if we grabbed some tweets from the capital city of each country and found the average sentiment score? Where do the positive tweeters live? Ideally, we’d account for different languages, grab a wide selection of tweets covering a longer timeline and do all sorts of other careful analyses. But since this entire project is the result of one night’s insomnia I just grabbed the latest 200 English tweets from each country’s capital (using the countryinfo library to get the coordinates) and went with those. Plotting the average sentiment as a choropleth map using Plotly gives us the title image of this post. Don’t read too much into this - it’s just a demo to show what might be possible with a bit more work."
  },
  {
    "objectID": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#conclusions",
    "href": "dsc/2021-11-09-playing-with-tweet-sentiment-analysis.html#conclusions",
    "title": "Playing with Tweet Sentiment Analysis",
    "section": "Conclusions",
    "text": "Conclusions\nData Science gives us the tools to ask questions about the world around us. And thanks to the kind folks who put so much effort into the libraries and tools we can access for free, it doesn’t have to be hard! I hope this post inspires you to ask your own questions. Feel free to modify and share the code, and PLEASE tag me on Twitter @johnowhitaker with your own visualizations and extensions. Happy scraping :)\nEDIT: I made a Huggingface space where you can try this for yourself: https://huggingface.co/spaces/johnowhitaker/twitter_viz"
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "",
    "text": "Following on from the last tutorial, this post will look at some ways we can improve our crop identification method. At the end of the last post, we were using a CART classifier to classify crops based on a greenest-pixel composite made from landsat 8 imagery. It didn’t do too well compared to other submissions, and the classifier was getting around 65% accuracy on the training data. Let’s start fixing some of the more obvious errors."
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#improving-the-input-data-for-the-classifier",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#improving-the-input-data-for-the-classifier",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "Improving the input data for the classifier",
    "text": "Improving the input data for the classifier\nUsing a greenest-pixel composite was an easy first step. However, the competition is focused on a single year (2017), while the composite image likely drew data from previous years. And, with a single composite image, any growth cycles or seasonal variation between the different crops is lost. This leads to our first major improvement: using images from different times of year and combining them into one input image that preserves the seasonal changes.\n\nBest quality landsat imagery from Jan-March 2017, one of the new model inputs\nThe new Earth Engine code filters the available Landsat imagery by date, splitting it into 4-month sections. The earliest high-quality imagery from each time period is selected (based on the code in this guide). Once this step is complete, the images are combined int a single new image that maintains the bands from each. The result is an image with 28 bands, which will be sampled and used by the model.\n\nMerging the images into one\nUsing the resultant merged image in place of the greenest-pixel composite, a CART classifier now achieves an accuracy of 76% on the training data, and scores 16.56 on the test data - an improvement over our previous score for this model. A randomForest classifier with 100 trees does even better, bringing the score down to 13.56, our new best."
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#training-models-and-making-predictions-locally-for-faster-iteration",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#training-models-and-making-predictions-locally-for-faster-iteration",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "Training models and making predictions locally for faster iteration",
    "text": "Training models and making predictions locally for faster iteration\nSo far, we’ve been using GEE’s classifiers and making predictions over the whole area, then sampling the predictions to get a single class as our final prediction. Instead, let’s sample the landsat data for each polygon in the train and test sets, download that data and use it to train models locally. This will be make experimenting with different models much faster.\nThe full code is here, and by taking the median value for each band of the merged image for each region of the training and test datasets, we get a pair of CSV files that we can easily load into Pandas for further analysis.\n\nLoading the data\nBefore experimenting with different models, optimizing parameters and so on, the first thing I tried was switching from predicting a single output class to predicting the probabilities that a given set of inputs belong to each of the different classes. Using the RandomForestClassifier from Scikit-learn, this is as simple as calling predict_proba(X) instead of predict(X). This gives a submission file much closer to the example provided by Zindi:\n\nPredicting probability for each class\nSo how does this new, improved submission score? 1.48! We’ve jumped from near-last to top 50% (15’th as of today) while still not using the provided satellite data!"
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#model-tuning",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#model-tuning",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "Model Tuning",
    "text": "Model Tuning\nJust for fun, let’s see how good we can get. Instead of submitting to Zindi to get a score (limited to 5 a day), we need a way to compare models locally, ideally with the same metric the contest uses. Fortunately, they’re open about the scoring method - it’s based on log-loss. By splitting the training data, using part to train a model and the rest to test it, we can get a rough idea of what out model would score:\n\nScoring a model with log_loss\nThe score depends on the test/train split. For better accuracy, we can average the scores with several different test/train splits. With a scoring method in place, we can start optimizing our models. As an example, we can pick the number of trees to use with the random forest model by plotting how the scores change with more estimators. In this case, anything above 200 looks to provide minimal extra advantage.\n\nWith Random Forest bottoming out at ~1.4 after some tweaking, I turned to XGBoost. A nice summary of tuning XGBoost can be found here. Starting with some suggested values and tweaking the max_depth and learning_rate parameters led me to a model that scored 1.15 in my tests - enough of an improvement that I made a submission using it’s predictions on Zindi. Score: 1.51. Pretty much the same as the Random Forest model."
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#combining-good-models---ensemble-modelling",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#combining-good-models---ensemble-modelling",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "Combining good models - Ensemble Modelling",
    "text": "Combining good models - Ensemble Modelling\nGiven several good models, can we get a better prediction by combining their outputs? This is a complex subject, but by simply taking the mean of the predictions made by my two best models, I achieved a score of 1.41 - 14’th place."
  },
  {
    "objectID": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#conclusions",
    "href": "dsc/2019-06-27-tutorial-improving-crop-type-predictions.html#conclusions",
    "title": "Tutorial: Improving Crop Type Predictions",
    "section": "Conclusions",
    "text": "Conclusions\nThis GitHub repository contains the training and test datasets I generated with sampled Landsat data, as well as explanatory notebooks containing all the code described in this post. Feel free to follow along, make improvements and try it yourself. The key to further score improvements will be feature engineering - trying imagery from different time periods, adding features for plot area, distance to river, variation within the field etc. Lowering the scale variable in GEE to 30 will give slightly better data, as will sampling from the central areas of the fields. If I try any of these, I’ll update this post.\nFor now, however, I am content. We’ve seen that it is possible to perform the specified task (crop identification) using nothing but some free Landsat data in GEE and some open source libraries to do the ML heavy lifting. While the fancy imagery provided is no doubt useful (see the top scores as evidence of this), this exercise shows that it is not essential to this kind of analysis. I hope that it inspires some of you to see what else is possible."
  },
  {
    "objectID": "dsc/2020-02-22-self-supervised-learning-with-image网.html",
    "href": "dsc/2020-02-22-self-supervised-learning-with-image网.html",
    "title": "Self-Supervised Learning with Image网",
    "section": "",
    "text": "Until fairly recently, deep learning models needed a LOT of data to get decent performance. Then came an innovation called transfer learning, which we’ve covered in some previous posts. We train a network once on a huge dataset (such as ImageNet, or the entire text of Wikipedia), and it learns all kinds of useful features. We can then retrain or ‘fine-tune’ this pretrained model on a new task (say, elephant vs zebra), and get incredible accuracy with fairly small training sets. But what do we do when there isn’t a pretrained model available?\n\nPretext tasks (left) vs downstream task (right). I think I need to develop this style of illustration - how else will readers know that this blog is just a random dude writing on weekends? :)\nEnter Self-Supervised Learning (SSL). The idea here is that in some domains, there may not be vast amounts of labeled data, but there may be an abundance of unlabeled data. Can we take advantage of this by using it somehow to train a model that, as with transfer learning, can then be re-trained for a new task on a small dataset? It turns out the answer is yes - and it’s shaking things up in a big way. This fastai blog post gives a nice breakdown of SSL, and shows some examples of ‘pretext tasks’ - tasks we can use to train a network on unlabeled data. In this post, we’ll try it for ourselves!\nFollow along in the companion notebook."
  },
  {
    "objectID": "dsc/2020-02-22-self-supervised-learning-with-image网.html#image网",
    "href": "dsc/2020-02-22-self-supervised-learning-with-image网.html#image网",
    "title": "Self-Supervised Learning with Image网",
    "section": "Image网",
    "text": "Image网\nRead the literature on computer vision, and you’ll see that ImageNet has become THE way to show off your new algorithm. Which is great, but coming in at 1.3 million images, it’s a little tricky for the average person to play with. To get around this, some folks are turning to smaller subsets of ImageNet for early experimentation - if something works well in small scale tests, *then* we can try it in the big leagues. Leading this trend have been Jeremy Howard and the fastai team, who often use ImageNette (10 easy classes from ImageNet), ImageWoof (Some dog breeds from ImageNet) and most recently Image网 (‘ImageWang’, 网 being ‘net’ in Chinese).\nImage网 contains some images from both ImageNette and ImageWoof, but with a twist: only 10% of the images are labeled to use for training. The remainder are in a folder, unsup, specifically for use in unsupervised learning. We’ll be using this dataset to try our hand at self-supervised learning, using the unlabeled images to train our network on a pretext task before trying classification."
  },
  {
    "objectID": "dsc/2020-02-22-self-supervised-learning-with-image网.html#defining-our-pretext-task",
    "href": "dsc/2020-02-22-self-supervised-learning-with-image网.html#defining-our-pretext-task",
    "title": "Self-Supervised Learning with Image网",
    "section": "Defining Our Pretext Task",
    "text": "Defining Our Pretext Task\nA pretext task should be one that forces the network to learn underlying patterns in the data. This is a new enough field that new ideas are being tried all the time, and I believe that a key skill in the future will be coming up with pretext tasks in different domains. For images, there are some options explained well in this fastai blog. Options include:\n\nColorization of greyscale images\nClassifying corrupted images\nImage In-painting (filling in ‘cutouts’ in the image)\nSolving jigsaws\n\nFor fun, I came up with a variant of the image in-painting task that combines it with colorization. Several sections of the input image are blurred and turned greyscale. The network tries to replace these regions with sensible values, with the goal being to have the output match the original image as closely as possible. One reason I like the idea of this as a pretext task is that we humans get something similar. Each time we move our eyes, things that were in our blurry, greyscale peripheral vision are brought into sharp focus in our central vision - another input for the part of our brain that’s been pretending they were full HD color the whole time :)\nHere are some examples of the grey-blurred images and the desired outputs:\n\nInput/Output pairs for our pretext task, using the RandomGreyBlur transform\nWe train our network on this task for 15 epochs, and then save its parameters for later use in the downstream task. See the notebook for implementation details."
  },
  {
    "objectID": "dsc/2020-02-22-self-supervised-learning-with-image网.html#downstream-task-image-classification",
    "href": "dsc/2020-02-22-self-supervised-learning-with-image网.html#downstream-task-image-classification",
    "title": "Self-Supervised Learning with Image网",
    "section": "Downstream Task: Image Classification",
    "text": "Downstream Task: Image Classification\nNow comes the fun part: seeing if our pretext task is of any use! We’ll follow the structure of the Image网 leaderboard here, looking at models for different image sizes trained with 5, 20, 80 or 200 epochs. The theory here is that we’d hope that out pretext task has given us a decent network, so we should get some results after 5 epochs, and keep getting better and better results with more training.\n\nResults from early testing\nThe notebook goes through the process, training models on the labeled data provided with Image网 and scoring them on the validation set. This step can be quite tedious, but the 5-epoch models are enough to show that we’ve made an improvement on the baseline, which is pretty exciting. For training runs 20 epochs and greater, we still beat a baseline with no pre-training, but fall behind the current leaderboard entry based on simple inpainting. There is much tweaking to be done, and the runs take ~1 minute per epoch, so I’ll update this when I have more results."
  },
  {
    "objectID": "dsc/2020-02-22-self-supervised-learning-with-image网.html#where-next",
    "href": "dsc/2020-02-22-self-supervised-learning-with-image网.html#where-next",
    "title": "Self-Supervised Learning with Image网",
    "section": "Where Next?",
    "text": "Where Next?\nImage网 is fairly new, and the leaderboard still needs filling in. Now is your chance for fame! Play with different pretext tasks (for eg, try just greyscale instead of blurred greyscale - it’s a single line of code to change), or tweak some of the parameters in the notebook and see if you can get a better score. And someone please do 256px?\nBeyond this toy example, remember that unlabeled data can be a useful asset, especially if labeled data is sparse. If you’re ever facing a domain where a pretrained model is unavailable, self-supervised learning might come to your rescue."
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html",
    "title": "A Recipe for Training Good Generative Models",
    "section": "",
    "text": "Generative models are all the rage at the moment, and quality seems to be skyrocketing across the board. In this post, I share what I’m realizing is *the* key recipe that is powering the best models at the moment."
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#pre-train-on-lots-of-data",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#pre-train-on-lots-of-data",
    "title": "A Recipe for Training Good Generative Models",
    "section": "1) Pre-train on LOTS of data",
    "text": "1) Pre-train on LOTS of data\n\nA robot working hard to read every book possible\nThis makes sense - more data = better, right? And so we see language models training on every scrap of text they can find - books, video transcripts, the entire internet. In the text-to-image domain, datasets like LAION contain billions of images, scraped from the web. This stage is necessary if you want your model to have an ‘understanding’ of as many topics as possible."
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#fine-tune-with-high-quality-data",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#fine-tune-with-high-quality-data",
    "title": "A Recipe for Training Good Generative Models",
    "section": "2) Fine-Tune with HIGH-QUALITY data",
    "text": "2) Fine-Tune with HIGH-QUALITY data\nAfter (1), your model can hopefully produce anything. The downside is that it can produce, well, anything! Including badly spelled Reddit rants (for text models), or low-quality PowerPoint screenshots (for image models). For text-to-image models, another aspect of ‘high-quality’ data is image-caption alignment. If captions don’t match images very well, the model will learn to rely less on the prompt when generating. The fix is to continue training on ‘better’ data, to bias the model toward generating the good stuff. You’re not necessarily teaching it much new in this stage - just pushing it towards a subset of the possibilities it could already generate.\n\nSD 1.5 trained on ~1B samples - “A professional photo of an astronaut riding a horse on the moon”\n\nSD1.5 fine-tuned on ~1M ‘good’ samples\nFor a model like ChatGPT, this step involves manually finding or creating high-quality examples of chat dialogs. For something like Midjourney, it presumably involves collecting a dataset of stunning-looking images and making sure they have good captions (either by filtering out existing captions or by using auto-generated captions). Next time you read about a cool new generative model, keep an eye out for mention of this ‘high-quality fine tune’ step. For example, in this post on the new Kandinsky 2.1 text-to-image model, they note that after training on a large dataset “Further, at the stage of fine-tuning, a dataset of 2M very high-quality high-resolution images with descriptions … was used separately collected from open sources.”"
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#incorporate-human-feedback",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#incorporate-human-feedback",
    "title": "A Recipe for Training Good Generative Models",
    "section": "3) Incorporate HUMAN FEEDBACK",
    "text": "3) Incorporate HUMAN FEEDBACK\n\n\n\nMost apps have a way for users to provide feedback on generations or to explicitly rank images/texts\n(1), or maybe (1) + (2), will get you great results on automatic evaluations and benchmarks, and may be enough for getting a publication with a SOTA result. However, successful products require pleasing users, so making sure the model creates things that users like is a big deal. Midjourney is a great example - they’ve been collecting user feedback since day 1, and presumably using said feedback to improve their models. Apart from explicit ratings, there are also other ways to get user feedback - for example, when a user selects one of four possible images to download or upscale they provide a signal that can be used to estimate their preference:\nhttps://twitter.com/johnowhitaker/status/1640608207730081792?s=20\nThe exact method for incorporating this feedback varies. For text, the standard approach is to do something called “Reinforcement Learning from Human Feedback” (RLHF) where a large number of human-labeled outputs are used to train a ‘reward model’ that scores a given generation. This model is then used to train the generative model, evaluating its outputs and providing a signal which is then used to update the model such that it produces better ones according to the reward model. You could also use this kind of preference model to filter out low-quality data (feeding back into (2)) or to condition your model on quality, such that at inference time you can simply ask for 10/10 generations! Whatever the method used, this final stage is once again not teaching the model anything new but is instead ‘aligning’ the model such that its outputs more often look like something humans will like."
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#cheating",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#cheating",
    "title": "A Recipe for Training Good Generative Models",
    "section": "‘Cheating’",
    "text": "‘Cheating’\nOpenAI spent tons of money and compute doing their supervised fine-tuning and RLHF magic to create ChatGPT and friends. Facebook released a research preview of LLaMa, a family of models trained on more than a billion tokens. The LlaMa models have only had step (1) applied, and aren’t great out-of-the-box for chat applications. Then along come various groups with access to OpenAI’s models via API, who created a training dataset based on ChatGPT outputs. It turns out that fine-tuning LlaMa on this data is a quick way to get a high-quality chatbot! A similar dynamic is playing out with various open-source models being trained on Midjourney outputs. By copy-catting powerful models, it is possible to skip (2) and (3) to a large extent, leaving the companies investing so much in their models in an interesting position. It will be interesting to see how this plays out going forward…"
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#conclusions",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#conclusions",
    "title": "A Recipe for Training Good Generative Models",
    "section": "Conclusions",
    "text": "Conclusions\nThis recipe isn’t necessarily new. The ULMFiT paper from Jeremy Howard and Sebastian Ruder in 2018 did something similar, where they pre-train a language model on a large dataset (1), fine-tune it on industry-specific data (2), and then re-train for a specific task such as classification. That said, I feel like this year we’re seeing it really pay dividends as apps like ChatGPT reach hundreds of millions of people and companies scramble to offer free access to powerful models in exchange for that all-important user preference data. Excitingly, there are open-source efforts to collect the necessary data too - see the PickAPic effort (for images) or the Open Assistant project (for chat data) among many others. And open source models such as stable diffusion let others skip the expensive pre-training phase and move straight to fine-tuning, lowering the barrier to entry substantially."
  },
  {
    "objectID": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#ps-whos-feedback",
    "href": "dsc/2023-04-06-a-recipe-for-training-good-generative-models.html#ps-whos-feedback",
    "title": "A Recipe for Training Good Generative Models",
    "section": "PS: Who’s Feedback?",
    "text": "PS: Who’s Feedback?\n\nImages generated with the generic prompt “lol” in Midjourney\nSomething worth mentioning (thanks @LuciaCKun for highlighting this) is that using human feedback has some ethical considerations. It could be that a small set of people (employees of a company, or early testers) get to spend time telling a model “This is good, that is bad”, and their biases end up defining the behavior of the model for everyone. You see this with images - anything trained on early user preferences for text-to-image models is likely to skew toward fantasy women and art-station aesthetics. Finding ways to align with different people’s values rather than locking in a specific behavior is an active area of research, in which there is plenty of work left to do."
  },
  {
    "objectID": "dsc/2020-02-05-meta-data-glimpse-google-dataset-search.html",
    "href": "dsc/2020-02-05-meta-data-glimpse-google-dataset-search.html",
    "title": "Meta ‘Data Glimpse’ - Google Dataset Search",
    "section": "",
    "text": "Christmas came in January this year, with Google’s release of ‘Dataset Search’. They’ve indexed millions of cool datasets and made it easy to search through them. This post isn’t about any specific dataset, but rather I just wanted to share this epic new resource with you.\n\n\n\n\n\nGoogle’s Dataset Search\nI saw the news as it came out, which meant I had the pleasure of sharing it with my colleagues - all of whom got nerd sniped to some degree, likely resulting a much loss of revenue and a ton of fun had by all :) A few minutes after clicking the link I was clustering dolphin vocalizations and smiling to myself. If you’re ever looking for an experiment to write up, have a trawl through the datasets on there and pick one that hasn’t got much ML baggage attached - you’ll have a nice novel project to brag about.\n\nClustering Dolphin noises\nSay what you like about Google, there are people there doing so much to push research forward. Tools like Colab, Google Scholar, and now Dataset Search make it easy to do some pretty amazing research from anywhere. So go on - dive in :)"
  },
  {
    "objectID": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html",
    "href": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html",
    "title": "Pothole Detection (aka Johno tries fastai)",
    "section": "",
    "text": "This week saw folks from all over the AI space converge in Cape Town for the AI Expo. The conference was inspiring, and I had a great time chatting to all sorts of interesting people. There were so many different things happening (which I’m not going to cover here), but the one that led to this post was a hackathon run by Zindi for their most recent Knowledge competition: the MIIA Pothole Image Classification Challenge. This post will cover the basic approach used by many entrants (thanks to Jan Marais’ excellent starting notebook) and how I improved on it with a few tweaks. Let’s dive in."
  },
  {
    "objectID": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#the-challenge",
    "href": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#the-challenge",
    "title": "Pothole Detection (aka Johno tries fastai)",
    "section": "The Challenge",
    "text": "The Challenge\nThe dataset consists of images taken from behind the dashboard of a car. Some images contain potholes, some don’t - the goal is to correctly discern between the two classes. Some example pictures:\n\n\n\n\n\n\n\nTrain and test data were collected on different days, and at first glance it looks like this will be a tough challenge! It looks like the camera is sometimes at different angles (maybe to get a better view of potholes) and the lighting changes from pic to pic."
  },
  {
    "objectID": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#the-first-solution",
    "href": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#the-first-solution",
    "title": "Pothole Detection (aka Johno tries fastai)",
    "section": "The first solution",
    "text": "The first solution\nJan won a previous iteration of this hackathon, and was kind enough to share a starting notebook (available here) with code to get up and running. You can view the notebook for the full code, but the steps are both simple and incredibly powerful:\n\nLoad the data into a ‘databunch’, containing both the labeled training data and the unlabeled test data. Using 15% of the training data as a validation set. The images are scaled to 224px squares and grouped into batches.\n\n\nThe images are automatically warped randomly each time (to make the model more robust). This can be configured, but the default is pretty good.\n\nCreate a model: learn = cnn_learner(data, resnet18, metrics=accuracy). This single line does a lot! It downloads a pre-trained network (resnet18) that has already been optimised for image classification. It reconfigures the output of that network to match the number of classes in our problem. It links the model to the data, freezes the weights of the internal layers, and gives us a model ready for re-training on our own classes.\nPick a learning rate, by calling learn.lr_find() followed by learn.recorder.plot() and picking one just before the graph bottoms out (OK, it’s more complicated than that but you can learn the arcane art of lr optimization elsewhere)\n\n\n*sucks thumb* A learning rate of 0.05 looks fine to me Bob.\n\nFit the model with learn.fit_one_cycle(3, lr) (Change number of epochs from 3 to taste), make predictions, submit!\n\nThere is some extra glue code to format things correctly, find the data and so on. But this is in essence a full image classification workflow, in a deceptively easy package. Following the notebook results in a log-loss score of ~0.56, which was on par with the top few entries on the leaderboard at the start of the hackathon. In the starter notebook Jan gave some suggestions for ways to improve, and it looks like the winners tried a few of those. The best score of the day was Ivor (Congrats!!) with a log-loss of 0.46. Prizes were won, fun was had and we all learned how easy it can be to build an image classifier by standing on the shoulders of giants."
  },
  {
    "objectID": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#making-it-better",
    "href": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#making-it-better",
    "title": "Pothole Detection (aka Johno tries fastai)",
    "section": "Making it better",
    "text": "Making it better\nAs the day kicked off, I dropped a few hints about taking a look at the images themselves and seeing how one could get rid of unnecessary information. An obvious answer would be to crop the images a little - there aren’t potholes in the dashboard or the sky! I don’t think anyone tried it, so let’s give it a go now and see where we get. One StackOverflow page later, I had code to crop and warp an image:\n\nBefore and after warping. Now the road is the focus, and we’re not wasting effort on the periphery.\nI ran my code to warp all the images and store them in a new folder. Then I basically re-ran Jan’s starting notebook using the warped images (scaled to 200x200), trained for 5 epochs with a learning rate of 0.1, made predictions and…. 0.367 - straight to the top of the leader-board. The image warping and training took 1.5 hours on my poor little laptop CPU, which sort of limits how much iterating I’m willing to do. Fortunately, Google Colab gives a free GPU, cutting that time to a few minutes."
  },
  {
    "objectID": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#conclusions",
    "href": "dsc/2019-09-06-pothole-detection-aka-johno-tries-fastai.html#conclusions",
    "title": "Pothole Detection (aka Johno tries fastai)",
    "section": "Conclusions",
    "text": "Conclusions\n\nMy time in the sun\nThanks to Google’s compute, it didn’t take long to have an even better model. I leave it to you dear readers to figure out what tweaks you’ll need to hop into that top spot.\nMy key takeaway from this is how easy it’s become to do this sort of thing. The other day I found code from 2014 where I was trying to spot things in an image with a kludged-together neural network. The difference between that and today’s exercise, using a network trained on millions of images and adapting it with ease thanks to a cool library and a great starting point… it just blows my mind how much progress has been made.\nWhy are you still reading this? Go enter the competition already! :)"
  },
  {
    "objectID": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html",
    "href": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html",
    "title": "WhistleGen: Generating Traditional Irish music with ML",
    "section": "",
    "text": "https://www.youtube.com/watch?v=B7SEP2p4Y1A\nVideo overview of this project - do check out my channel if you like this!\nEarlier this year I did an experiment where I tried to write some code on a small, atomic project every day. The results are documented at https://johnowhitaker.github.io/days_of_code/. In this post I want to share one of my favorite little diversions - my attempt at teaching a computer to compose some whistle music!"
  },
  {
    "objectID": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#getting-the-data",
    "href": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#getting-the-data",
    "title": "WhistleGen: Generating Traditional Irish music with ML",
    "section": "Getting the Data",
    "text": "Getting the Data\nTo train a model we will need some data. Previous attempts at music generation have worked on midi, or raw audio. However, a lot of Irish music is shared in a simplified form called ‘ABC Notation’ using letters and a limited set of symbols to encode the essential melody and leaving embellishments, harmonies and accents largely up to the interpretation of the player. thesession.org is one large central repository of these tunes, but I couldn’t find an easy way to download them in bulk. Web Scraping to the rescue!\n\nA neat(ish) dataset of tunes in ABC notation\nYou can see the code and details here. Web scraping is one of those cases where there are many valid approaches one could take, but all of them in essence boil down to identifying ways of identifying the specific parts of the html code that surround the data you are interested in. For example, on a page of results from thesession each song is listed as a list item taking the form <li class=\"manifest-item\">. With a bit of patience we can get URLs for each tune and then scrape the relevant info from those URLs with some more effort. At the end of this process, a nice neat dataframe with the titles, metadata and note sequences."
  },
  {
    "objectID": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#modelling",
    "href": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#modelling",
    "title": "WhistleGen: Generating Traditional Irish music with ML",
    "section": "Modelling",
    "text": "Modelling\nWe’re going to train a ‘language mode’ - a concept from the field of NLP, where a model (usually an LSTM or transformer - based architecture) tries to predict the next token in a sequence, allowing it to learn from unstructured data such as large chunks of text or, in this case, music. The end result of this is a generative model that can ‘autocomplete’ sequences. These language models can then be re-purposed for classification, translation etc. but in this case we want a generative model so that is unnecessary.\nThe text needs to be tokenized. We can simply split into individual characters, but since the notation includes ‘note modifiers’ such as ‘=’ which are sometimes placed before a note to sharpen or flatten it and some other 2-character symbols (like ‘|:’ for the start of a bar with a repeat), I chose to build a custom tokenizer. The notebook shows how to construct fastai dataloaders that package everything up neatly ready for training.\n\nOnce the dataloaders are ready, we can simply train this like any other language model. I used the learning rate finder (output shown above) to pick an initial learning rate and then, following the example in the fastai docs, gradually unfroze the model and continued to train it. After a few minutes the model is predicting the next token with ~38% accuracy!"
  },
  {
    "objectID": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#getting-some-output",
    "href": "dsc/2021-05-13-whistlegen-generating-traditional-irish-music-with-ml.html#getting-some-output",
    "title": "WhistleGen: Generating Traditional Irish music with ML",
    "section": "Getting Some Output",
    "text": "Getting Some Output\n\nSome early WhistleGen output\nWe can feed our model a few tokens and ask it to continue making guesses for the next token in the sequence: learn.predict('|:G', 100, temperature=0.7). The temperature parameter controls how ‘conservative’ the model is; higher values result in output with more randomness. To convert the string of letters that the model spews out into playable music, I used this handy online editor to preview, edit and download the songs.\nThe model is OK at guessing sensible notes, but it doesn’t produce much in the way of song structure. I found it best to use the output as a starting point, tweaking the odd bit of timing and adding repeats, separate parts and the odd extra flourish to create a song that is effectively co-written by myself and my AI assistant. It’s surprisingly fun! I hope this inspires you to try something like this yourself - do let me know what you create."
  },
  {
    "objectID": "dsc/2021-11-24-aiaiart-course-retrospective.html",
    "href": "dsc/2021-11-24-aiaiart-course-retrospective.html",
    "title": "AIAIART Course Retrospective",
    "section": "",
    "text": "A small selection of images generated during the course\nA few weeks ago we wrapped up the first run of ‘AIAIART’, a short course on creating art with deep learning. The course was originally delivered over Discord, but you can access recordings of the lessons on YouTube alongside Colab Notebooks containing the code and examples.\nhttps://youtube.com/playlist?list=PL23FjyM69j910zCdDFVWcjSIKHbSB7NE8\nThe experience of putting this together and sharing it was highly enjoyable. I always get a kick out of seeing my code or teaching being used by other people to make cool stuff, and our Discord server is a steady stream of fun projects and experiments that make me so happy.\nIf I had to distil a few key takeaways I’ve gained from this endeavour, they would be\n\nOptimization is magic. Set things up so that  uses  to produce  which can be evaluated against  in a differentiable way, and suddenly you can iteratively update those parameters bit by bit until (if all goes well) you achieve said goal. The secret here is that code for updating an image to look more like a description is practically identical to the code for updating the parameters of a neural network to solve some complicated task. And so while we were busy making art, everyone was secretly learning the much broader skill of solving problems with optimization :)\nYou don’t need a PhD to dabble with deep learning. Quite a few students had been playing with various AI art models but hadn’t been able to dig in and understand the code or inner workings. But once we started building up from simple examples, it suddenly ‘clicked’ and what was previously intimidating walls of code became fancier versions of the patterns we’d already seen again and again.\nI really like teaching. Seeing that ‘aha’ moment makes me so happy - I’m going to have to find ways to do more of this :)\nPeople are SO COOL! I love seeing how different people can see the same material and get inspired to create wildly different things.\nAI art is SO COOL! We’re still at the beginning of this movement, but already there are such powerful and amazing models and techniques available to us. With a little bit of tinkering you cna learn how to make them sing, and the results can be simply stunning. I look forward to seeing where the next few generations of tech take us.\n\nAnyway, that’s about all I have for this post. Check out the videos or come and hang out in the discord to see what we’re playing with next, and stay tuned since I might turn this V1 course into something a little more polished over the Christmas holidays. Happy arting - J"
  },
  {
    "objectID": "dsc/2020-09-16-data-glimpse-predicted-historical-air-quality-for-african-cities.html",
    "href": "dsc/2020-09-16-data-glimpse-predicted-historical-air-quality-for-african-cities.html",
    "title": "Data Glimpse: Predicted Historical Air Quality for African Cities",
    "section": "",
    "text": "Air quality has been in the news a lot recently. Smoke from fires has had thousands of Californians searching for info around the health hazards of particulate matter pollution. Lockdown-induced changes have shown how a reduction in transport use can bring a breath of fresh air to cities. And a respiratory virus sweeping the globe has brought forward discussions around lung health and pollution, and the health burden associated with exposure to unhealthy levels of pollutants. There are thousands of air quality sensors around the world, but if you view a map of these sensors, it’s painfully obvious that some areas are underserved, with a marked lack of data:\n\nAir Quality from sensors around the world. Source: https://waqi.info/\nThe ‘gap in the map’ was the motivation for a weekend hackathon hosted through Zindi, which challenged participants to build a model capable of predicting air quality (specifically PM25 concentration) based on available satellite and weather data.\nThe hackathon was a success, and was enough of a proof-of-concept that we decided to put a little more work into taking the results and turning them into something useful. Myself and Yasin Ayami spent a bit of time re-creating the initial data preparation phase (pulling the data from the Sentinel 5P satellite data collections in Google Earth Engine, creating a larger training set of known air quality readings etc) and then we trained a model inspired by the winning solutions that is able to predict historical air quality with a mean absolute error of less than 20.\n\nDashboard for exploring air quality across Africa (http://www.datasciencecastnet.com/airq/)\nA full report along with notebooks and explanation can be found in this GitHub repository. But the good news is that you don’t need to re-create the whole process if you’d just like a look at the model outputs - those predictions are available in the repository as well. For example, to get the predictions for major cities across Africa you can download and explore this CSV file. And if you don’t want to download anything, I’ve also made a quick dashboard to show the data, both as a time-series for whatever city you want to view and as a map showing the average for all the locations.\nI’ve tagged this post as a ‘Data Glimpse’ since the details are already written up elsewhere :) I hope it’s of interest, and as always let me know if you have any questions around this. J."
  },
  {
    "objectID": "dsc/2021-05-13-in-brief-playing-with-class-imbalance.html",
    "href": "dsc/2021-05-13-in-brief-playing-with-class-imbalance.html",
    "title": "In Brief: Playing with Class Imbalance",
    "section": "",
    "text": "We often encounter imbalanced data in the world of machine learning, and have to decide how best to handle this. In ‘real life’ it is up to us to decide how to evaluate performance, which types of errors we care the most about and so on. But in the example we’ll look at today, the situation is slightly different: we have an imbalanced training set, and the test set (we’re working with this competition) has had the class distribution modified to make it more balanced. So, we need to find a way to take this into account when submitting predictions. The following plot shows the difference in distributions:\n\nClass distribution of the training set compared to the validation set\nIt’s worth pointing out that this is showing the results for the validation set - there is an unseen test set that could very well have it’s own slightly different class distribution. There isn’t much to say that wasn’t covered in the notebook, so check that out for implementation details. That said, let’s go over the main strategies we could use:\n\nDo nothing and hope for the best… Not great, but when the imbalance is small then some models are pretty decent at making sensible predictions. This isn’t going to win any competitions though!\nDrop some fraction of the majority class. This turned out to work surprisingly well - I suspect this mimics the steps the organizers took when preparing the data.\nGenerate some extra ‘synthetic’ samples for the under-represented class using Synthetic Minority Oversampling Technique (SMOTE)\nCombine the steps 2 and 3, to avoid relying on too much synthetic data. In this case I chose to use the imblearn library’s RandomunderSampler to discard some of the majority class.\nTake advantage of the sample_weights parameter available in some models. For example, with Catboost we can explicitly tell the model to assign less weight to samples from the majority class. This lets us use the whole dataset (no need to throw out perfectly good data) and it performed the best in some experiments, loosing only to the basic under-sampling technique in the final assessment.\n\n\nDropping 5/6 of the rows from the majority class - a frustratingly successful approach!\nAgain, check out the notebook for details and code. Here are the results:\n\n\n\n\n\nStrategy\n\n\nLog Loss (local)\n\n\n\n\nUnder-sampling the majority class\n\n\n0.556998\n\n\n\n\nCatBoost with altered sample weights\n\n\n0.559395\n\n\n\n\nSMOTE + RandomUnderSampler\n\n\n0.579939\n\n\n\n\nNo modifications\n\n\n0.674555\n\n\n\n\n\nResults\nThe big takeaway here for me was that getting this right makes a huge difference in these types of competition. Without a good strategy even the fanciest model has no hope of matching the top submissions. Fortunately, even basic under-sampling can get great results, and I hope that between my notebook and discussions from others sharing their tips we have an even playing field on this front, allowing competitors to work on the more interesting aspects like feature engineering."
  },
  {
    "objectID": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html",
    "href": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html",
    "title": "Why and how I’m shifting focus to LLMs",
    "section": "",
    "text": "While I’ve previously consulted on NLP projects, in the past few years my research focus has been chiefly on images. If you had asked me a few months ago about looking at LLMs, my default response would have been “No way, I bet there are far too many people working on that hyped topic”. But then my research buddies (a crew originally put together by Jeremy Howard to look into diffusion models) switched focus to LLMs, a friend started trying to convince me to join him in starting an LLM-focused company, and I began to re-think my hesitancy. In this blog post, I’ll try to unpack why I’m now excited to shift focus to LLMs, despite my initial misgivings about moving into a crowded market. And then I’ll try to outline how I’ve gone about loading up my brain with relevant research so that I can become a useful contributor in this space as quickly as possible. Here goes!"
  },
  {
    "objectID": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#part-1-why-llms-are-exciting",
    "href": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#part-1-why-llms-are-exciting",
    "title": "Why and how I’m shifting focus to LLMs",
    "section": "Part 1: Why LLMs are exciting",
    "text": "Part 1: Why LLMs are exciting\nThe TL;DR of this section is that it turns out there is a lot of innovation happening in this space, and lots of low-hanging fruit available in terms of improvements to be made, all driven by open-source models and the influx of new ideas. For a while, it felt like you needed to be at a big org with tons of compute to research LLM stuff and that OpenAI was just too far ahead for most things. Here are some specific thoughts I wrote out the other day when someone asked what I found exciting:\nThe explosion of interest in LLMs has led to a flurry of innovations around them. In particular, there are some cool techniques around lower-resource training and inference that I’m excited to see:\n- Quantization methods to reduce the VRAM required to train and serve larger models\n- Things like GGML for fast inference without any dependencies, optimized for things like Apple hardware and consumer GPUs (see Modular for a direction in which inference gets easy and fast on lots of different hardware)\n- Parameter-efficient fine-tuning methods that allow training with much less compute. It’s tricky to fine-tune the smallest GPT-2 model (125M parameters) on Google Colab when not using any tricks, and yet there are notebooks for SFT on Falcon 7B that can be run happily on the free tier thanks to LoRA and 8-bit Adam.\nThe upshot of all this is that it’s now doable to train variants of these fairly powerful open-source models with a single GPU in very little time and to share the resulting models (or the much smaller LoRA weights) through things like HuggingFace so that anyone can play with them. \nI think the next direction where things will rapidly improve is datasets for fine-tuning. We’ve already seen a big leap in quality over the past few months, with more and more chat / instruct datasets being curated. An obvious next step is using existing LLMs to generate better training data, and/or filter existing data. \nThe evaluation is lagging a little IMO. The open LLM leaderboard is a cool initiative, although it highlights how fragile these benchmarks can be. I’m excited about using LLMs to evaluate other LLMs (even though this can be precarious) and also hoping to see other alternatives emerge - perhaps something equivalent to the PickScore model that is a useful tool for evaluating image generators and is based on crowd-sourced ratings. Actual human ratings are still super important and hard to do well.\nAnother fun dynamic is just the constant stream of better base models available for fine-tuning as people compete to make ‘The BEST Truly Open Source Model’. Better base models mean better synthetic data, easier fine-tuning, more use-cases unlocked, more data as a result… it’s a nice flywheel of improvement. And since fine-tuning is getting faster and faster, when a great new base model comes out it won’t take long to apply the same dataset+training strategy as you’ve used on whatever current best model you have. \nIt feels like all these things make it easier than ever to do cool stuff with LLMs, but also that there are a lot of improvements still on the table - a good time to dive in!\nPS: Other interesting directions:\n\nWays to use multiple models of different sizes to speed up inference for ‘easy’ completions without sacrificing accuracy\nMoving away from RLHF towards something more like Direct Policy Optimization where you still incorporate feedback but without the RL messiness\nI still think very few people have good recipes for fine-tuning models and getting to know a specific model/task well would likely yield some interesting insights\n\nIt’s important to be able to iterate quickly for research to be effective, and when testing an idea meant training an LLM for weeks on tons of GPUs I was not excited. But now that we can potentially tune a good open-source base model on a single machine it seems like we might be close to rapid iterations especially if we just focus on the fine-tuning/alignment/tweaking steps or inference-time innovations. “LLMs are having their Stable Diffusion moment”."
  },
  {
    "objectID": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#part-2-how-the-heck-would-you-learn-llms",
    "href": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#part-2-how-the-heck-would-you-learn-llms",
    "title": "Why and how I’m shifting focus to LLMs",
    "section": "Part 2: How the heck would you ‘learn LLMs’?",
    "text": "Part 2: How the heck would you ‘learn LLMs’?\nI’ve been vaguely keeping up-to-date with the field for years - reading the big announcements and maybe skimming the odd paper here and there. But it had mostly been in a ‘wow PaLM seems cool’ style outsider mode, rather than taking in any details of architecture or training that might be needed to actually work with the darn things. So, step one: start catching up on all the cool tricks everyone knows, and seeing what gems are hidden in some lesser-known papers.\nThe secret sauce for this is our research group. Twice a week we meed at look through papers we find interesting. Often something recent sparks a specific line of inquiry. For example, there was some buzz on Twitter about the “Textbooks are all you need” paper that used synthetic data alongside heavy LLM-assisted filtering of existing training data to train very good small code models. This leads us to look into some prior work (e.g. the TinyStories paper by some of the same authors that tested similar ideas at a smaller scale) which in turn cites other papers which… Before you know it we have a Zotero library with 300+ papers and some ongoing experiments to start building our own intuition for some of the methods we found interesting.\nSome specific things I find extremely powerful about this group-study approach:\n\nTeaching others about something is an extremely good way to learn it, especially if your audience consists of people who can come up with questions to probe deeper and expand your collective understanding\nMore people => more chance for ‘aha’ moments where something suddenly clicks, which can then be explained back in a different way. It seems crazy, but we’ve bootstrapped our understanding of some extremely complex topics just by explaining an idea back and forth to each other in different ways until it really makes sense!\nMore people => more perspectives with different experiences to draw from. Someone shares a paper on document retrieval, which sparks a memory of a cool contrastive method used in images, which reminds someone of a paper aligning code and language from a few years ago, which brings up a nice benchmark we could use to test our new ideas…\nPractical experiments are great learning tools. Having multiple people tinkering with things or sharing proof-of-concept implementations is another major boost to understanding something.\n\nIt’s one thing to load up your brain with tons of papers and techniques, but that on its own isn’t quite enough to make you useful. So, for me, the next step is getting into the weeds with some actual projects. Run some models, try to train a few on some different tasks, dive into some larger projects… A lot of what I’ve done in this phase isn’t particularly new or interesting, but it builds the muscles for later stuff. If you’re bold you could find a way to get paid for this as a consultant, since everyone wants ‘talk to your docs’ bots and such! I have yet to cave in to that particular temptation, but I *AM* writing a book with some chapters devoted to LLMs (with some amazing co-authors to catch any glaring mistakes I make) which I guess is also killing two birds with one stone in terms of learning while (eventually, hypothetically) earning… And soon I may be full-time at the aforementioned LLM-based startup at which point it stops being ‘hacking around in my spare time’ and turns into ‘ML research’ with a proper job title and everything :)"
  },
  {
    "objectID": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#final-thoughts",
    "href": "dsc/2023-07-01-why-and-how-im-shifting-focus-to-llms.html#final-thoughts",
    "title": "Why and how I’m shifting focus to LLMs",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThis is a weird post, mostly me thinking out loud, but I hope you’ve found it interesting! I’ve gone from thinking LLMs are ‘solved and saturated’ to seeing all sorts of opportunities, and tons of ways someone with a novel perspective or a bit of luck can come in and contribute. So, wish me luck ;)"
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "",
    "text": "Prompt: ‘A sunset landscape painting, oil on canvas’ (fine-tuned Wikiart model)\nAs part of the Huggingface ‘#huggan’ event, I thought it would be interesting to fine-tune a latent diffusion model on the WikiArt dataset, which (as the name suggests) consists of paintings in various genres and styles."
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#what-is-cloob-conditioned-latent-diffusion",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#what-is-cloob-conditioned-latent-diffusion",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "What is CLOOB-Conditioned Latent Diffusion?",
    "text": "What is CLOOB-Conditioned Latent Diffusion?\nDiffusion models are getting a lot of fame at the moment thanks to GLIDE and DALL-E 2 which have recently rocked the internet with their astounding text-to-image capabilities. They are trained by gradually adding noise to an input image over a series of steps, and having the network predict how to ‘undo’ this process. If we start from pure noise and have the network progressively try to ‘fix’ the image we eventually end up with a nice looking output (if all is working well).\n\nAn illustration of this kind of model from the website related to one of the key papers that first outlined this idea.\nTo add text-to-image capacity to these models, they are often ‘conditioned’ on some representation of the captions that go along with the images. That is, in addition to seeing a noisy image, they also get an encoding of the text describing the image to help in the de-noising step. Starting from noise again but this time giving a description of the desired output image as the text conditioning ideally steers the network towards generating an image that matches the description.\n\nCLOOB architecture diagram (from the project page - which is worth a read!)\nDownsides: these diffusion models are computationally intensive to train, and require images with text labels. Latent diffusion models reduce the computational requirements by doing the denoising in the latent space of an autoencoder rather than on images directly. And since CLOOB maps both images and text to the same space, we can substitute the CLOOB encodings of the image itself in place of actual caption encodings if we want to train with unlabelled images. A neat trick if you ask me!\nThe best non-closed text-to-image implementation at the moment is probably the latent diffusion model trained by the CompVis team, which you can try out here."
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#trainingfine-tuning-a-model",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#trainingfine-tuning-a-model",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "Training/Fine-Tuning a model",
    "text": "Training/Fine-Tuning a model\n@JDP provides training code for CLOOB conditioned latent diffusion (https://github.com/JD-P/cloob-latent-diffusion) based on the similar CLIP conditioned diffusion trained by Katherine Crowson (https://github.com/crowsonkb/v-diffusion-pytorch). One of my #huggan team members, Théo Gigant, uploaded the WikiArt dataset to the huggingface hub, and the images were downloaded, resized and saved to a directory on a 2xA6000 GPU machine provided by Paperspace.\nAfter a few false starts figuring out model loading and other little quirks, we did a ~12 hour training run and logged the results using Weights and Biases. You can view demo outputs from the model as it trains in the report, which thanks to the W&B magic showed them live as the model was training, making for exciting viewing among our team :)"
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#evaluating-the-resulting-model",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#evaluating-the-resulting-model",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "Evaluating The Resulting Model",
    "text": "Evaluating The Resulting Model\nWikiArt is not a huge dataset relative to the model (which has over a billion parameters). One of the main things we were curious about was how the resulting model would be different from the one we started with, which was trained on a much larger and more diverse set of images. Has it ‘overfit’ to the point of being unuseable? How much more ‘arty’ do the results look when passing descriptions that don’t necessarily suggest fine art? And has fine-tuning on a relatively ‘clean’ dataset lowered the ability of the model to produce disturbing outputs? To answer these questions, we generated hundreds of images with both models.\n\n\n\nGenerated images from the prompts ‘winter landscape’, ‘autumn landscape’ and ‘spring landscape’ (WikiArt model). Note: all results are ‘painterly’ despite no allusion to paintings or art in the prompts. Seeds kept consistent for each set - note the slight similarity in overall structure for corresponding images.\nI’ve moved the side-by-side comparisons to a gallery at the end of this post. These were the key takeaways for me:\n\nStarting from a ‘photorealistic’ autoencoder didn’t stop it from making very painterly outputs. This was useful - we thought we might have to train our own autoencoder first as well.\nThe type of output definitely shifted, almost everything it makes looks like a painting\nIt lost a lot of more general concepts but does really well with styles/artists/image types present in the dataset. So landscape paintings are great, but ‘a frog’ is not going to give anything recognizable and ‘an avocado armchair’ is a complete fail :)\nIt may have over-fit, and this seems to have made it much less likely to generate disturbing content (at the expense of also being bad at a lot of other content types)."
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#closing-thoughts",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#closing-thoughts",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nApproaches like CLOOB-Conditioned Latent Diffusion are bringing down the barrier to entry and making it possible for individuals or small organisations to have a crack at training diffusion models without $$$ of compute.\n\nOur model during training (left) vs OpenAI’s DALL-E 2 (right) which was unveiled during our project and inspired various memes :)\nThis little experiment of ours has shown that it is possible to train one of these models on a relatively small dataset and end up with something that can create pleasing outputs, even if it can’t quite manage an avocado armchair. And as a bonus, it’s domain-focused enough that I’m happily sharing a live demo that anyone can play with online, without worrying that it’ll be used to generate any highly-realistic fake photographs of celebrity nudity or other such nonsense. What a time to be alive!"
  },
  {
    "objectID": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#comparison-images",
    "href": "dsc/2022-04-12-fine-tuning-a-cloob-conditioned-latent-diffusion-model-on-wikiart.html#comparison-images",
    "title": "Fine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt",
    "section": "Comparison images",
    "text": "Comparison images\n\nA watercolor painting of a rose\n\nAutumn watercolor\n\nAutumn landscape\n\nA Monet Pond\n\nA pink lilly\n\nTarsila do Amaral\n\nBlue and pink hydrangeas, impressionistic oils\n\nNew York skyline in winter\n\nA face, portrait in oils\n\nA female figure, charcoal\n\nThe moon over a landscape\n\nPeaceful Blue\nComparison images from our finetuned model (top) and the original model (bottom). Captions are the prompts used."
  },
  {
    "objectID": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html",
    "href": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html",
    "title": "Language Models for Protein Sequence Classification",
    "section": "",
    "text": "A 3D model of a protein kinase\nWe recently hosted a Zindi hackathon in partnership with Instadeep that challenged participants to predict the functional class of protein kinases (enzymes with some very specific functions in the cell) based on nothing but their amino acid sequences. This kind of sequence classification task has lots of potential applications - there is a lot of un-labelled data lying around on every computational biologist’s computer, and a tool that could guess a given protein’s function would be mighty handy.\nJust one problem - it’s not exactly a simple task! There are 20-something amino acids which we represent as letters. Given a sequence like ‘AGASGSUFOFBEASASSSSSASBBBDGDBA’ (frantically monkey-types for emphasis) we need to find a way to a) encode this as something a model can make sense of and b) do the making-sense-of-ing! Fortunately, there’s another field where we need to go from a string of letters to something meaningful: Natural Language Processing. Since I’d just been watching the NLP lesson in the latest amazing fastai course I felt obliged to try out the techniques Jeremy was talking about on this sequence classification task."
  },
  {
    "objectID": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#the-basic-approach",
    "href": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#the-basic-approach",
    "title": "Language Models for Protein Sequence Classification",
    "section": "The Basic Approach",
    "text": "The Basic Approach\n\nTokenized input (left) and class (right)\nTreating this as a language task and drawing inspiration from ULMFiT[1], this was my basic approach:\n\nI tokenized the sequences using ‘subword tokenization’ which captures not just individual amino acids as tokens but common groupings as well (eg ‘EELR’ is encoded as a single token). I think this basic approach was suggested by the SentencePiece paper[4] and it’s now part of fastai[5].\nI then created a ‘pretext task’ of sequence completion to train a ‘language model’ (based on the AWD-LSTM architecture[2]). The model learns to predict the next token in a sequence with ~32% accuracy - the hope is that in doing so it also learns useful embeddings and some sort of latent understanding of how these sequences are structured.\nWe keep most of this network as the ‘encoder’ but modify the final layers for the actual task: sequence classification. Thanks to the pre-training, the model can very quickly learn the new task. I can get to 98% accuracy in a couple of minutes by training on only a small subset of the data.\nTraining the model for the sequence classification task takes a while on the full competition dataset, but it eventually reaches 99.8% accuracy with a log_loss on the test set (as used in the competition) of 0.08, which is equivalent to 3rd place.\nDoing the normal tricks of ensembling, training a second model on reversed sequences etc quite easily bumps this up to glory territory, but that’s the boring bit.\n\nIt was fun to see how well this worked. You can find a more detailed write-up of the initial experiments on that competition dataset here. Spurred by these early results, I figured it was worth looking into this a little more deeply. What have others been doing on this task? Is this approach any good compared to the SOTA? Has anyone tried this particular flow on this kind of problem?"
  },
  {
    "objectID": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#getting-formal",
    "href": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#getting-formal",
    "title": "Language Models for Protein Sequence Classification",
    "section": "Getting Formal",
    "text": "Getting Formal\nIt should come as no surprise that the idea of treating sequence classification like a language modelling task has already occurred to some people. For example, USDMProt[7] turns out to have very nearly the same approach as that outlines above (self-five!). Their github is a great resource.\nThere are other approaches as well - for example, ProtCNN[6] and DEEPPred[8] propose their own deep learning architectures to solve these kinds of tasks. And there are some older approaches such as BLAST and it’s derivatives[9] that have long been standards in this field which still do decently (although they seem to be getting out-performed by these newer techniques).\nSo, we’re not the first to try this. However, I couldn’t find any papers using anything like the ‘subword’ tokenization. They either use individual amino acids as tokens, or in rare cases some choice of n-grams (for example, triplets of amino acids). The advantage of subword tokenization over these is that it can scale between the complexity of single-acid encodings and massive n-gram approaches by simply adjusting the vocabulary size."
  },
  {
    "objectID": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#your-homework",
    "href": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#your-homework",
    "title": "Language Models for Protein Sequence Classification",
    "section": "Your Homework",
    "text": "Your Homework\nI did some initial tests - this definitely smells promising, but there is a lot of work to do for this to be useful to anyone, and I don’t currently have the time or compute to give it a proper go. If you’re looking for a fun NLP challenge with the potential to turn into some interesting research, this could be the job for you! Here’s my suggestions:\n\nPick one or more benchmarks. Classification of the PFam dataset is a nice one to start with. The ProtCNN paper[6] (quick link) has done a bunch of the ‘standard’ algorithms and shared their split as a kaggle dataset, so you can quickly compare to those results.\nGet some data for language model training. The SWISSProt dataset is a nice one, and for early tests even just the PFam dataset is enough to try things out.\nTrain some language models. Do single-acid tokenization as a baseline and then try subword tokenization with a few different vocab sizes to compare.\nSee which models do best on the downstream classification task. Lots of experimenting to be done on sequence length, training regime and so on.\nFor bonus points, throw a transformer model or two at this kind of problem. I bet they’d be great, especially if pre-trained on a nice big dataset.\nIf (as I suspect) one of these does very well, document your findings, try everything again in case it was luck and publish it as a blog or, if you’re a masochist, a paper.\n… profit?\n\nI really hope someone reading this has the motivation to give this a go. If nothing else it’s a great learning project for language modelling and diving into a new domain. Please let me know if you’re interested - I’d love to chat, share ideas and send you the things I have tried. Good luck :)"
  },
  {
    "objectID": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#references",
    "href": "dsc/2020-10-30-language-models-for-protein-sequence-classification.html#references",
    "title": "Language Models for Protein Sequence Classification",
    "section": "References",
    "text": "References\n[1] - Howard, J. and Ruder, S., 2018. Universal language model fine-tuning for text classification. arXiv preprint arXiv:1801.06146.\n[2] - Merity, S., Keskar, N.S. and Socher, R., 2017. Regularizing and optimizing LSTM language models. arXiv preprint arXiv:1708.02182.\n[3] - Smith, L.N., 2017, March. Cyclical learning rates for training neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV) (pp. 464-472). IEEE.\n[4] - Kudo, T. and Richardson, J., 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.\n[5] - Howard, J. and Gugger, S., 2020. Fastai: A layered API for deep learning. Information, 11(2), p.108.\n[6] - Bileschi, M.L., Belanger, D., Bryant, D.H., Sanderson, T., Carter, B., Sculley, D., DePristo, M.A. and Colwell, L.J., 2019. Using deep learning to annotate the protein universe. bioRxiv, p.626507. (ProtCNN)\n[7] - Strodthoff, N., Wagner, P., Wenzel, M. and Samek, W., 2020. UDSMProt: universal deep sequence models for protein classification. Bioinformatics, 36(8), pp.2401-2409. (USDMProt)\n[8] - Rifaioglu, A.S., Doğan, T., Martin, M.J., Cetin-Atalay, R. and Atalay, V., 2019. DEEPred: automated protein function prediction with multi-task feed-forward deep neural networks. Scientific reports, 9(1), pp.1-16. (DEEPPred)\n[9] - Altschul, S.F., Madden, T.L., Schäffer, A.A., Zhang, J., Zhang, Z., Miller, W. and Lipman, D.J., 1997. Gapped BLAST and PSI-BLAST: a new generation of protein database search programs. Nucleic acids research, 25(17), pp.3389-3402."
  },
  {
    "objectID": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html",
    "href": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html",
    "title": "Zindi UberCT Part 2: Stepping Up",
    "section": "",
    "text": "In part 1, we looked at the SANRAL challenge on Zindi and got a simple first submission up on the leaderboard. In this tutorial I’ll show some extra features you can add on the road segments, bring in an external weather dataset, create a more complex model and give some hints on other things to try. Part 3 will hopefully add Uber movement data (waiting on the Oct 29 launch) and run through some GIS trickery to push this even further, but even without that you should be able to get a great score based on the first two tutorials.\nYou can follow along in the accompanying notebook, available here. Let’s dive in."
  },
  {
    "objectID": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#reading-a-shapefile-with-geopandas",
    "href": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#reading-a-shapefile-with-geopandas",
    "title": "Zindi UberCT Part 2: Stepping Up",
    "section": "Reading a shapefile with GeoPandas",
    "text": "Reading a shapefile with GeoPandas\n\nReading the data from the road_segments shapefile\nIf you unzip the road_segments.zip file downloaded from Zindi (!unzip road_segments.zip), you’ll find a group of files with all sorts of weird extensions: .shp, .shx, .dbf, .cpg…. What is all this? This is a standard format for geospatial vector data known as a shapefile. The .shp file is the key, while the others add important extra info such as attributes and shape properties. Fortunately, we don’t have to deal with these different files ourselves - the geopandas library makes it fairly simple (see above). Once we have the data in a dataframe, all we need to do is merge on segment_id (train = pd.merge(train, road_segments, on='segment_id', how='left') to get some juicy extra info in our training set. These new features include the number of lanes, the surface type, the segment length and condition… all useful inputs to our model."
  },
  {
    "objectID": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#finding-weather-data",
    "href": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#finding-weather-data",
    "title": "Zindi UberCT Part 2: Stepping Up",
    "section": "Finding weather data",
    "text": "Finding weather data\n\nZindi included a sentence on the data page: “You may use weather in your model. Please suggest weather datasets…”. I googled around and found rp5.ru - an excellent site that lets you download some historical weather data for locations around the globe. You’re welcome to check out the site, enter a date range, download, rename, etc. Or you can use my csv file, available here on github.\n\nWe can read the data from the CSV file and then link it to our training data with another simple merge command. The details are in the notebook. You can read about what the columns mean on the rp5.ru site. I the example I only use the numeric columns, but you could add extra features like wind direction, clouds_present etc based on the text components of this dataset."
  },
  {
    "objectID": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#deep-learning-for-tabular-data",
    "href": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#deep-learning-for-tabular-data",
    "title": "Zindi UberCT Part 2: Stepping Up",
    "section": "Deep learning for tabular data",
    "text": "Deep learning for tabular data\nI’ve recently been playing around a lot with the incredible fastai library. The course (fast.ai) will get you going quickly, and I highly recommend running through some of the examples there. In one of the lessons, Jeremy shows the use of a neural network on tabular data. This was traditionally fairly hard, and you had to deal with embeddings, normalization, overfitting….. Recently however, I’m seeing more and more use of these models for tabular data, thanks in no small part to fastai’s implementation that handles a lot of the complexity for you.\n\nUsing fastai’s tabular learner.\nI was going to go in-depth here with a tutorial, but honestly you’d be better off going to the source and seeing a lesson from Jeremy Howard (founding researcher at fast.ai) who takes you through dealing with tabular data as part of the aforementioned course. The relevant lesson is lesson 4, but if you have a few hours I’d suggest starting from the beginning."
  },
  {
    "objectID": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#how-far-have-we-come-and-where-do-we-go-next",
    "href": "dsc/2019-10-21-zindi-uberct-part-2-stepping-up.html#how-far-have-we-come-and-where-do-we-go-next",
    "title": "Zindi UberCT Part 2: Stepping Up",
    "section": "How far have we come, and where do we go next?",
    "text": "How far have we come, and where do we go next?\nI haven’t talked much about model scores or performance in this post. Is it worth adding all this extra data? And do these fancy neural networks do anything useful? Yes and yes - by making the improvements described above we take our score from 0.036 to 0.096, placing us just behind the top few entries.\nBut we have a secret weapon: the additional data! This score is achieved without making use of the vehicle counts per zone, the incident records or the vehicle data from SANRAL, and we haven’t even looked at Uber Movement yet.\nI’m going to wait on writing the next part of this series. So, dear reader (or readers, if this gets traction!), the baton lies with you. Add that extra data, get creative with your features, play with different models and let’s see how good we can get."
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html",
    "title": "ML and IR Tomography",
    "section": "",
    "text": "I studied Electrical and Computer Engineering at UCT, and the final year project was my chance to really dive deep into a topic. I chose IR tomography, and explored various questions around that topic. For today’s post, I’ll focus on one small aspect: the use of machine learning. This post will go through some background and then show a couple of ML models in use. For much more detail and my full thesis, see this GitHub repository."
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#background",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#background",
    "title": "ML and IR Tomography",
    "section": "Background",
    "text": "Background\nThis project arose because I wanted to do some experiments with Computed Tomography, but I didn’t know enough to see what would work and what wouldn’t. How many sensors does one need to achieve a particular goal for resolution or performance? What geometries work best? And what if we can’t keep everything nice and regular? \n\n\n\n\n\nI built some tools that let me simulate these kinds of arrangements, and did some early experiments on image reconstruction and on the use of machine learning (specifically neural networks) to make sense of readings. Even with a weird arrangement like the one on the right, I could make some sense of the data. For more information on the simulation side, see the report in the GitHub repository.\nI tested out these arrangements in the real world by building some fixed arrangements, and by using a 3D printed scanner to position an LED and a phototransistor (PT from now on) in different locations to slowly simulate having many detectors and emitters. Using light as opposed to X-rays means cheap emitters and detectors, and of course much less danger.\n\nA ring of 8 LEDs and 8 PTs. Larger rings were also constructed, and the scanner could simulate arrangements of >1000 sensors and emitters.\nBy taking a set of readings, we can start to estimate how much light travels along different paths, and thus build up an image of whatever is being scanned. This works well with lots of readings from the scanner:\n\nA reconstructed image of some small nails. The scanner could resolve objects less than 1mm in size.\nHowever, for arrangements with relatively few sensors (such as the static arrangement of 8 shown above), the reconstructed images are an apparently meaningless blur. The goal of this project was to use ML to make sense of these sets of readings, for example by identifying objects placed within the sensor ring or estimating their position."
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#model-selection",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#model-selection",
    "title": "ML and IR Tomography",
    "section": "Model Selection",
    "text": "Model Selection\nTo answer the question “can machine learning be useful”, I needed to pick a good approach to take. Simply throwing the data at a decision tree and then noting the performance wouldn’t cut it - every choice needs to be justified. I wrote a notebook explaining the process here, but the basics are as follows:\n\nDefine your problem (for example, classifying objects) and load the data\nPick a type of model to try (for example, Logistic Regression)\nTrain a model, and see how well it performs by splitting your data into training and testing sets. Use cross-validation to get more representative scores.\nTune the model parameters. For example, try different values on ‘gamma’ (a regularisation parameter) for a Support Vector based classifier.\nRepeat for different types of model, and compare the scores\n\n\nChoosing the optimum number of hidden layers for a Multi-Layer Perceptron model (Neural Network)\nFor example, in the case of object classification, a neural network approach worked best (of the models tested):\n\nModel scores on a classification task"
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#object-classification",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#object-classification",
    "title": "ML and IR Tomography",
    "section": "Object Classification",
    "text": "Object Classification\nUsing the ring with 8 LEDs and 8 PTs, I’d place an object randomly within the ring. The object (one of four used) and location (which of four ‘quadrants’ contained the object) were recorded along with a set of readings from the sensors. This data was stored in a csv file for later analysis.\nUsing the model selected according to the method in the previous section, I was able to achieve an accuracy of 85% (multi-class classification) or 97% (binary classification with only two objects) using 750 samples for training. More training data resulted in better accuracy.\n\nModel performance with more training samples for multi-class classification (orange)\nand binary classification (blue)\nThis was a fun result, and a good ML exercise. The data and a notebook showing the process of loading the data and training models can be found in the ‘Model Selection’ folder of the GitHub repository."
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#position-inference",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#position-inference",
    "title": "ML and IR Tomography",
    "section": "Position Inference",
    "text": "Position Inference\nHere, instead of trying to identify an object we attempt to predict it’s location. This requires knowing the position precisely when collecting training data - a problem I solved by using a 3D printer to move the object about under computer control.\n\nGathering training data for position inference\nThis results in a dataset consisting of a set of readings followed by an X and Y position. The goal is to train a model to predict the position based on the readings. For the ring of 8, the model could predict the location with an error of ~10% of the radius of the ring - approximately 7mm. For the ring of 14 (pictured above, and the source of the linked dataset), I was able to get the RMSE down to 1.6mm (despite the ring being larger) using the tricks from the next section. You can read more about this on my hackaday.io page.\n\nPlaying a game with the sensor ring.\nThe ring can take readings very fast, and being able to go from these readings to a fairly accurate position opens up some fun possibilities. I hooked it up to a game I had written. A player inserts a finger into the ring and moves it about to control a ‘spaceship’, which must dodge enemies to survive. It was a hit with my digs-mates at the time."
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#using-simulation-to-boost-performance",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#using-simulation-to-boost-performance",
    "title": "ML and IR Tomography",
    "section": "Using Simulation to boost performance",
    "text": "Using Simulation to boost performance\nOne downside of this approach is that it takes many training samples to get a model that performs adequately. It takes time to generate this training data, and in an industrial situation it might be impossible to simulate all possible positions in a reasonable time-frame. Since I already had a simulator I had coded, why not try to use it to generate some fake training data?\nUsing purely simulated data resulted in some spectacularly bad results, but if a model was ‘primed’ with even a small real-world training dataset (say, 50 samples) then adding simulated data could improve the model and make it more robust. I’ll let the results speak for themselves:\n\nModel performance for position inference with and without simulated data for training\nThe simulator didn’t map to real life exactly, and no doubt could be improved to offer even more performance gains. But even as-is, it allows us to use far less training data to achieve the same result. Notice that a model trained on 150 samples does worse than one using only 50 samples but augmented with extra simulated data. A nifty result to keep in mind if you’re ever faced with a dataset that’s just a little too small!"
  },
  {
    "objectID": "dsc/2019-05-17-ml-and-ir-tomography.html#conclusions",
    "href": "dsc/2019-05-17-ml-and-ir-tomography.html#conclusions",
    "title": "ML and IR Tomography",
    "section": "Conclusions",
    "text": "Conclusions\nI had a ton of fun on this project, and this post only really scratches the surface. If you’re keen to learn more, do take a look at the full report(PDF) and hackaday project. This is a great example of machine learning being used to get meaningful outputs from a set of noisy, complicated data. And it shows the potential for using simulation of complex processes to augment training data for better model performance - a very cool result.\nI’m thinking about moving this website in a different direction as I start on a new project - stay tuned for news!"
  },
  {
    "objectID": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html",
    "href": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html",
    "title": "Looking at traffic/congestion vs air quality AKA a quest for data",
    "section": "",
    "text": "I’m currently on a mission to explore the different datasets available publicly, and hopefully to add to that list. One key reason I’m passionate about this is that data often generates good questions. This post documents an example of this. Soon after seeing the air quality databases available on BigQuery, I started to think about if/how this relates to traffic. I had Uber movement in mind, but I was also curious what other traffic data is available. Once that initial question lodges in the mind, a journey begins."
  },
  {
    "objectID": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#first-try-uber-movement-travel-times-as-a-proxy-for-traffic-levels",
    "href": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#first-try-uber-movement-travel-times-as-a-proxy-for-traffic-levels",
    "title": "Looking at traffic/congestion vs air quality AKA a quest for data",
    "section": "First try: Uber movement travel times as a proxy for traffic levels",
    "text": "First try: Uber movement travel times as a proxy for traffic levels\nUber movement makes various data from millions of Uber rides available to the public. However, it isn’t particularly easy to estimate traffic from the available data. Daily average travel times are only available for a single, selected route at a time, and for a maximum of three months per download. The restrictions make sense, but are a little inconvenient. To get around this, I chose one route through the centre of my city of interest and decided to use this as a rough measure - longer travel times would hopefully translate to days with heavier traffic. To get better estimates, one could repeat this for many different routes to improve the metric.\n\nAverage uber ride travel times - a rough proxy for traffic conditions?\nI initially selected Nairobi for my investigation, since Uber data is available there and it seemed like air quality data was available as well. However, looking more closely at the air quality data revealed that it is sparse and there was almost none available for the dates that uber movement data had been gathered. So, with regret, I moved my initial exploration to the States, where air quality data has been gathered in many locations for decades - just one more way in which Africa lags behind in terms of data availability.\nI chose Pittsburgh, since it seemed as good a place as any when I looked at the list of cities with Uber Movement data. As before, I picked a route through town and downloaded the average daily travel times from movement.uber.com. To get the air quality data, I turned to Google Bigquery. The following code pulls the relevant data from the giant (>1GB) dataset:\n\nQuerying the epa historical air quality dataset\nThe resultant 1MB csv can be downloaded and loaded into a pandas dataframe for analysis. Combining it with the uber data meant it was time for the moment of truth: is there a correlation between travel times (as a measure of traffic intensity) and air quality? Let’s plot the two:\n\nAir quality (X axis) vs mean travel times (y axis)\nIf anything, there was a tiny negative correlation. But the main issue is the quality of the data. A single route is probably not a good metric for traffic as a whole. Less than a year’s worth of data is not great. This ignores co-factors such as weather. Etc, etc. Can we do better?"
  },
  {
    "objectID": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#take-two-better-traffic-data",
    "href": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#take-two-better-traffic-data",
    "title": "Looking at traffic/congestion vs air quality AKA a quest for data",
    "section": "Take Two: Better traffic data",
    "text": "Take Two: Better traffic data\nI plan on looking deeper into Uber Movement data in the future, but for this quick project I wanted a better source of traffic data to answer my initial question. Fortunately, the wonderful City of Chicago has a treasure-trove of data available: https://data.cityofchicago.org/. Their historical traffic data comes from sensor-laden busses tracking traffic speed. The dataset is fairly large, so to avoid taxing my Zimbabwean internet connection I used Google Colab to download the data and upload it to BigQuery for later. I could also start playing with the data in Colab’s notebook interface:\n\nLoading the data into pandas with Google Colab\nA description of the dataset from the data portal:\n\nI processed the data to get an average speed over all regions for each day. This, combined with the historical air quality measurements from the EPA database, gave me the data I desired:\n\nAnalysing the data\nNo obvious trend (correlation coefficient of -0.008). Honestly, not quite what I was expecting!"
  },
  {
    "objectID": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#so-what-does-this-mean",
    "href": "dsc/2019-06-07-looking-at-traffic-congestion-vs-air-quality-aka-a-quest-for-data.html#so-what-does-this-mean",
    "title": "Looking at traffic/congestion vs air quality AKA a quest for data",
    "section": "So what does this mean?",
    "text": "So what does this mean?\nI haven’t done any proper analysis yet (scatter plots don’t really count), but that wasn’t the point. I saw some data, I had a question in mind, I found the data I needed to start answering it and I got some quick results. Tools like Google BigQuery and Colaboatory let anyone access and manipulate large datasets, and the availability of data means that you don’t have to belong to a company or research organisation to do serious research.\nI have 8 minutes left before I need to get back to work, so this is where I’ll end this post. I hope you’ve enjoyed this small demo of what is possible when curiosity meets open data. If you’d like to try this yourself, contact me for code if I haven’t yet uploaded the notebooks and data used here to GitHub. Share any other questions you’d like me to look into, and please send me any interesting datasets you come across. Farewell until next time."
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "",
    "text": "https://youtu.be/TBqzMfWpQrs\nIn this series, I’d like to explore how to take an idea within machine learning from proof of concept to production. This first post is going to get things going with a little mini-project that I did in the downtime between Christmas activities, creating a website called DistilHN.com using a bit of machine learning magic and some basic web scraping. Let’s get started.\nThe DistilHN page"
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#the-idea",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#the-idea",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "The Idea",
    "text": "The Idea\n\nI’ve been thinking about how to make a better news feed. When confronted with a clickbait headline, I often want a little more info, but don’t feel like clicking through to the article (and dismissing the cookie popup, and scrolling past the ads, and declining their invite to sign up for the newsletter, and …) just to see what it’s about. So, this is the idea: use AI to generate a short summary that you can read before deciding whether you’re going to commit to the full article or just skip straight to the comments section on Hacker News."
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#scraping-text",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#scraping-text",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "Scraping Text",
    "text": "Scraping Text\nI started working on a way to get the main text from an arbitrary website using Beautiful Soup, writing heuristics for which elements were worth including or ignoring. It turns out this is a very hard problem! After a while I had something that sort of worked for some sites, but in desperation I decided to take another look around online to see if someone else had already done the hard work.\n\nExtracting text from a website using Trafiltura\nEnter the Trafilatura library, purpose-built for this exact task! It makes it super easy to grab the text from any website, as shown in the screenshot above. Aside: all the code shown in this post is also available as a notebook on Google Colab here."
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#summarization",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#summarization",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "Summarization",
    "text": "Summarization\nFor the actual summarization step, I choose to use this model from Facebook which was fine-tuned for news article summarization. You can run it locally with a huggingface pipeline, but I chose to use the free inference API since we’re not going to need to run this thousands of times an hour and we may as well do as little work as possible ourselves! We set up a query, specify the text we want to summarize and the min and max length for the summary, post the request and wait for the summary back.\n\nSummarizing the text with the HuggingFace Inference API\nThis was a bit of a revelation for me. In the past I’d be downloading and training models as soon as I started a project like this, but here is an existing solution that does the job perfectly. If we want to scale up, Huggingface has paid inference options or we can switch to running the model ourselves. But for this proof-of-concept, the inference API makes our lives easy :)"
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#sharing",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#sharing",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "Sharing",
    "text": "Sharing\nIt’s one thing to run something like this once in a notebook. To make this a permanent solution, we need a few things:\n\nSome server to run a script every hour or so to fetch and summarize the latest articles.\nA website or something so that we can share our project with others, including a place to host it\nIdeally, an RSS feed that users can read from their RSS app of choice.\n\nI decided to start by wrapping up the scraping and summarization code into a script and having it write the results to an RSS feed (using the feedgenerator Python library). This way I’d have the content in a known format and a useable output before I start hacking on the front end.\n\nMy PythonAnywhere Dashboard - the script has only used ~20 seconds of CPU time so far today!\nWhile you could host something like this yourself on a small VPS, I chose to go the easy route and use a website called PythonAnywhere which handles some of the admin for you. They have a tutorial for hosting a static site and make it easy to upload files like the aforementioned script and set them to run on a schedule. I did end up making a minimal flask app too in case I want to develop this further, but for the initial demo, I just exposed the index.html and feed.xml files to the web via the PythonAnywhere web UI. This is great for getting demos up quickly, and since this is just serving a static site it should scale extremely well.\nSpeaking of index.html, I made a simple HTML page and modified a Javascript snippet from this tutorial to load in the items from the RSS feed and add them to the page. I’m not particularly comfortable with HTML/CSS so styling this took ages, and it still looks a little clunky. ChatGPT and GitHub CoPilot turned out SUPER useful for this step - I find myself relying on CoPilot’s suggestions much more when working with languages that I am less familiar with, and being able to just type /* Make the image appear at the top, centered */ and then hit tab to get the CSS I needed for something is delightful compared to my usual fiddle->test->google->repeat cycle."
  },
  {
    "objectID": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#taking-this-further",
    "href": "dsc/2022-12-30-distilhn-summarizing-news-articles-with-transformers.html#taking-this-further",
    "title": "DistilHN: Summarizing News Articles with Transformers",
    "section": "Taking This Further",
    "text": "Taking This Further\nYou can see the final website at https://www.distilhn.com/. I’m quite pleased with how it turned out, even if there are still a few things to iron out. I’m already working on a more ambitious follow-on project, pulling news from across the globe and filtering it using more ML magic… but that will have to wait for a future post :) Until then, have fun with the website, and let me know if you have ideas for improvements! Happy hacking."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "",
    "text": "I’m going to be exploring the world of Reinforcement Learning. But there will be no actual RL in this post - that’s for part two. This post will do two things: describe the game we’ll be training our AI on, and show how I developed it using a tool called NBDev which is making me so happy at the moment. Let’s start with NBDev."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#what-is-nbdev",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#what-is-nbdev",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "What is NBDev?",
    "text": "What is NBDev?\nLike many, I started my programming journey editing scripts in Notepad. Then I discovered the joy of IDEs with syntax highlighting, and life got better. I tried many editors over the years, benefiting from better debugging, code completion, stylish themes… But essentially, they all offer the same workflow: write code in an editor, run it and see what happens, make some changes, repeat. Then came Jupyter notebooks. Inline figures and explanations. Interactivity! Suddenly you don’t need to re-run everything just to try something new. You can work in stages, seeing the output of each stage before coding the next step. For some tasks, this is a major improvement. I found myself using them more and more, especially as I drifted into Data Science.\nBut what about when you want to deploy code? Until recently, my approach was to experiment in Jupyter, and then copy and paste code into a separate file or files which would become my library or application. This caused some friction - which is where NBDev comes in.\n~~~~~ “Create delightful python projects using Jupyter Notebooks” - NBDev website ~~~~~\nWith NBDev, everything happens in your notebooks. By adding special comments like #export to the start of a cell, you tell NBDev how to treat the code. This means you can write a function that will be exported, write some examples to illustrate how it works, plot the results and surround it with nice explanations in markdown. The exported code gets paces in a neat, well-ordered .py file that becomes your final product. The Notebook(s) becomes documentation, and the extra examples you added to show functionality work as tests (although you can also add more formal unit testing). An extra line of code uploads your library for others to install with pip. And if you’re following their guide, you get a documentation site and continuous integration that updates whenever you push your changes to GitHub.\nThe upshot of all this is that you can effortlessly create good, clean code and documentation without having to switch between notebooks, editors and separate documentation. And the process you followed, the journey that lead to the final design choices, is no longer hidden. You can show how things developed, and include experiments that justify a particular choice. This is ‘literate programming’, and it feels like a major shift in the way I think about software development. I could wax lyrical about this for ages, but you should just go and read about it in the launch post here."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#what-on-earth-is-swoggle",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#what-on-earth-is-swoggle",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "What on Earth is Swoggle?",
    "text": "What on Earth is Swoggle?\nChristmas, 2019. Our wedding has brought a higher-than-normal influx of relatives to Cape Town, and when this extended family gets together, there are some things that are inevitable. One of these, it turns out, is the invention of new games to keep the cousins entertained. And thus, Swoggle was born :)\n\nA Swoggle game in progress - 2 players are left.\nThe game is played on an 8x8 board. There are usually 4 players, each with a base in one of the corners. Players can move (a dice determines how far), “spoggle” other players (capturing them and placing them in “swoggle spa” - none of this violent termnology) or ‘swoggle’ a base (gently retiring the bases owner from the game - no killing here). To make things interesting, there are four ‘drones’ that can be used as shields or to take an occupied base. Moving with a drone halves the distance you can travel, to make up for the advantages. A player with a drone can’t be spoggled by another player unless they too have a drone, or they ‘powerjump’ from their base (a half-distance move) onto the droned player. Maybe I’ll make a video one day and explain the rules properly :)\nSo, that’s the game. Each round is fairly quick, so we usually play multiple rounds, awarding points for different achievements. Spoggling (capturing) a player: 1 point. Swoggling (taking out a base): 3 points. Last one standing: 5 points. The dice rolls add lots of randomness, but there is still plenty of room for tactics, sibling rivalry and comedic mistakes."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#game-representation",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#game-representation",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "Game Representation",
    "text": "Game Representation\nIf we’re going to teach a computer to play this, we need a way to represent the game state, check if moves are valid, keep track of who’s in the swoggle spa and which bases are still standing, etc. I settled on something like this:\n\nGame state representation\nThere is a Cell in each x, y location, with attributes for player, drone and base. These cells are grouped in a Board, which represents the game grid and tracks the spa. The Board class also contains some useful methods like is_valid_move() and ways to move a particular player around. At the highest level, I have a Swoggle class that wraps a board, handles setting up the initial layout, provides a few extra convenience functions and can be used to run a game manually or with some combination of agents (which we’ll cover in the next section). Since I’m working in NBDev, I have some docs with almost no effort, so check out https://johnowhitaker.github.io/swoggle/ for details on this implementation. Here’s what the documentation system turned my notebooks into:\n\nPart of the generated documentation\nThe ability to write code and comments in a notebook, and have that turn into a swanky docs page, is borderline magical. Mine is a little messy since this is a quick hobby project. To see what this looks like in a real project, check out the docs for NBDev itself or Fastai v2."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#creating-agents",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#creating-agents",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "Creating Agents",
    "text": "Creating Agents\nSince the end goal is to use this for reinforcement learning, it would be nice to have an easy way to add ‘Agents’ - code that defines how a player in the game will make a move in a given situation. It would also be useful to have a few non-RL agents to test things out and, later, to act as opponents for my fancier bots. I implemented two types of agent:\n\nRandomAgent Simply picks a random but valid move by trial and error, and makes that move.\nBasicAgent Adds a few simple heuristics. If it can take a base, it does so. If it can spoggle a player, it does so. If neither of these options are possible, it moves randomly.\n\nYou can see the agent code here. The notebook also defines a few other useful functions, such as win_rates() to pit different agents against each-other and see how they do. This is fun to play with - after a few experiments it’s obvious that the board layout and order of players matters a lot. A BasicAgent going last will win ~62% of games against three RandomAgents - not unexpected. But of the three RandomAgents, the one opposite the BasicAgent (and thus furthest from it) will win the majority of the remaining games."
  },
  {
    "objectID": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#next-step-reinforcement-learning",
    "href": "dsc/2020-01-20-swoggle-part-1-rl-environments-and-literate-programming-with-nbdev.html#next-step-reinforcement-learning",
    "title": "Swoggle Part 1- RL Environments and Literate Programming with NBDev",
    "section": "Next Step: Reinforcement Learning!",
    "text": "Next Step: Reinforcement Learning!\nThis was a fun little holiday coding exercise. I’m definitely an NBDev convert - I feel so much more productive using this compared to any other development approach I’ve tried. Thank you Jeremy, Sylvain and co for this excellent tool!\nNow, the main point of this wasn’t just to get the game working - it was to use it for something interesting. And that, I hope, is coming soon in Part 2. As I type this, a neural network is slowly but surely learning to follow the rules and figuring out how to beat those sneaky RandomAgents. Wish it luck, stay tuned, and, if you’re *really* bored, pip install swoggle and watch some BasicAgents battle it out :)"
  },
  {
    "objectID": "dsc/2020-06-16-update-why-the-pause.html",
    "href": "dsc/2020-06-16-update-why-the-pause.html",
    "title": "Update: Why the pause?",
    "section": "",
    "text": "It’s been nearly four months since my last post on this blog. I’d like to get back into writing things up here, but before putting up another normal post I figured it was worth giving a quick update on why things went quiet. Consider this less of a technical post and more of a personal update, and feel free to skip if you’re not interested :)\n\nExcuse #1 - Finding the balance when consulting is hard\nI’ve been working “full time” as a consultant for 18+ months now. For much of that I kept my hours fairly low, leaving plenty of time for personal research and other interests. I’m fortunate indeed to be able to live comfortably on fairly little - monthly expenses for my wife and I tally to something like 600 USD here in Zimbabwe [1]. However, work tends to arrive in batches and it’s always tempting to take on additional projects, especially if they’re interesting problems to work on.\nBy March, I had 5 ‘jobs’ ongoing, and was suddenly finding it hard to stay on track, especially with a global pandemic and local economic craziness adding their own wobbles. Thankfully I had supportive clients who were able to be slightly flexible - a lesson I’d share with anyone in this situation is to be very clear with everyone rather than trying to get it all figured out in secret. If you’re up front about difficulties, you can work out ways to minimize damage by pushing deadlines or working together to figure out what is most important to work on. I found that I had trouble focusing on anything without a near deadline, so I’d literally email clients and say “hey, as added motivation can we set an arbitrary deadline of Friday for this feature?” - this helped make sure I wasn’t just letting some projects slip through the cracks while focusing on the most urgent demands.\nSomehow I seem to have escaped with no harm done, but this has been a good warning about the danger of taking on too much. I still have a few months ahead that will be busier than I’d like, but I’m saying no to all sorts of things now. If I keep it up, I can start picking up threads like this blog, and I might even manage a vacation in September :)\n[1] - Fairly little by global Data Scientist standards. Many folks here live on less than a dollar a day, which is mind blowing.\n\n\nExcuse #2 - Outlets Abound\nSharing knowledge is somewhat addictive, which is why I started this blog in the first place. However, over the last few months I found that blog posts as a medium were simply too time-consuming to be a viable outlet for me. The didactic urge was instead satisfied by:\n\nStarter Notebooks. I like throwing up the quickest entry possible for data science competitions, and spending 10 minutes cleaning up the resulting notebook and sharing with the community is a very rewarding way to help fellow learners out and share knowledge. This also tends to lead to…\nEmails! I love getting emails from folks working on different problems. Most rewarding are those where it’s a problem I’ve encountered myself, and I can save somebody some major headaches by sharing the solution.\nTutorials for Zindi (it’s not time-consuming if it’s the employers time!). I’ve done a few posts for Zindi, such as “Climbing The Ladder” - a tutorial on leveling up in computer vision competitions. It’s fun to have a more targeted audience, and to share what I can with that great community.\nTeaching. I’m involved in a few online courses, which means creating lots of content for those classes (which might otherwise be turned into blog posts). As with emails, it reaches less people than publicly sharing something but it is super useful to those it does reach, which is rewarding in a different sort of way.\n\n\n\nConclusion\nSo, there are my excuses for the lack of posts. I have a backlog of ideas and half-finished projects, so hopefully we’ll be back on schedule soon. As always, do let me know if you have any thoughts or questions on this post, or any requests for topics to cover :)"
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "",
    "text": "NB: A scoring glitch caused this approach to look very good on the leaderboard, but local validation and a fix from Zindi later confirmed that it isn’t as magical as it first seemed. Still interesting from an educational point of view but if you’re looking to compete I’d suggest investigating alternate strategies."
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#introduction",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#introduction",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "Introduction",
    "text": "Introduction\nZindi has a competition running to identify individual turtles based on images from different views. This presents an interesting challenge for a few reasons:\n1) There are relatively few images per turtle (10-50 each) and these have been taken from multiple angles. Given how similar they are, simply treating this as a normal multi-class classification challenge is hard.\n2) There is an emphasis on generalization - it would be great if the organizations involved could add additional turtles without expensive re-training of models.\nOne potential approach that should help address these problems is to learn useful representations - some way to encode an image in a meaningful way such that the representations of images of one individual are all ‘similar’ by some measure while at the same time being dissimilar to the representations of images from other individuals. If we can pull this off, then given a new image we can encode it and compare the resulting representation with those of all known turtle images. This gives a ranked list of the most likely matches as well as a similarity score that could tell us if we’re looking at a completely new turtle.\nTo keep this post light on code, I have more info and a working example in this colab notebook. I’m also working on a video and will update this post once that’s done. And a modified version of this might be posted on Zindi learn, which again will be linked here once it’s up."
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#contrastive-learning",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#contrastive-learning",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "Contrastive Learning",
    "text": "Contrastive Learning\nThe goal of contrastive learning is to learn these useful representations in an unsupervised or loosely-supervised fashion (aka self-supervised learning). A typical approach is to take some images, create augmented versions of those images and then embed both the originals and the augmented versions with some encoder network. The objective is to maximise the similarity between an image and its augmented version while minimising the similarity between that image and all the rest of the images in the batch. The trick here is that augmentation is used to create two ‘versions’ of an image. In our turtle case, we also have pictures of the same individual from different angles which can be used in place of (or in addition to) image augmentations to get multiple versions depicting one individual.\n\nTop two rows: 16 turtles. Bottom 2 rows: augmented versions of different views of those same 16 turtles.\nIn my implementation, we generate a batch by picking batch_size turtles and then creating two sets of images with different pictures of those turtles. A resnet50 backbone acts like the encoder and is used to create embeddings of all of these images. We use a contrastive loss function to calculate a loss and update the network weights.\n\nThis is my first time using jax so there are probably more elegant ways to do this!\n\nMy post-it note with the formula\nMy attempt at a contrastive loss function\nYou can check the notebook or the video for more details on the implementation here. Once all the bugs were ironed out, the training loop runs and the loss shrinks nicely over time. But the question arises: how do we tell if the representations being learnt are actually useful?\nKey reference for going deeper: SimCLR - A Simple Framework for Contrastive Learning of Visual Representations"
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#representational-similarity-matrices",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#representational-similarity-matrices",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "Representational Similarity Matrices",
    "text": "Representational Similarity Matrices\nRemember, our end goal is to be able to tell which individual turtle is in a new image. If things are working well, we’ll feed the new image through our encoder model to get a representation and then compare that to the encoded representations of the known turtles. All pictures of a given individual should be ‘similar’ in this space, but should not be similar to images of other individuals. A neat way to visualize this is through something called a Representational Similarity matrix. We take, say, 16 images of 5 different turtles. We embed them all and compute all possible pair-wise similarities and then plot them as a heatmap:\n\nA Representation Similarity Matrix (RSM) comparing embeddings of 16 images from each of 5 turtles.\nThe images are obviously identical to themselves - hence the thin bright diagonal. But here you can also see that images of a given turtle seem to be similar to others of that same turtle - for instance, the bottom right 16x16 square shows that all images of the red turtle are quite similar to each other. This also shows us which turtles might be regularly confused (pink and yellow for eg) and which are relatively easy to disambiguate (pink and green).\nRSMs are a useful tool for quickly getting a feel for the kind of representations being learnt, and I think more people should use them to add visual feedback when working on this kind of model. Looking at RSMs for images in the training set vs a validation set, or for different views, can shed more light on how everything is working. Of course, they don’t tell the whole story and we should still do some other evaluations on a validation set."
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#so-does-it-work",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#so-does-it-work",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "So does it work?",
    "text": "So does it work?\nI trained a model on a few hundred batches with an embedding size of 100. For the test set, I took the turtle_ids of the most similar images in the training set to each test image and used those as the submission. If there were no images with a similarity above 0.8 I added ‘new_turtle’ as the first guess. This scores ~0.4 in local testing and ~0.36 on the public leaderboard. This is pretty good considering we ignored the image_position label, the label balance and various flaws in the data! However, a classification-based baseline with FastAI scores ~0.6 and the top entries are shockingly close to perfect with mapk scores >0.98 so we have a way to go before this is competitive.\nOne benefit of our approach: adding a new turtle to the database doesn’t require re-training. Instead, we simply encode any images of that individual we have and add the embeddings to the list of possible matches we’ll use when trying to ID new images."
  },
  {
    "objectID": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#where-next",
    "href": "dsc/2022-02-18-turtle-recall-a-contrastive-learning-approach.html#where-next",
    "title": "Turtle Recall: A Contrastive Learning Approach",
    "section": "Where Next?",
    "text": "Where Next?\nThere are many ways to improve on this:\n\nExperiment with parameters such as embedding size, batch size, augmentation types, training approach, regularization etc.\nIncorporate the image_position labels, either doing separate models for different angles, filtering potential matches based on the test labels or finding some way to feed the label into the model as an extra type of conditioning.\nExperiment with fine-tuning the model on the classification task. Since it has now (theoretically) learnt good representations, we could likely fine-tune it with a classification loss and get even better competition performance (at the cost of lower genaralizability)\nExplore automated data cleaning. Some images are out-of-domain, showing random background as opposed to turtle faces . Some images are just bad quality, or just don’t work with center-cropping.\nTry different models as the backbone\nInvestigate label balance\n\n…And many more. I hope this post gets you excited about the competition! Feel free to copy and adapt the notebook (with attribution please) and let me know if you manage to make any improvements. See you on the leaderboard :)"
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "",
    "text": "Every now and again, the World Bank conducts something called a Living Standards Measurement Study (LSMS) survey in different countries, with the purpose being to learn about people, their incomes and expenses, how they’re doing economically and so on. These surveys provide very useful info to various stakeholders, but they’re expensive to conduct. What if we could estimate some of the parameters they measure from satellite imagery instead? That was the goal of some researchers at Stanford back in 2016, who came up with a way to do just that and wrote it up into this wonderful paper in Science. In this blog post, we’ll explore their approach, replicate the paper (using some more modern tools) and try a few experiments of our own."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#predicting-poverty-where-do-you-start",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#predicting-poverty-where-do-you-start",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Predicting Poverty: Where do you start?",
    "text": "Predicting Poverty: Where do you start?\n\nNighttime lights\nHow would you use remote sensing to estimate economic activity for a given location? One popular method is to look at how much light is being emitted there at night - as my 3 regular readers may remember, there is a great nighttime lights dataset produced by NOAA that was featured in a data glimpse a while back. It turns out that the amount of light sent out does correlate with metrics such as assets and consumption, and this data has been used in the past to model things like economic activity (see another data glimpse post for more that). One problem with this approach: the low end of the scale gets tricky - nighttime lights don’t vary much below a certain level of expenditure.\nLooking at daytime imagery, we see many things that might help tell us about the wealth in a place: type of roofing material on the houses, the number of roads, how built-up an area is…. But there’s a problem here too: these features are quite complicated, and training data is sparse. We could try to train a deep learning model to take in imagery and spit out income level, but the LSMS surveys typically only cover a few hundred locations - not a very large dataset, in other words."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#jean-et-als-sneaky-trick",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#jean-et-als-sneaky-trick",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Jean et al’s sneaky trick",
    "text": "Jean et al’s sneaky trick\nThe key insight in the paper is that we can train a CNN to predict nighttime lights (for which we have plentiful data) from satellite imagery, and in the process it will learn features that are important for predicting lights - and that these features will likely also be good for predicting our target variable as well! This multi-step transfer learning approach did very well, and is a technique that’s definitely worth keeping in mind when you’re facing a problem without much data.\nBut wait, you say. How is this better than just using nightlights? From the article: “How might a model partially trained on an imperfect proxy for economic well-being—in this case, the nightlights used in the second training step above—improve upon the direct use of this proxy as an estimator of well-being? Although nightlights display little variation at lower expenditure levels (Fig. 1, C to F), the survey data indicate that other features visible in daytime satellite imagery, such as roofing material and distance to urban areas, vary roughly linearly with expenditure (fig. S2) and thus better capture variation among poorer clusters. Because both nightlights and these features show variation at higher income levels, training on nightlights can help the CNN learn to extract features like these that more capably capture variation across the entire consumption distribution.” (Jean et al, 2016). So the model learns expenditure-dependent features that are useful even at the low end, overcoming the issue faced by approaches that use nightlights alone. Too clever!"
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#can-we-replicate-it",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#can-we-replicate-it",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Can we replicate it?",
    "text": "Can we replicate it?\nThe authors of the paper shared their code publicly but… it’s a little hard to follow, and is scattered across multiple R and Python files. Luckily, someone has already done some of the hard work for us, and has shared a pytorch version in this GitHub repository. If you’d like to replicate the paper exactly, that’s a good place to start. I’ve gone a step further and consolidated everything into a single Google Colab notebook that borrows code from the above and builds on it. The rest of this post will explain the different sections of the notebook, and why I depart from the exact method used in the paper. Spoiler: we get a slightly better result with much fewer images downloaded."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#getting-the-data",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#getting-the-data",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Getting the data",
    "text": "Getting the data\nThe data comes from the Fourth Integrated Household Survey 2016-2017. We’ll focus on Malawi for this post. The notebook shows how to read in several of the CSV files downloaded from the website, and combine them into ‘clusters’ - see below. For each cluster location, we have a unique ID (HHID), a location (lat and lon), an urban/rural indicator, a weighting for statisticians, and the important variable: consumption (cons). This last one is the thing we’ll be trying to predict.\n\nThe relevant info from the survey data\nOne snag: the lat and lon columns are tricksy! They’ve been shifted to protect anonymity, so we’ll have to consider a 10km buffer around the given location and hope the true location is close enough that we get useful info."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#adding-nighttime-lights",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#adding-nighttime-lights",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Adding nighttime lights",
    "text": "Adding nighttime lights\n\nGetting the nightlights value for a given location\nTo get the nightlight data, we’ll use the python library to run Google Earth Engine queries. You’ll need a GEE account, and the notebook shows how to authenticate and get the required data. We can get the nightlights for each cluster location (getting the mean over an 8km buffer around the lat/lon points) and add this number as a column. To give us a target to aim at, we’ll compare any future models to a simple model based on these nightlight values only."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#downloading-static-maps-images",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#downloading-static-maps-images",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Downloading static maps images",
    "text": "Downloading static maps images\n\nGetting imagery for a given location\nThe next step takes a while: we need to download images for the locations. BUT: we don’t just want one for each cluster location - instead, we want a selection from the surrounding area. Each of these will have it’s own nightlights value, so that we get a larger training set to build our model on. Later, we’ll extract features for each image in a cluster and combine them. Details are in the notebook. The code takes several hours to run, but at the end of it you’ll have thousands of images ready to use.\n\nTracking requests/sec on in my Google Cloud Console\nYou’ll notice that I only generate 20 locations around each cluster. The original paper uses 100. Reasons: 1) I’m impatient. 2) There is a rate limit of 25k images/day, and I didn’t want to wait (see #1), 3) The images are 400 x 400, but are then shrunk to train the model. I figured I could split the 400px image into 4 (or 9) smaller images that overlap slightly, and thus get more training data for free. This is suggested as a “TO TRY” in the notebook, but hint: it works. If you really wanted to get a better score, trying this or adding more imagery is an easy way to do so."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#training-a-model",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#training-a-model",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Training a model",
    "text": "Training a model\nI’ll be using fastai to simplify the model creation and training stages. before we can create a model, we need an appropriate databunch to hold the training data. An optional addition at this stage is to add image transforms to augment our training data - which I do with tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.) as suggested in the fastai satelite imagery example based on Planet labs. The notebook has the full code for creating the databunch:\n\nData ready for modelling\nNext, we choose a pre-trained model and re-train it with our data. Remember, the hope is that the model will learn features that are related to night lights and, by extension, consumption. I’ve had decent results with resnet models, but in the shared notebook I stick with models.vgg11_bn to more closely match the original paper. You could do much more on this model training step, but we pick a learning rate, train for a few epochs and move on. Another place to improve!\n\nTraining the model to predict nightlights"
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#using-the-model-as-a-feature-extractor",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#using-the-model-as-a-feature-extractor",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Using the model as a feature extractor",
    "text": "Using the model as a feature extractor\nThis is a really cool trick. We’ll hook into one of the final layers of the network, with 512 outputs. We’ll save these outputs as each image is run through the network, and they’ll be used in later modelling stages. To save the features, you could remove the last few layers and run the data through, or you can use a trick I learnt from this TDS article and keep the network intact.\n\nCumulative explained variance of top PCA features\n512 (or 4096, depending on the mode and which layer you pick) is a lot of features. So we use PCA to get 30 or so meaningful features from those 512 values. As you can see from the plot above, the top few components explain most of the variance in the data. These top 30 PCA components are the features we’ll use for the last step in the process: predicting consumption."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#putting-it-all-together",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#putting-it-all-together",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Putting it all together",
    "text": "Putting it all together\nFor each image, we now have a set of 30 features that should be meaningful for predicting consumption. We group the images by cluster (aggregating their features). Now, for each cluster, we have the target variable (‘cons’), the nighttime lights (‘nl’) and 30 other potentially useful features. As we did right at the start, we’ll split the data into a test and a train set, train a model and then make predictions to see how well it does. Remember: our goal is to be better than a model that just uses nighttime lights. We’ll use the r^2 score when predicting log(y), as in the paper. The results:\n\nScore using just nightlights (baseline): 0.33\nScore with features extracted from imagery: 0.41\n\nUsing just the features derived from the imagery, we got a significant score increase. We’ve successfully used deep learning to squeeze some useful information out of satellite imagery, and in the process found a way to get better predictions of survey outcomes such as consumption. The paper got a score of 0.42 for Malawi using 100 images to our 20, so I’d call this a success."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#improvements",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#improvements",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Improvements",
    "text": "Improvements\nThere are quite a few ways you can improve the score. Some are left as exercises for the reader :) here are a few that I’ve tried:\n1) Tweaking the model used in the final step: 0.44 (better than the paper)\n2) Using sub-sampling to boost size of training dataset + using a random forest model: 0.51 (!)\n3) Using a model trained for classification on binned NL values (as in paper) as opposed to training it on a regression task: score got worse\n4) Cropping the downloaded images into 4 to get more training data for the model (no other changes): 0.44 up from 0.41 without this step. >0.5 aggregating features of 3 different subsets of images for each cluster\n5) Using a resnet-50 model: 0.4 (no obvious change this time - score likely depends less on model architecture and more on how well it is trained)\nOther potential improvements:\n- Download more imagery\n- Train the model used as a feature extractor better (I did very little experimentation or fine-tuning)\n- Further explore the sub-sampling approach, and perhaps make multiple predictions on different sub-samples for each cluster in the test set, and combine the predictions.\nPlease let me know if any of these work well for you. I’m less interested in spending more time on this - see the next section."
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#where-next",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#where-next",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Where next",
    "text": "Where next\nI’m happy with these results, but don’t like a few aspects:\n\nUsing static maps from Google means we don’t know the date the imagery was acquired, and makes it hard to extend our predictions over a larger area without downloading a LOT of imagery (meaning you’d have to pay for the service or wait weeks)\nUsing RGB images and an imagenet model means we’re starting from a place where the features are not optimal for the task - hence the need for the intermediate nighttime lights training step. It would be nice to have some sort of model that can interpret satellite imagery well already and go straight to the results.\nDownloading from Google Static Maps is a major bottleneck. I used only 20 images / cluster for this blog - to do 100 per cluster and for multiple countries would take weeks, and to extend predictions over Africa months. There is also patchy availability in some areas.\n\nSo, I’ve been experimenting with using Sentinel 2 imagery, which is freely available for download over large areas and comes with 13 bands over a wide spectrum of wavelengths. The resolution is lower, but the imagery still has lots of useful info. There are also large, labeled datasets like the EuroSAT database that have allowed people to pretrain models and achieve state of the art results for tasks like land cover classification. I’ve taken advantage of this by using a model pre-trained on this imagery for land cover classification tasks (using all 13 bands) and re-training it for use in the consumption prediction task we’ve just been looking at. I’ve been able to basically match the results we got above using only a single Sentinel 2 image for each cluster.\nUsing Sentinel imagery solves both my concerns - we can get imagery for an entire country, and make predictions for large areas, at different dates, without needing to rely on Google’s Static Maps API. More on this project in a future post…"
  },
  {
    "objectID": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#conclusion",
    "href": "dsc/2019-11-12-deep-learning-remote-sensing-using-nns-to-turn-imagery-into-meaningful-features.html#conclusion",
    "title": "Deep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features",
    "section": "Conclusion",
    "text": "Conclusion\nAs always, I’m happy to answer questions and explain things better! Please let me know if you’d like the generated features (to save having to run the whole modelling process), more information on my process or tips on taking this further. Happy hacking :)"
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "",
    "text": "Zindi is currently hosting a competition to classify fields by crop type using Sentinel-2 satellite imagery. They provide labeled fields with the crop type, and a separate set of fields as the ‘test’ set. The goal is to use the provided imagery to predict the crop type as accurately as possible. It’s a great contest, BUT: The imagery files are huge (although they offer Azure credits to help mitigate this by using cloud computing), and extending such an analysis to other areas is not easy. The goal of this post is to show how we can use the labeled fields to train our own classifier in Google Earth Engine (GEE), using Landsat imagery for the classification (EDIT: Sentinel 2 imagery is also available in GEE, making this choice somewhat arbitrary). This results in a model that can be applied over any region, and is a process that could be replicated by anyone with some known crop fields and an internet connection.\nI won’t include all the code here. Instead, view it and try it for yourself here."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#the-training-data",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#the-training-data",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "The Training Data",
    "text": "The Training Data\nThe important data is contained in a shapefile (a mapping-related file format for ‘vector’ layers that can contain points, lines or polygons). It contains multiple features (polygons), each representing a field with a certain kind of crop. The crop type is encoded as a number from 1 to 10. More info here.\n\nSome features in the ‘train’ shapefile.\nWe can upload this data as an asset in GEE by using the ‘New Table Upload’ option and selecting all the files except train.qpj (which is unnecessary). I named the asset ‘farm_zindi_train’, and repeated the steps for the test dataset.\n\nThere is one last hurdle we must overcome when using this data to train classifiers in GEE. Each feature in the training shapefile contains a property, ‘Crop_Id_Ne’, that tells us the crop type. Unfortunately, this is represented as a string. To convert it to the required type, we create a function that is mapped over the feature collection and use ee.Number.parse() to convert the string into a number for the model to use.\n\nGetting the required properties in the correct type by mapping a function over the collection"
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#landsat-imagery",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#landsat-imagery",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Landsat Imagery",
    "text": "Landsat Imagery\nInstead of the Sentinel-2 imagery the competition is using, we’ll see if we can achieve the same results with freely available Landsat 8 imagery. I used code from this tutorial to load the landsat data and create a ‘greenest pixel composite’ based on a computed value called NDVI (normalized difference vegetation index). This is not an ideal approach - we could instead have chosen a time of year when the differences between crops are most obvious, or used multiple images from different times in the growing season. These improvements will be considered in a future tutorial."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#training-a-classifier",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#training-a-classifier",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Training A Classifier",
    "text": "Training A Classifier\nThe ‘Supervised Classification’ guide by Google is good place to start when attempting this kind of classification task. The only changes I made to the provided code was to change the references to match my own training data, tweak the scale to reduce memory use and specify the property we’re trying to predict (in our case, ‘CID’ for crop ID). Looking at the output, it seems to roughly match the farm outlines - a good sign.\n\nClassifier output with farm boundaries shown."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#comparing-classification-accuracy",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#comparing-classification-accuracy",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Comparing Classification Accuracy",
    "text": "Comparing Classification Accuracy\nIdeally, we’d split the data into training and test sets, compare different classifiers and pick the best. We might even keep a third set of data, the ‘validation’ set, to get a better idea of how our chosen classifier will perform on unseen data. As with the different options for input layers, I’ll leave this for a second tutorial. For now, we will be lazy and evaluate the accuracy on the training data: print(‘Accuuracy’, trained.confusionMatrix().accuracy());\nThe accuracy of a CART classifier is listed as 65%. Not bad, given that there are 10 classes, but not great either. Switching to a random forest model gives a much higher accuracy score, but may be subject to overfitting."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#exporting-predictions",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#exporting-predictions",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Exporting Predictions",
    "text": "Exporting Predictions\nTo get the predicted crop type in each region of the test file, we look at the most common crop type predicted by the classifier in each region and export the predictions to a CSV file:\n\nExporting predictions\nThis results in a file containing columns for Field_Id and predicted crop type. Normally, this is what we’d like. However, the Zindi contest specifies the submission with predicted probabilities for each different crop:\n\nThe submission format\nTo get the data in this format, I used Python and pandas, with the pandas get_dummies function:\n\nFormatting the data correctly\nThis is not ideal - we see a 1 for our predicted class, with 0s for the rest. It would be better to predict the probabilities and hedge our bets, but let’s see see how this does. predictions.to_csv('pred_test_cart.csv', index=False) gives a file we can upload on Zindi… And the final score? ~17.4 (or ~15 with the random forest model), putting this submission in 30th place out of 31 entries as of today."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#future-improvements",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#future-improvements",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Future Improvements",
    "text": "Future Improvements\nThere are many ways we could improve this score. A different classifier might perform better. Selecting the greenest pixels was probably not the best approach. Instead of using ee.Reducer.mode(), we could count how many pixels are predicted for each crop type and use those counts to assign probabilities for our submission. Etc Etc. Some of these improvements will be covered in a future tutorial, hopefully coming soon."
  },
  {
    "objectID": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#conclusions",
    "href": "dsc/2019-06-26-tutorial-predicting-crop-types-with-gee.html#conclusions",
    "title": "Tutorial: Predicting Crop Types with GEE",
    "section": "Conclusions",
    "text": "Conclusions\nDespite our lackluster score, this exercise has hopefully shown the possibilities of this approach. Using only freely available imagery, which we never had to download thanks to Google Earth Engine, we were able to make predictions about which crops were being grown in different fields. If you’ve followed along, I hope you’ve seen what is possible with GEE - simply by copying snippets of code and gluing them all together. Once the accuracy is improved, this technique could be applied in many different situations."
  },
  {
    "objectID": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html",
    "href": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html",
    "title": "Data Glimpse: Visualizing Economic Activity with the G-Econ Project data",
    "section": "",
    "text": "This is the first ‘data glimpse’ - a short exploration of an existing dataset, with code and examples showing some of the ways the data can be used. For today’s glimpse, I’ll be playing with the ‘G-Econ’ dataset [1], as recommended by  on Pioneer. This dataset looks at economic activity for different locations, as opposed to breaking it down by country. There is data available from 1990, 2000 and 2005, broken down by ‘grid cell’ (a square one degree wide and one degree high).\nEconomic Activity by Grid Cell - G-Econ data for 1990"
  },
  {
    "objectID": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#loading-the-data",
    "href": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#loading-the-data",
    "title": "Data Glimpse: Visualizing Economic Activity with the G-Econ Project data",
    "section": "Loading the data",
    "text": "Loading the data\nThe data is shared as a Microsoft Excel worksheet [2]. There are 27,446 rows, and it’s a little overwhelming visually. Spreadsheets aren’t my forte, so my first step was to load the data into a Pandas DataFrame in a Jupyter notebook (available here for anyone who wants to follow along). With the data ready, I set out on the most obvious task: showing the data as a map. A few minutes of StackOverflow later, we have a visual and a GeoTiff file that can be opened in mapping software such as QGIS:"
  },
  {
    "objectID": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#asking-questions",
    "href": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#asking-questions",
    "title": "Data Glimpse: Visualizing Economic Activity with the G-Econ Project data",
    "section": "Asking questions",
    "text": "Asking questions\nBecause the data is aggregated by location (as opposed to population), it can answer some interesting questions. How does economic output vary with temperature or rainfall? How ‘centralized’ is industry in different regions? What’s the deal with all that $$$ hugging the coastlines? Let’s dig in.\n\nEnvironmental Factors\nFirst up, the effect of temperature:\n\nNot much gets done where it’s cold, it seems\nWhat about rainfall?\n\nEconomic Activity (2000) vs max precipitation (mm rainfall)\nAnd finally, distance to the ocean:\n\nCoasts are the place to be?\nIt appears that the most productive places are those where people like to be: accessible, not too hot, not too dry but not constantly drenched… A Goldilocks zone for human activity. The data already contains these environmental variables - I highly encourage you to try your own plots, or to read up the more thorough analyses in [1].\n\n\nComparing Countries\nThere are many ways we could compare countries. A bar plot of average economic activity per grid cell, perhaps, or comparison between the most productive single grid cell in each country. I was interested to see which countries had the most spread. The GIF below shows this dramatically: the top few cells in Russia are responsible for a huge chunk of the economic activity, while India has much more of a spread:\n\nScaled fraction of the total economic activity in four countries.\nFor the code, see the GitHub repository associated with this post."
  },
  {
    "objectID": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#conclusions",
    "href": "dsc/2019-06-28-data-glimpse-visualizing-economic-activity-with-the-g-econ-project-data.html#conclusions",
    "title": "Data Glimpse: Visualizing Economic Activity with the G-Econ Project data",
    "section": "Conclusions",
    "text": "Conclusions\nI hope you’ve enjoyed this quick, informal look at a fun dataset. I’m planning on doing more of these ‘Data Glimpse’ posts, since they take less time than a full write-up. The trade-off is that quality is lower, since I’m not going to invest time into perfectly labelled axes, long explanations or extra figures. Let me know what you think about this plan!\nReferences:\n[1] - Nordhaus, W., Azam, Q., Corderi, D., Hood, K., Victor, N.M., Mohammed, M., Miltner, A. and Weiss, J., 2006. The G-Econ database on gridded output: Methods and data. Yale University, New Haven, 6.References:\n[2] - https://gecon.yale.edu/data-and-documentation-g-econ-project (accessed June 2019)"
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "",
    "text": "Driven Data launched a competition around the Snapshot Serengeti database - something I’ve been intending to investigate for a while. Although the competition is called “Hakuna Ma-data” (which where I come from means something like “there is no data”), this is actually the largest dataset I’ve worked with to date, with ~5TB of high-res images. I suspect that that’s putting people off (there are only a few names on the leaderboard), so I’m writing this post to show how I did an entry, run through some tricks for dealing with big datasets, give you a notebook to get started quickly and try out a fun new tool I’ve found for monitoring long-running experiments using neptune.ml.Let’s dive in."
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#the-challenge",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#the-challenge",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "The Challenge",
    "text": "The Challenge\nThe goal of the competition is to create a model that can correctly label the animal(s) in an image sequence from one of many camera traps scattered around the Serengeti plains, which are teeming with wildlife. You can read more about the data and the history of the project on their website. There can be more than one type of animal in an image, making this a multi-label classification problem.\n\nSome not-so-clear images from the dataset\nThe drivendata competition is interesting in that you aren’t submitting predictions. Instead, you have to submit everything needed to perform inference in their hidden test environment. In other words, you have to submit a trained model and the code to make it go. This is a good way to practice model deployment."
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#modelling",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#modelling",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "Modelling",
    "text": "Modelling\nThe approach I took to modelling is very similar to the other fastai projects I’ve done recently. Get a pre-trained resnet50 model, tune the head, unfreeze, fine-tune, and optionally re-train with larger images right at the end. It’s a multi-label classification problem, so I followed the fastai planet labs example for labeling the data. You can see the details of the code in the notebook (coming in the next section) but I’m not going to go over it all again here. The modelling in this case is less interesting than the extra things needed to work at this scale."
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#starter-notebook",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#starter-notebook",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "Starter Notebook",
    "text": "Starter Notebook\nI’m a big fan of making data science and ML more accessible. For anyone intimidated by the scale of this contest, and not too keen on following the path I took in the rest of this post, I’ve created a Google Colab Notebook to get you started. It shows how to get some of the data, label it, create and train a model, score your model like they do in the competition and create a submission. This should help you get started, and will give a good score without modification. The notebook also has some obvious improvements waiting to be made - using more data, training the model further…..\n\nTraining a quick model in the starter notebook\nThe code in the notebook is essentially what I used for my first submission, which is currently the top out of the… 2 total submissions on the leaderboard. As much as I like looking good, I’ll be much happier if this helps a bunch of people jump ahead of that score! Please let me know if you use this, so that I don’t feel that this wasn’t useful to anyone?"
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#moar-data---colab-wont-cut-it",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#moar-data---colab-wont-cut-it",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "Moar Data - Colab won’t cut it",
    "text": "Moar Data - Colab won’t cut it\nOK, so there definitely isn’t 5TB of storage on Google Colab, and even though we can get a decent score with a fraction of the data, what if we want to go further? My approach was as follows:\n\nCreate a Google Cloud Compute instance with all the fastai libraries etc installed, by following this tutorial. The resultant machine has 50GB memory, a P100 GPU and 200GB disk space by default. It comes with most of what’s required for deep learning work, and has the added bonus of having jupyter + all the fastai course notebooks ready to get things going quickly. I made sure not to make the instance preemptible - we want to have long-running tasks going, so having it shut down unexpectedly would be sad.\nAdd an extra disk to the compute instance. This tutorial gave me the main steps. It was quite surreal typing in 6000 GB for the size! I mounted the dist at /ss_ims - that will be my base folder going forward.\nDownload a season of data, and then begin experimenting while more downloads. No point having that pricey GPU sitting idle!\nTrain the full model overnight, tracking progress.\nSubmit!\n\n\nMounting a scarily large disk!\nI won’t go into the cloud setup here, but in the next section let’s look at how you can track the status of a long-running experiment."
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#neptune-ml---tracking-progress",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#neptune-ml---tracking-progress",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "Neptune ML - Tracking progress",
    "text": "Neptune ML - Tracking progress\nI’d set the experiments running on my cloud machine, but due to lack of electricity and occasional loss of connection I couldn’t simply leave my laptop running and connected to the VM to show how the model training was progressing. With so many images, each epoch of training took ages, and I had a couple of models crash early in the process. This was frustrating - I would try to leave it going overnight but if the model failed in the evening it meant that I had wasted some of my few remaining cloud credits on a machine sitting idle. Luckily, I had recently seen how to monitor progress remotely, meaning I could check my phone while I was out and see if the model was working and how good it was getting.\n\nTracking loss and metrics over time with neptune.ml\nThe process is pretty simple, and well documented here. You sign up for an account, get an API key and add a callback to your model. This will then let you log in to neptune.ml from any device, and track your loss, any metrics you’ve added and the output of the code you’re running. I could give more reasons why this is useful, but honestly the main motivation is that it’s cool! I had great fun surreptitiously checking my loss from my phone every half hour while I was out and about.\n\nTracking model training with neptune"
  },
  {
    "objectID": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#where-next",
    "href": "dsc/2019-11-29-snapshot-serengeti-working-with-large-image-datasets.html#where-next",
    "title": "Snapshot Serengeti - Working with Large Image Datasets",
    "section": "Where next?",
    "text": "Where next?\nI’m out of cloud credits, and as an ‘independent scientist’ my funding situation doesn’t really justify spending more money on cloud compute to try a better entry. If you’d like to sponsor some more work, I may have another go with a properly trained model. I did manage to experiment on using more than the first image in a sequence, and using Jeremy Howard’s trick of doing some final fine-tuning on larger images - would be interesting to see how much these improve the score in this contest.\nI hope this post encourages more of you to try this contest out! As the starter notebook shows, you can get close to the top (beating the benchmark) with a tiny fraction of the data and some simple tricks. Give it a try and report how you do in the comments!"
  },
  {
    "objectID": "dsc/2019-06-19-pioneer-tournament-has-begun.html",
    "href": "dsc/2019-06-19-pioneer-tournament-has-begun.html",
    "title": "Pioneer Tournament has Begun!",
    "section": "",
    "text": "I have some more posts in the pipeline, including some more Zindi fun. BUT, that will all have to wait. The next round of the Pioneer Tournament (pioneer.app) has begun, and I’ll be entering and using this blog to share progress towards my goal: Creating rich new datasets (along with educational material on how to use them) for ‘data-poor’ regions using satellite imagery and other public data sources. Stay tuned for the first output, and enjoy this AI-generated music while you wait:\nhttps://www.youtube.com/watch?v=jIYHE38Qn0M"
  },
  {
    "objectID": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html",
    "href": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html",
    "title": "Behind the scenes of a Zindi Contest",
    "section": "",
    "text": "User comments\nEver wondered what goes into launching a data science competition? If so, this post is for you. I spent the last few days working on the Fowl Escapades: Southern African Bird Call Audio Identification Challenge on Zindi, and thought it would be fun to take you behind the scenes a little to show how it all came together."
  },
  {
    "objectID": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-1-inspiration",
    "href": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-1-inspiration",
    "title": "Behind the scenes of a Zindi Contest",
    "section": "Step 1: Inspiration",
    "text": "Step 1: Inspiration\nMany competitions spring from an existing problem in need of a solution. For example, you may want a way to predict when your delivery will arrive based on weather, traffic conditions and the route your driver will take. In cases like this, an organization will reach out to Zindi with this problem statement, and move to stage 2 to see if it’s a viable competition idea. But this isn’t the only way competitions are born!\nSometimes, we find a cool dataset that naturally lends itself to answering an interesting problem. Sometimes we start with an interesting problem, and go looking for data that could help find answers. And occasionally, we start with nothing but a passing question at the end of a meeting: ‘does anyone have any other competition ideas?’. This was the case here.\nI had been wanting to try my hand at something involving audio data. Since I happen to be an avid birder, I thought automatic birdsong identification would be an interesting topic. For this to work, we’d need bird calls - lot’s of them. Fortunately, after a bit of searching I found the star of this competition: https://www.xeno-canto.org/. Hundreds of thousands of calls from all over the world! A competition idea was born."
  },
  {
    "objectID": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-2-show-me-the-data",
    "href": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-2-show-me-the-data",
    "title": "Behind the scenes of a Zindi Contest",
    "section": "Step 2: Show me the data",
    "text": "Step 2: Show me the data\nTo run a competition, you need some data (unless you’re going to ask the participants to find it for themselves!). This must:\n\nBe shareable. Anything confidential needs to be masked or removed, and you either need to own the data or have permission to use it. For the birdsong challenge, we used data that had CC licences but we still made sure to get permission from xeno-canto and check that we’re following all the licence terms (such as attribution and non-modification).\nBe readable. This means no proprietary formats, variable definitions, sensible column names, and ideally a guide for reading in the data.\nBe manageable. Some datasets are HUGE! It’s possible to organize contests around big datasets, but it’s worth thinking about how you expect participants to interact with the data. Remember - not everyone has fast internet or free storage.\nBe useful. This isn’t always easy to judge, which is why doing data exploration and building a baseline model early on is important. But ideally, the data has some predictive power for the thing you’re trying to model!\n\n\nVisualizing birdsongs\nBy the time a dataset is released as part of a competition, it’s usually been through several stages of preparation. Let’s use the birdsong example and look at a few of there steps.\n\nCollection: For an organization, this would be an ongoing process. In our example case, this meant scraping the website for files that met our criteria (Southern African birds) and then downloading tens of thousands of mp3 files.\nCleaning: A catch-all term for getting the data into a more usable form. This could be removing unnecessary data, getting rid of corrupted files, combining data from different sources…\nSplitting and Masking: We picked the top 40 species with the most example calls, and then split the files for each species into train and test sets, with 33% of the data kept for the test set. Since the file names often showed the bird name, we used ''.join(random.choices(string.ascii_uppercase + string.digits, k=6)) to generate random IDs. However you approach things, you’ll need to make sure that the answers aren’t deducible from the way you organize things (no sorting by bird species for the test set!)\nChecking (and re-checking, and re-checking): Making sure everything is in order before launch is vital - nothing is worse than trying to fix a problem with the data after people have started working on your competition! In the checking process I discovered that some mp3s had failed to download properly, and others were actually .wav files with .mp3 as the name. Luckily, I noticed this in time and could code up a fix before we went live.\n\nMany of these steps are the same when approaching a data science project for your own work. It’s still important to clean and check the data before launching into the modelling process, and masking is useful if you’ll need to share results or experiments without necessarily sharing all your secret info."
  },
  {
    "objectID": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-3-getting-ready-for-launch",
    "href": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#step-3-getting-ready-for-launch",
    "title": "Behind the scenes of a Zindi Contest",
    "section": "Step 3: Getting ready for launch",
    "text": "Step 3: Getting ready for launch\nAside from getting the data ready, there are all sorts of extra little steps required to arrive at something you’re happy to share with the world. An incomplete list of TODOs for our latest launch:\n\nDecide on a scoring metric. This will be informed by the type of problem you’re giving to participants. In this case, we were torn between accuracy and log loss, and ended up going with the latter. For other cases (eg imbalanced data), there are a host of metrics. Here’s a guide: https://machinelearningmastery.com/tour-of-evaluation-metrics-for-imbalanced-classification/\nPut together an introduction and data description. What problem are we solving? What does the solution need to do? What does the training data look like? This will likely involve making some visualizations, doing a bit of research, finding some cool images to go with your topic…\nSocial media. This isn’t part of my job, but I gather that there is all sorts of planning for how to let people know about the cool new thing we’re putting out into the world :)\nTutorials. Not essential, but I feel that giving participants a way to get started lowers the barriers to entry and helps to get more novices into the field. Which is why, as is becoming my habit, I put together a starter notebook to share as soon as the contest launches.\n\n\nA confusion matrix - one way to quickly see how well a classification algorithm is working. (from the starter notebook)\n\nBaseline/benchmark. This is something I like to do as early as possible in the process. I’ll grab the data, do the minimal cleaning required, run it through some of my favorite models and see how things go. This is nice in that it gives us an idea of what a ‘good’ score is, and whether the challenge is even doable. When a client is involved, this is especially useful for convincing them that a competition is a good idea - if I can get something that’s almost good enough, imagine what hundreds of people working for prize money will come up with! If there’s interest in my approach for a quick baseline, let me know and I may do a post about it.\nNames, cover images, did you check the data???, looking at cool birds, teaser posts on twitter, frantic scrambles to upload files on bad internet, overlaying a sonogram on one of my bird photos… All sorts of fun :)\n\n\nFine-tuning the benchmark model\nI could add lots more. I’ve worked on quite a few contests with the Zindi team, but usually I’m just part of the data cleaning and modelling steps. I’ve had such a ball moving this one from start to finish alongside the rest of the team, and I really appreciate all the hard work they do to keep us DS peeps entertained!"
  },
  {
    "objectID": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#try-it-yourself",
    "href": "dsc/2020-01-16-behind-the-scenes-of-a-zindi-contest.html#try-it-yourself",
    "title": "Behind the scenes of a Zindi Contest",
    "section": "Try it yourself!",
    "text": "Try it yourself!\nI hope this has been interesting. As I said, this whole process has been a blast. So if you’re sitting on some data, or know of a cool dataset, why not reach out and host a competition? You might even convince them to let you name it something almost as fun as ‘Fowl Escapades’. :)"
  },
  {
    "objectID": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html",
    "href": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html",
    "title": "New Database: Forest Change in Different Regions",
    "section": "",
    "text": "Forest loss is a major problem facing many parts of the world right now. Trees are being cleared to make way for agriculture, or simply cut down for fuel and timber. Tracking this loss is an important goal, and much work has been done in this area.\nOne of the best datasets on the topic is the Hansen Global Forest Change [1] dataset, available for free on the Google Earth Engine platform. This dataset tracks forest loss since the year 2000, and has become a key tool in fighting deforestation.\nForest cover (green), loss (red) and gain(blue) - from the Hansen dataset[1]\nThere is only one issue that I have with this data: it is HUGE! Approximately 1.22 TB. For anyone unable to write the code needed to analyse the data in GEE, this size means that downloading the data or importing it into traditional mapping applications is not feasible. And often we don’t need all of this data, instead simply requiring a few key stats on an area of interest. Consider wanting a graph of forest loss in your country over the last 20 years: it’s a nice visual to help you make a point, but it’s not worth learning to code or downloading >1TB of data for.\nThis leads to today’s project. I wrote some code that takes in a file specifying the boundaries of different regions. It then aggregates the data from the Hansen dataset over each of the specified regions. For example, I used the Large Scale International Boundary Polygons (LSIB) [2] map of the world’s countries as an input, ending up with total forest loss, loss per year and forest cover for every country in a convenient 98 KB csv file. It also outputs a version of the input file as a shapefile, with added attributes containing the summarized forest change data. The former is all you need to plot change over time, see which regions have experienced the most loss or identify which country has lost the most forest in the last ten years. The latter is nice for creating colorful maps displaying this information - it’s only ~60MB, and loads quickly into the mapping software on my laptop.\nForest loss in different regions\nThe Earth Engine code is available here.The rest of this post will explain how to use the generated datasets (available here) for simple analyses."
  },
  {
    "objectID": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#viewing-the-shapefile-in-qgis",
    "href": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#viewing-the-shapefile-in-qgis",
    "title": "New Database: Forest Change in Different Regions",
    "section": "Viewing the shapefile in QGIS",
    "text": "Viewing the shapefile in QGIS\nQGIS [3] is an open source GIS application. The vector file (available here) can be opened in QGIS with ‘Open Data Source Manager’ -> ‘Vector Layer’ -> browse to the .shp file and click ‘Add’. By default, it looks uniform. To see the information better, right click on the layer, open properties and change the style from ‘single symbol’ to ‘graduated’:\n\nSetting the style of the vector layer in QGIS\nWith these settings applied, the differences between countries become apparent. Play with the colours and classes until it looks good. To query the exact value of the loss in a given country, use the ‘Identify Features’ tool (Ctrl-Shift-I) and click to see all the attributes. To create a beautiful PDF map, consult a tutorial such as this one for all the fancy output options.\n\nForest loss displayed in QGIS"
  },
  {
    "objectID": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#analyzing-the-data-with-python-pandas",
    "href": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#analyzing-the-data-with-python-pandas",
    "title": "New Database: Forest Change in Different Regions",
    "section": "Analyzing the data with Python + Pandas",
    "text": "Analyzing the data with Python + Pandas\nThe smaller csv file (available here) is good for cases where the country outlines are not required. It is possible to open the file in Excel or Google Sheets, but let’s stretch our Python muscles and make some simple plots. A notebook with the full code for this example is available in the GitHub repository.\nThe first step is loading the data: we import the necessary libraries then load the data into a pandas DataFrame with “df = pd.read_csv(‘countries_w_hansen.csv’)”. For our first plot, let’s look at the total loss (from the ‘loss’ column) for different world regions:\n\nPlotting forest loss for different regions\nThe Hansen data encodes the years different areas experienced loss events. This data is captured in the ‘Group X’ columns. We can sum these columns to see the total loss each year, and note the worrying trend:\n\nForest loss per year\nOf course, we have the country data, and can focus on a single country or region using df.loc:\n\nForest loss over time in Africa. The drop looks encouraging… until you consider the latest date this data was updated (2018 was still ongoing)"
  },
  {
    "objectID": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#where-next",
    "href": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#where-next",
    "title": "New Database: Forest Change in Different Regions",
    "section": "Where next?",
    "text": "Where next?\nThis data is fairly depressing, but my hope is that an exploration of it doesn’t end with resignation. There are things we can do, ways we can help reduce this loss. Take a look at the data. Share the stats on your country, and push for change. Post those graphs on Facebook, call your representatives and demand action, find an organization working to fight this… If we’re serious about saving our planet, we’re all going to have to be involved."
  },
  {
    "objectID": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#references",
    "href": "dsc/2019-06-19-new-database-forest-change-in-different-regions.html#references",
    "title": "New Database: Forest Change in Different Regions",
    "section": "References",
    "text": "References\n[1] - Hansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, S. V. Stehman, S. J. Goetz, T. R. Loveland, A. Kommareddy, A. Egorov, L. Chini, C. O. Justice, and J. R. G. Townshend. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (15 November): 850–53. Data available on-line at: http://earthenginepartners.appspot.com/science-2013-global-forest.\n[2] - LSIB: Large Scale International Boundary Polygons, Simplified\nThe United States Office of the Geographer provides\nthe Large Scale International Boundary (LSIB) dataset. The detailed\nversion (2013) is derived from two other datasets: a LSIB line\nvector file and the World Vector Shorelines (WVS) from the National\nGeospatial-Intelligence Agency (NGA).\n[3] - QGIS. A Free and Open Source Geographic Information System. qgis.org\n[4] - GitHub repository containing data and code: https://github.com/johnowhitaker/hansen_data_countries"
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "",
    "text": "The Cornell Lab of Ornithology run an annual competition to identify bird calls in soundscapes. I decided to have a go at this year’s competition to get back into audio classification and try out some new approaches. For this first post I will examine the data, choose methods for picking the right clips within larger recordings and for generating a spectrogram from said clip, and train a simple model to use as a baseline for future experiments."
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#finding-the-calls",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#finding-the-calls",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "Finding the Calls",
    "text": "Finding the Calls\nIn many recordings, the bird in question is not calling continuously. The final task involves predicting which birds are calling at 5-second intervals, so that is my chosen input length. If we just sample a random 5-second clip from a full recording, we might end up with a clip in which the bird is not calling - not ideal! To get around this, we compute a sort of signal-to-noise measure (in this case, PCEN-based SNR as used by the BirdVox project). With this, we can choose ‘peaks’ where the calls are most prominent.\n\nIdentifying ‘peaks’ with a high PCEN-based SNR\nThe code for this is in my first notebook. For each train file, we store the location of 20 peaks in a csv file which we will than use during training to select the appropriate clips."
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#preparing-the-data-for-modelling",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#preparing-the-data-for-modelling",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "Preparing the data for modelling",
    "text": "Preparing the data for modelling\nWe could try feeding the raw audio data into a model, but 5 seconds of audio represents quite a lot of data. Some models can handle this, but in most cases a better approach is to find a more compressed representation of the sound. In this case I chose a fairly standard approach: the mel spectrogram. A spectrogram looks like a 2D image, with time on the X axis, frequency on the y axis and intensity represented by colour.\n\nAn example spectrogram\nThe model training notebook shows how we set up the dataloaders to read in a specified clip and turn it into a spectrogram that can be fed to the model. This is quite CPU-heavy, which does slow the training down. But I still chose this approach over pre-computing the spectrograms once at the start because it allows for data augmentation such as shifting the window, adding noise etc on the raw audio before it gets converted to a spectrogram.\n\nYou can see all the code in the baseline model notebook. Taking inspiration from the pets tutorial, we create our own custom Transform that handles ‘encoding’ a given clip/label pair, which in turn is used to create our DataLoaders. By adding a ‘decodes’ method we also enable functionality such as ‘show_batch()’."
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#training",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#training",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "Training",
    "text": "Training\n\nLoss plot over 3 epochs of training\nI’m not doing anything fancy for training - our goal here is a simple model to use as a baseline for future tests. A few things I did learn however:\n\nTo be able to access the output of a Kaggle notebook, you have to re-run it by clicking ‘save’. This can eat up GPU time, so I have started running my interactive tests with small subsets of the data and then relying on the run triggered by a save to actually do the full training.\nBecause this is then running ‘in the background’, saving any info you need is a must. I use the CSVLogger callback to save the stats after each epoch, and do other things like saving loss plots as pngs. Finally, we save the model itself for future use.\nWith this small model and CPU heavy dataloader, running on CPU was only a couple of times slower than on GPU. Wih a bit of patience, one could simply run this overnight rather than using up your weekly GPU quota, saving the GPU goodness for fast iteration when experimenting. In fact after the save failed a few times I ended up switching off the GPU and letting it train on the CPU over 7 or 8 hours.\n\nAgain, full code is in the notebook. After 3 epochs (the number of epochs and the learning rate chosen somewhat arbitrarily) we get to an accuracy of ~53% - impressive given the large number of classes. I’m sure a better model and more training would boost this, but that is something we can play with later…"
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#evaluation",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#evaluation",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "Evaluation",
    "text": "Evaluation\nDuring training we calculate an ‘accuracy’ score based on some clips withheld from the training data. These all have a single label (even though there may well be other calls mixed in) and they are taken from a different source to the actual test data that we will be scored on in the competition. We would assume a better accuracy in our simplified case will mean a better model, but ideally we want a way to evaluate our model in a setting that is as close as possible to the final task.\nFortunately, the competition hosts have provided some labelled audio recordings that match the format of the test set. We can use this in our evaluation notebook to simulate a submission. Our model needs to provide a list of all bird species calling in a given 5-second clip. The way we will approach this for now is to take the model’s output probabilities and pick some threshold above which we will include a given species.\nIn the future, we will want to take geographic location into account, as well as ideally training a model directly on this kind of multi-label task. Even without this, our very simple model gets and F1-score of about 0.64 on the provided evaluation set and a leaderboard score of 0.55. The notebook is very rough, but for completeness here is a link."
  },
  {
    "objectID": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#conclusions-and-next-steps",
    "href": "dsc/2021-04-22-birdclef-entry-bird-call-classification-with-fastai.html#conclusions-and-next-steps",
    "title": "BirdClef Entry: Bird Call Classification with FastAI",
    "section": "Conclusions and Next Steps",
    "text": "Conclusions and Next Steps\nOur submission scores 0.55, placing 167th on the leaderboard. Not terrible, but there is a ways to go before we are up there near the top. If I manage to spend some time on this, there will hopefully be a part 2 in which I explore ways in which we can get the score boost… Stay tuned for that :)"
  },
  {
    "objectID": "dsc/2019-07-13-data-glimpse-cropland-and-settlement-maps-from-qed-ai.html",
    "href": "dsc/2019-07-13-data-glimpse-cropland-and-settlement-maps-from-qed-ai.html",
    "title": "Data Glimpse: Cropland and Settlement maps from QED.AI",
    "section": "",
    "text": "The point of this Data Glimpse post is to feature a wonderful yet badly publicized data source: https://maps.qed.ai/. Using crowd-sourced data, they built really accurate maps of fields and settlements for the whole of Africa. They also make related spatial layers available (Enhanced Vegetation Index for different years, soil metrics etc). Their focus is “data systems and AI for health and agriculture”. The soil maps draw heavily on the AfSIS project, which makes the data from thousands of soil samples available (https://www.isric.org/projects/africa-soil-information-service-afsis).\n\nThe maps.qed.ai interface showing cropland probability\nThe QED maps interface makes it really easy to download all the available maps at 1km resolution. I’m not going to do any further analysis in this post - these maps are useful without modification, and it was really interesting for me to see the distribution of agriculture in Africa. The cropland probability map will be making an appearance in the next post."
  },
  {
    "objectID": "dsc/2019-06-11-zindi-competition-2-trying-catboost-on-the-traffic-jam-challenge.html",
    "href": "dsc/2019-06-11-zindi-competition-2-trying-catboost-on-the-traffic-jam-challenge.html",
    "title": "Zindi Competition 2 - Trying CatBoost on the Traffic Jam Challenge",
    "section": "",
    "text": "Zindi ran a challenge predicting bus ticket sales into Nairobi. It is now closed, but we can still make predictions and see how they would have done. This was a very quick attempt, but I wanted to try out CatBoost, a magical new algorithm that’s gaining popularity at the moment.\nWith a little massaging, the data looks like this:\n\nThe ‘travel_time’ (in minutes) and ‘day’ columns were derived from the initial datetime data. I’ll spare you the code (it’s available in this GitHub repo) but I pulled in travel times from Uber Movement, and added them as an extra column. The test data looks the same, but lacks the ‘Count’ column - the thing we’re trying to predict. Normally you’d have to do extra processing: encoding the categorical columns, scaling the numerical features… luckily, catboost makes it very easy:\n\nTraining the model\nThis is convenient, and that would be enough reason to try this model first. As a bonus, they’ve implemented all sorts of goodness under the hood to do with categorical variable encoding, performance improvements etc. My submission (which took half an hour to implement) achieved a score of 4.21 on the test data, which beats about 75% of the leaderboard. And this is with almost no tweaking! If I spent ages adding features, playing with model parameters etc, I have no doubt this could come close to the winning submissions.\nIn conclusion, I think this is definitely a tool worth adding to my arsenal. It isn’t magic, but for quick solutions it seems to give good performance out-of-the-box and simplifies data prep - a win for me.\nThis was a short post since I’m hoping to carry on working on the AI Art contest - expect more from that tomorrow!"
  },
  {
    "objectID": "dsc/2019-01-11-the-journey-begins.html",
    "href": "dsc/2019-01-11-the-journey-begins.html",
    "title": "init(self): What is this blog",
    "section": "",
    "text": "Welcome!\nIn this first post, I figured I’d lay out the goals of this blog and explain a bit of background. As soon as I’m done writing this I’m planning on following up with the first proper post. With luck, this intro will be the only post ‘fluff’ post you’ll see here.\nLet’s start with me. My name is Jonathan Whitaker. I’m an Electrical Engineer with a Data Science background, currently pursuing some personal research projects while my wife and I take a working vacation around Zimbabwe. For the last 5 years I’ve been writing code to solve problems. Big, important problems for work. Small, interesting projects for fun. Obscure, not-quite-problems because something bugged me and I thought “I can do that better”. I’m hoping that this blog will become a place for me to share these solutions and associated musings.\nI’ve called the blog ’The Data Science Cast-net”. This is because I have developed a fairly chronic case of something I call the data science mindset - something I try to instil in my students when I teach this stuff. In essence, this is a mental practice of looking at pretty much everything as a data science problem. Looking for somewhere to live? I can map travel times, house prices, crime rates etc to efficiently narrow down the search. Idly wondering how a romantic relationship is affecting your health? Google Fit makes all sorts of data available - we can compare different years and do some fun statistics to see if you’re walking more or less now that you’re hitched. And so on, down a slippery slope that ends with you tracking all aspects of your life and thinking in terms of variables and models far too often. Now, armed with the tools to make sense of data, I am throwing my cast-net out into the world and seeing what interesting information-fish I can pull in.\nI look forward to sharing this experiment with you."
  },
  {
    "objectID": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html",
    "href": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 1)",
    "section": "",
    "text": "I’m going to be working on a project that will ultimately require manually outlining tobacco fields in Zimbabwe. To help locate potential fields, it would be nice to have a model that can predict whether a giver area contains cropland. To train such a model required labeled fields - a chicken and egg scenario that should have me resigned to hours of manual work. But I much prefer not to do things if I can possibly get a computer to do it, and this post (along with one or more sequels) will document the ways I’ve made my life easier, and the lessons learnt in the process."
  },
  {
    "objectID": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#trick-1-standing-on-shoulders",
    "href": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#trick-1-standing-on-shoulders",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 1)",
    "section": "Trick #1: Standing on shoulders",
    "text": "Trick #1: Standing on shoulders\nI recently encountered a project that already did a lot of the hard work tagging fields all over Africa. Their results (featured in today’s Data Glimpse post) look great, but the training data used isn’t published. Now, I could just use their published map, but I’m also interested in change over time, while their map is based solely on 2015 data. What if we train a new model on the output of their model? This isn’t generally a great idea (since you’re compounding errors) but it might be good enough for our purposes.\n\nSatellite image (left) and my predicted cropland (right, in red)\nIn Google Earth Engine (script available here), I created a composite image from Landsat 8 images taken in 2015, including NDVI, band values from a greenest-pixel composite and band values from late in the year (planting season for most crops). This is to be the input to out model. I then sampled 2500 points, recording the inputs (the bands of the composite image) and the desired output (the cropland probability made available by the qed.ai team). This data was used to train a random forest model (framing the task as a classification problem) and the predictions compared to the predictions from the QED data. The result: 99% accuracy.\n\nConfusion matrix and accuracy\nWhat does this accuracy figure mean? How is it so high? It’s less astonishing when we look more deeply. This is a model, the same type as that used by the QED team, with roughly the same inputs. It isn’t surprising that it can quickly replicate the decision function so accurately. It’s highly unlikely that it’s this accurate when compared to the ground truth. But we can say the following: we now have a model that is very similar to that used by the QED team to predict cropland probability for the year 2015."
  },
  {
    "objectID": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#now-what-looking-at-change-over-time",
    "href": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#now-what-looking-at-change-over-time",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 1)",
    "section": "Now what? Looking at change over time",
    "text": "Now what? Looking at change over time\nThe model takes landsat 8 image data as it’s inputs. It was trained on 2015 data, but there is no reason why we can’t make predictions based on other years, and see where these predictions differ from the 2015 ones. Subtracting two years’ predictions gives a difference image, shown below for 2015 - 2018. Red indicated areas where cropland is predicted in 2018 and not 2015 (new cropland). Black and green are areas where the model predicts no change or less cropland in 2018.\n\nDifference Image (2018). Potential new cropland shown in red.\nI don’t want to trust this model too much, but if nothing else this shows some areas where there might be fields that have appeared in the last few years. I now have a much better idea where to look, and where to dig deeper with manual inspection of images from different years."
  },
  {
    "objectID": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#conclusions-and-next-steps",
    "href": "dsc/2019-07-14-mapping-change-in-cropland-in-zimbabwe-part-1.html#conclusions-and-next-steps",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 1)",
    "section": "Conclusions and next steps",
    "text": "Conclusions and next steps\nThis sets the scene for my next steps: manually outlining fields, differentiating between different crop types, training an improved model, adding more inputs… Stay tuned for part 2."
  },
  {
    "objectID": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html",
    "href": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html",
    "title": "Zindi UberCT Part 3: Uber Movement",
    "section": "",
    "text": "Uber Movement has launched in Cape Town\nToday, Uber Movement launched in Cape Town. This is good news, since it means more data we can use in the ongoing Zindi competition I’ve been writing about! In this post we’ll look at how to get the data from Uber, and then we’ll add it to the model from Part 2 and see if it has allowed us to make better predictions. Unlike the previous posts, I won’t be sharing a full notebook to accompany this post - you’ll have to do the work yourself. That said, if anyone is having difficulties with anything mentioned here, feel free to reach out and I’ll try to help. So, let’s get going!"
  },
  {
    "objectID": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html#getting-the-data",
    "href": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html#getting-the-data",
    "title": "Zindi UberCT Part 3: Uber Movement",
    "section": "Getting the data",
    "text": "Getting the data\n\nMy rough travel ‘zones’\nZindi provided some aggregated data from Uber movement at the start of the competition. This allows you to get the average travel time for a route, but not to see the daily travel times (it’s broken down by quarter). But on the Uber Movement site, you can specify a start and end location and get up to three months of daily average travel times. This is what we’ll be using.\n\n\n\n\nUsing sophisticated mapping software (see above), I planned 7 routes that would cover most of the road segments. For each route, I chose a start and end zone in the Uber Movement interface (see table above) and then I downloaded the data. To do it manually would have taken ages, and I’m lazy, so I automated the process using pyautogui, but you could also just resign yourself to a few hours of clicking away and get everything you need. More routes here would have meant better data, but this seemed enough to give me a rough traffic proxy.\n\nSome of the travel times data\nI manually tagged each segment with the equivalent Uber Movement trip I would be using to quantify traffic in that area, using QGIS. This let me link this ‘zone id’ from the segments shapefile to my main training data, and subsequently merge in the Uber Movement travel times based on zone id and datetime."
  },
  {
    "objectID": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html#does-it-work",
    "href": "dsc/2019-10-29-zindi-uberct-part-3-uber-movement.html#does-it-work",
    "title": "Zindi UberCT Part 3: Uber Movement",
    "section": "Does it work?",
    "text": "Does it work?\n\nScore (y axis) vs threshold for predicting a 1. In my case, a threshold of ~0.35 was good.\nIn the previous post, the F1 score on my test set was about 0.082. This time around, without anything changed except the addition of the Uber data, the score rises above 0.09. Zindi score: 0.0897. This is better than an equivalent model did without the uber movement data, but it’s still not quite at the top - for that a little more tweaking will be needed :)\nI’m sorry that this post is shorter than the others - it was written entirely in the time I spent waiting for data to load or models to fit, and is more of a show-and-tell than a tutorial. That said, I hope that I have achieved my main goal: showing that the Uber Movement data is a VERY useful input for this challenge, and giving a hint or two about where to start playing with it.\n(PS: This model STILL ignores all of the SANRAL data. Steal these ideas and add that in, and you’re in for a treat. If you do this, please let me know? Good luck!)"
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "",
    "text": "This is the first in a multi-part series exploring Species Distribution Modelling (SDM) with the Google Earth Engine. In this post, we’ll take a look at the data we’ll be using, load up some environmental layers and create a simple linear regression model."
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#background",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#background",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "Background",
    "text": "Background\n\nGoogle Earth Engine\nGoogle Earth Engine (GEE) is an amazing tool for working with spatial data on a global scale. By writing some simple javascript, it’s possible to run computations on vast collections of image data thanks to the processing power hiding behind the scenes. Check out https://earthengine.google.com/ for more info.\n\n\nThe Tree\nThe African Baobab (Adansonia digitata and Adansonia kilima [1]) is an important tree in all countries where it is found. Besides its iconic looks, it provides tasty fruit full of good nutrients [2], bark fibre for crafts [3], traditional medicine [2], shade and an extra source of income [4] in some of the driest and most marginalized communities. Commercialization of the fruit is on the rise, especially for the export market. This is largely due to the fruit’s status as a ‘superfruit’. It’s important that organizations looking to harvest the fruit for sale have good information about the tree population so that they can pick good locations, estimate productivity and make sure that they are not over-harvesting and damaging this incredible resource.\nIn 2014, I was part of a team that set out to gather said information in Zimbabwe. We travelled all over the country, counting trees, assessing tree health, logging information about tree size and appearance and using questionnaires to find out more about how the trees were viewed and used by the communities who lived near them. This allowed us to produce a good map of the distribution within Zimbabwe, estimate potential yield in different areas and deliver a report on the overall health of the population. We also confirmed the presence of the newly discovered Adansonia kilima [5] - a second species of Baobab on mainland Africa that had only recently been described.\nFor that project, mapping the density of baobab trees was a tough task. I had to source gigabytes of data (not easy with Zimbabwe’s internet infrastructure), write some custom code to slowly crunch the numbers, tie together my own scripts with add-ons to QGIS (mapping software) and wait days for models to run. As you’ll see in the next few posts, Google Earth Engine makes the job significantly easier.\n\n\nThe data\nThere are two main types of data used in SDM. One is occurrence data - this can be points or areas where a species is known to occur. This is useful for calculating the probability of occurrence and creating maps showing where a species might be found, but less useful if you are trying to estimate density. The second type is ‘count data’ - the number of frogs in 10m2 or the total number of sightings along a transect. With count data, one can begin to predict how many of something will be found at a given location.\nThe data we collected in 2014 is count data - all the baobabs along road transects and walked transects were counted and their locations logged. The transects were subdivided into 200m by 200m plots, and each plot has an associated count - the number of baobab trees in that plot. There are 14,683 of these in the dataset, representing nearly 60 thousand hectares sampled. We could have subdivided the transects differently to get fewer, larger plots but we’ll leave that as a subject for a future post."
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#loading-input-layers",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#loading-input-layers",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "Loading input layers",
    "text": "Loading input layers\n\nEnvironmental Data\nGoogle Earth Engine has a vast amount of data available with a few clicks. We want to examine all the factors that could affect where a tree grows. You can go really deep here, but since this post is just a first rough pass we’ll grab some climate-related layers and altitude (the latter because every document on baobab mentions that it is important). You could try searching directly for things like temperature, rainfall etc, but conveniently an org [check] called Worldclim has put together 19 variables derived from climate data that they deem “biologically meaningful values”. These include temperature seasonality, max rainfall, etc. Search for ‘worldclim’ and select ‘Worldclim BIO Variables V1’, which will give you a description of the dataset and allow you to import the data. Hit ‘Import’ and give it a sensible name - it will appear at the top of your script.\n\nAdd a second import with some elevation data. Elevation data is available in up to 30m resolution, but since we’re working on a large scale and the climate data is 1km resolution, using 30m resolution elevation data is a little overkill, and will slow things down. “ETOPO1: Global 1 Arc-Minute Elevation” is a lower resolution image we can use, or you can resample the high-res layer (see part 3 of this series for examples)."
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#sampling",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#sampling",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "Sampling",
    "text": "Sampling\nWe need to create a training dataset that contains both the baobab density (from the count_data file) and the environmental layers (represented by bands in merged_image). Fortunately, GEE has a function to simplify this. We sample the image at each point:\nvar training = merged.sampleRegions(cd);\nTraining now contains a list of features. Each looks like this:\n\nWe can use this to train a model"
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#creating-and-training-the-model",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#creating-and-training-the-model",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "Creating and training the model",
    "text": "Creating and training the model\nGoogle Earth Engine provides a variety of models for us to choose from. For this post, we’ll stick to a simple linear model, available via ee.Classifier.gmoLinearRegression. We create the model, set it to regression mode (since we’re predicting density, a continuous variable) and train it with our prepared training data:\n\nThe model can now be applied to predict the density in different locations. We can use a different set of points and prepare them the way we did the training data, or we can simply apply the classifier to the whole image. The band names must match (see docs for details). Since we’ll use the merged image used for training, no further prep is needed:\nvar classified = merged.classify(trained);\nMap.addLayer(classified);\nTweaking the visualization parameters gives us our result:\n\nThe output can be saved as an asset or exported to Google Drive for later use."
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#conclusion",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#conclusion",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "Conclusion",
    "text": "Conclusion\nThere are many improvements that could be made, but this model is already very useful. Within the study area, it is fairly accurate (we’ll examine this in a future post) and it shows where baobabs can be found, and where we should expect high densities. In the next few posts, we’ll examine some better models, quantify model accuracy, map model applicability (i.e. where the model can be expected to produce useful output), experiment with different sampling techniques and so on.\nIf you have questions, please get in touch!\nYou can see a full demo script at https://code.earthengine.google.com/3635e796d66d348c2d3a152430dc1142"
  },
  {
    "objectID": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#references",
    "href": "dsc/2019-02-15-mapping-baobabs-part-1-modelling-the-density-of-baobab-trees-in-zimbabwe-with-a-linear-model-in-gee.html#references",
    "title": "Mapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE",
    "section": "References",
    "text": "References\n[1] - Pettigrew, F.R.S., Jack, D., Bell, K.L., Bhagwandin, A., Grinan, E., Jillani, N., Meyer, J., Wabuyele, E. and Vickers, C.E., 2012. Morphology, ploidy and molecular phylogenetics reveal a new diploid species from Africa in the baobab genus Adansonia (Malvaceae: Bombacoideae). Taxon, 61(6), pp.1240-1250.\n[2] - Kamatou, G.P.P., Vermaak, I. and Viljoen, A.M., 2011. An updated review of Adansonia digitata: A commercially important African tree. South African Journal of Botany, 77(4), pp.908-919.\n[3] - Rahul, J., Jain, M.K., Singh, S.P., Kamal, R.K., Naz, A., Gupta, A.K. and Mrityunjay, S.K., 2015. Adansonia digitata L.(baobab): a review of traditional information and taxonomic description. Asian Pacific Journal of Tropical Biomedicine, 5(1), pp.79-84.\n[4] - Alao, J.S., Wakawa, L.D. and Ogori, A.F., Ecology, Economic Importance and Nutritional Potentials of Adansonia digitata (BAOBAB): A Prevalent Tree Species in Northern Nigeria.\n[5] - Douie, C., Whitaker, J. and Grundy, I., 2015. Verifying the presence of the newly discovered African baobab, Adansonia kilima, in Zimbabwe through morphological analysis. South African Journal of Botany, 100, pp.164-168."
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html",
    "href": "dsc/2022-05-27-update-time.html",
    "title": "Update Time",
    "section": "",
    "text": "A few recent projects I’ve worked on have been documented elsewhere but haven’t made it to this blog. The point of this post is to summarize these so that they aren’t lost in the internet void."
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html#ai-art-course",
    "href": "dsc/2022-05-27-update-time.html#ai-art-course",
    "title": "Update Time",
    "section": "AI Art Course",
    "text": "AI Art Course\nhttps://www.youtube.com/watch?v=XGq6eWW-72E&list=PL23FjyM69j910zCdDFVWcjSIKHbSB7NE8&index=5\nThe playlist (you can also start from lesson 1)\nPart 2 of AIAIART launched last month. You can see all lessons and a link to the YouTube playlist here: https://github.com/johnowhitaker/aiaiart\n\nLesson 5 - Recap of key ideas and start of part 2: https://colab.research.google.com/drive/1cFqAHB_EQqDh0OHCIpikpQ04yzsjITXt?usp=sharing\nLesson 6 - Transformers for image synthesis and VQ-GAN revisited: https://colab.research.google.com/drive/1VhiIxMw9YClzmwamu9oiBewhZPnhmSV-?usp=sharing\nLesson 7 - Diffusion Models: https://colab.research.google.com/drive/1NFxjNI-UIR7Ku0KERmv7Yb_586vHQW43?usp=sharing\nLesson 8 - Neural Cellular Automata: https://colab.research.google.com/drive/1Qpx_4wWXoiwTRTCAP1ohpoPGwDIrp9z-?usp=sharing"
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html#image-generation-with-cloob-conditioned-latent-denoising-diffusion-gans",
    "href": "dsc/2022-05-27-update-time.html#image-generation-with-cloob-conditioned-latent-denoising-diffusion-gans",
    "title": "Update Time",
    "section": "Image Generation with CLOOB Conditioned Latent Denoising Diffusion GANs",
    "text": "Image Generation with CLOOB Conditioned Latent Denoising Diffusion GANs\n\nI had fun trying out a new(ish) approach for text-to-image tasks. The neat thing with conditioning on CLOOB embeddings is that you can train without text captions and still get some text guidance ability at inference time (see image above). This got written up as a nice report on Weights and Biases."
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html#getting-started-with-the-microsoft-rice-disease-classification-challenge",
    "href": "dsc/2022-05-27-update-time.html#getting-started-with-the-microsoft-rice-disease-classification-challenge",
    "title": "Update Time",
    "section": "Getting Started with the Microsoft Rice Disease Classification Challenge",
    "text": "Getting Started with the Microsoft Rice Disease Classification Challenge\n\nImages from the training data\nAn intro to the latest Zindi challenge with starter code and some thoughts on experiment tracking. You may see more of this at some point - for now, you can read the report here."
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html#fun-with-neural-cellular-automata",
    "href": "dsc/2022-05-27-update-time.html#fun-with-neural-cellular-automata",
    "title": "Update Time",
    "section": "Fun with Neural Cellular Automata",
    "text": "Fun with Neural Cellular Automata\nhttps://twitter.com/johnowhitaker/status/1528710004441751553?s=20&t=GifiFRsva0l4Ef_MTFMjBA\nBuilding on lesson 8 of the course, this project involved training various neural cellular automata and figuring out how to make them do tricks like taking a video as a driving signal. I’m particularly pleased with the W&B report for this - I logged interactive HTML previews of the NCAs as shaders as they train, and tracked just about everything during the experiments. I also made a Gradio demo that you can try out right now."
  },
  {
    "objectID": "dsc/2022-05-27-update-time.html#huggan-projects",
    "href": "dsc/2022-05-27-update-time.html#huggan-projects",
    "title": "Update Time",
    "section": "Huggan Projects",
    "text": "Huggan Projects\n\nSo many butterflies\nWe trained some GANs on butterflies! Have fun with the demo space. I also did a similar version with AI-generated orbs as the training data. I love how easy it is to get a demo running with HF spaces + gradio. Feels like cheating!"
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "",
    "text": "Update, July 2020: GEE now has a good random forest implementation (ee.Classifier.smileRandomForest) that can do regression - I’d suggest using that instead of the approach mentioned in this post."
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#recap-and-following-along",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#recap-and-following-along",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Recap and Following Along",
    "text": "Recap and Following Along\nThe last post looked at creating a simple linear model to predict the density of baobab trees across Zimbabwe. In this post, we’ll try to quantify how accurate the predictions are and then see if we can make them even better. Since we’ll want to try all kinds of models, we’ll take a break from Google Earth Engine and use Python (with scikit-learn) to play with some concepts before taking our final model back into GEE again.\nI’ll be working in a Jupyter notebook. This gives an interactive environment, perfect for trying out ideas and experimenting. If you’d like to follow along, I’ve uploaded the data and a complete notebook . It goes much deeper than I’m able to in blog form - consider this post a summary rather than a comprehensive explanation of the topics involved."
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#loading-the-data",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#loading-the-data",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Loading the data",
    "text": "Loading the data\nI’m using a library called pandas to load the training data (which we made to train the model in GEE) into a data structure called a DataFrame. Think of it as a spreadsheet, with columns representing the input variables (altitude, rainfall etc) and the output variable that we intend to model (in this case, tree density).\n\nLoading the data into a pandas DataFrame"
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#model-performance",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#model-performance",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Model Performance",
    "text": "Model Performance\nWe need ways of gauging model performance so that we can decide how reliable the predictions are or choose between two models. An easy way to do this is to hold back some of the training data and see how well the model performs with it. We train a model with, say, 80% of our data and then make predictions on the remaining 20%. The closer the predictions are to the true values, the better the model has done. Let’s see an example:\n\nThe data has been split into training and test sets. X represents the inputs to the model and y the desired outputs. So here, we train the model with X-train and y_train and then see how well it does on the unseen test data. But hang on, what does model.score() even do?\nThe score shown is known as the ‘R-Squared Score’. It is a measure of how well the model explains the variance in the output variable. Scores closer to 1 are better. We’ll use this going forward, but it isn’t very easy to understand (read more here). A quick way to get a more intuitive understanding of how well a model does, I like to plot the models predictions vs the actual figures. An ideal model would predict the outputs 100% correctly, resulting in a straight line (y=x). The closer to this ideal we get, the better our model is. Here we go:\n\nHmm, that’s not much like a straight line. But there is some relation - an encouraging sign. Also, the X axis (actual densities) seems to be appearing in increments of 25 - what’s up with that? Well, the data is split into very small plots (4ha each). In an area where the baobab density is 50 trees per square km, one plot might have 2 trees (giving a density of 50 trees/km^2), another might have none (density=0) and a third might have 5 (density=125). To smooth things out, we can clump adjacent plots together. This will give us fewer, larger plots, each with a better density figure. The code for this is in the accompanying notebook. Repeating the scoring process with this new input data, we get the following:\n\nBetter, and the score has improved. But still not great - for example, the model predicts a density of -100 trees/km^2 in some places. However, this gives us a starting point.\nUsing a single test/train split gives an idea of performance, but we can get more accurate scores by doing multiple splits (look up cross-validation for more info). It’s also important to think about HOW we split. Splitting randomly might be fine in some cases, but here the data was collected as we drove along roads. Having test points right next to training samples means the model can sometimes make a good guess, but we want to know how well it will perform in new areas, not just along roads we’ve sampled. A better approach is to split the data into sections - each represents a new area with different conditions, and more accurately represents the challenge. Going forward and looking at new models, I’ll record the score for both cases in CV (random) and CV (non-radom) respectively. More info in the notebook and a future post. I’ll also show scores with both the original training data and the resampled data (larger plots) for comparison.\nFinal bit in this section: let’s clear our heads by getting another measure of performance. Imaging we’ve driven 80% of the roads, and want to predict how many baobabs we’ll see on the final stretch. We’ll do it for this model (and all the following models) and compare:\n\nQuite the error!\nThe summary score for the linear model:\nRandom split: 0.169 (train), 0.152 (test)\nSequential Split: 0.172 (train), -0.549 (test)\nAnd Re-sampled plots\nRandom split: 0.257 (train), 0.213 (test)\nSequential Split: 0.262 (train), -1.119 (test)"
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#adding-polynomial-features",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#adding-polynomial-features",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Adding Polynomial Features",
    "text": "Adding Polynomial Features\nThe linear model essentially fits a set of straight lines to the data. density = a*altitude + b*temperature +… . This doesn’t work when more complicated relationships are at play. To arrive at a model that can fit more complex curves, we can add polynomial features and re-run the line fitting process. This lets us more accurately describe curves (y = 0.3x + 0.2x^2 for example). There is an excellent write-up of polynomial regression on towardsdatascience.com (which also has excellent resources on linear regression and other types of modelling with scikit-learn).\nAdding quadratic features (altitude^2 etc) gives a better R^2 score for a random split of 0.22, up from ~0.15 for Simple Linear Regression. Adding cubic features gives a further boost to 0.26. However, both models do even worse when the data is split sequentially - in other words, these models don’t generalize as well."
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#decision-trees",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#decision-trees",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Decision Trees",
    "text": "Decision Trees\nWe could keep on refining the simple models above, adding regularization parameters to help them generalize better, for example. But let’s move on and try a new kind of model - a decision tree. This post is already getting long, so I’ll leave an explanation of decision trees to someone else. Suffice to say that decision tree methods give a series of true/false splits that can be followed to get a prediction for a given set of inputs. For example, a simple (depth=2) tree predicting the baobab density looks like the following:\n\nWe can set how complex we want the decision tree to be by changing the max_depth parameter. Too simple, and we don’t account for the trends in the data. Too complex, and we ‘overfit’, reducing our model’s ability to generalize by fitting noise in our training data. We can make trees of different depth, and see how this affects the score. Observe the following two graphs:\n\nModel score with varying max_depth parameter\nMore complex models do better but are worse at generalizing. Since we don’t see much of an improvement in score for randomly split data above a depth of 10, and beyond that, the score on unseen data (when we split the data sequentially) gets significantly worse, a max depth of ~10 would be a reasonable parameter choice.\nComparing the prediction for the last 20% of the data (as we did with the first linear model), we see that this gives a much closer estimate:\n\nPrediction of the total within 15%."
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#random-forests",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#random-forests",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Random Forests",
    "text": "Random Forests\nRandom forests are a great example of the value of ensemble modelling. By creating a set of different decision trees, each of which does a mediocre job of making predictions, and then averaging the predictions to weed out extreme errors, they arrive at a more probable prediction. There is more to it than that, but let’s just try a couple out and see how they do:"
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#results",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#results",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#exporting-decision-trees-for-use-in-google-earth",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#exporting-decision-trees-for-use-in-google-earth",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Exporting Decision Trees for use in Google Earth",
    "text": "Exporting Decision Trees for use in Google Earth\nOur best model was used Random Forest Regression (which could be further improved with some extra tweaks), and this is what I’ve used previously for some Species Distribution Modelling tasks. However, Google Earth Engine doesn’t yet have support for doing regression (not classification) with random forests. A reasonable second place is Decision Trees, which have the added bonus of being computationally cheap - important when you’re working with gigabytes of data. We’ll export our best performing decision tree from python and load it using GEE’s ee.Classifier.decisionTree(), which takes in a string describing the tree.\nI wrote a function to export a tree from scikit-learn into GEE’s format. Code here  and example usage in GEE here .\n\nThe finished map doesn’t look as good as the smooth output of the linear model, but the predictions are more accurate."
  },
  {
    "objectID": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#where-next",
    "href": "dsc/2019-03-07-mapping-baobabs-part-2-qualifying-model-performance-and-more-complex-models.html#where-next",
    "title": "Mapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models",
    "section": "Where Next?",
    "text": "Where Next?\nAt this point we’ve looked at model accuracy, chosen a better model and applied that model in Google Earth Engine. We know roughly how well it does in areas similar to those we sampled. But is it reliable elsewhere? Would you trust the predicted densities for France? The model says lots of trees, but the French say il n’y a pas de baobabs and we both know who is right. To clear up these questions, we’ll spend the next post exploring the idea of model applicability, coverage of sample space and pitfalls with extrapolation. See you there!\nPS: This is still a draft but I’m hitting publish so that I can move on to the next one. I’ll refine it later. If you’ve hit a missing link or error write to me or wait a few days. Fortunately I don’t have readers yet. I hope I remember to come back to this."
  },
  {
    "objectID": "dsc/2019-03-12-mapping-baobabs-part-3-model-applicability.html",
    "href": "dsc/2019-03-12-mapping-baobabs-part-3-model-applicability.html",
    "title": "Mapping Baobabs, Part 3 - Model Applicability",
    "section": "",
    "text": "In the previous two posts, we built a Species Distribution Model and used it to predict the density of Baobab trees in Zimbabwe. Then we tried some more complex models, trained a decision tree regressor and imported it into Google Earth Engine. We showed various metrics of how well a model does, but ended with an open question: how to we tell how well the model will do when looking at a completely new area? This is the subject we’ll tackle in today’s post.\nThe key concept here is distance from the sample space. We sampled at a limited number of geographic locations, but there is no way these locations completely cover all possible combinations of temperature, rainfall, soil type and altitude. For example, all samples were at altitudes between 300 and 1300m above sea level. We might expect a model to make reasonable predictions for a new point at 500m above sea level. But what about a point at 290m elevation? Or 2000m? Or sea level? Fitting a linear model based purely on altitude, we see the problem clearly:\n\nLine of best fit: Elevation vs Density\nNegative tree densities at high altitude? Insanely high densities at sea level? Clearly, extrapolating beyond our sample space is risky. Incidentally, if it looks to you like there are two Gaussian distributions there in the data you are not alone - they might correspond to the two types* of baobabs found on mainland Africa. Until recently, conventional wisdom held that there is only one species present, and this is still contested. See a related paper I worked on here [1]. A more complex model might help, but that’s besides the point. A model’s predictions are only valid for inputs that are close enough to the training data for extrapolation to make sense.\nSo how do we deal with this? A simple approach might be to define upper and lower bounds for all input variables and to avoid making predictions outside of the range covered by our training data. We can do this in GEE using masking:\n\nBlack areas fall within 80% bounds for all variables\nThis is a reasonable approach - it stops us doing much dangerous extrapolating outside our sample space and has the added benefit of clearly conveying the limitations of the model. But we can do better. Imagine an area that is almost identical to some of our training data, but differs in a few attributes. Now further imagine that none of these attributes matter much to baobabs, and in any case they are only just below our thresholds. Surely we can expect a prediction in this area to have some value? We need a way to visualise how far away a point is from our sample space, so that we can infer how bad our predictions for that point are likely to be.\nEnter what I call the |Weighted Distance Vector|. We represent each input as a dimension. We consider how far away a point is from our sample space along each dimension, and compute the vector sum of these distances. I say the ‘weighted’ sum since we can scale the distance on each axis to reflect the relative importance of that variable, attaching higher weight to variables with larger effects on the output. Let’s clarify with an example.\nConsidering only two variables, elevation and temperature, we can represent all our training data as points (blue) on a grid where the X axis represents elevation and the y axis temperature. We’ll draw out our limits around the training data using bounds covering 90% of our data. A point within the limits has a |WDV| of 0. Now consider a point outside the limits (red). It’s 250m higher than any we’ve seen - 0.25 times the range of elevations observed. It’s 2.5 degrees cooler than any of our sampled locations, which is 0.3 times the upper-lower bounds for temperature. The distance is sqrt(0.25^2 +0.3^2) = 0.39. However, altitude has a large influence on distribution, while temperature does not. Scaling by appropriate weights (see the penultimate paragraph for where these come from) we get |WDV| = sqrt((0.173*0.25)^2 +(0.008*0.3)^2) = 0.043. The key point here is that the |WDV| captures the fact that elevation is important. A point at 750m elevation with a mean temp of 30 °C will have a low |WDV| (0.005), while one with a mean temp of 23 °C but an altitude of 1600m will have a high |WDV| (0.02).\n\nA point outside our sampled region\nTo do this in GEE is fairly simple, since we can map functions over the input images to get the |WDV| at each location. This script shows it in action. And the result gives us much more information than the mask we made earlier. Red areas have a very small |WDV|, and we expect our model to do well there. White areas are out of scope, and I’d take predictions in the yellow regions with a grain of salt. What isn’t included here is geographical distance - extrapolating to different continents, even if conditions match, is not advised.\n\n|WDV| over Southern Africa. Red areas are similar to sampled regions, white are not.\nOne thing I’ve glossed over so far: how do we get the weights used? I defined the |WDV| as weighted because we “scale the distance on each axis to reflect the relative importance of that variable.” The feature weights can be thumb-sucked by an expert (I’ve seen this done) but the easiest way to get reasonable weights is to look at the model.feature_importances_ variable of a trained random forest regressor. In the process of fitting the model, the relative importance of each input feature is computed, so we get this info for free if we’ve done the modelling as described in Part 2. Another option would be to use the correlation coefficients of each input var with the density. I leave that as an exercise for the reader.\nSo there you go - a way to visualise how applicable a model is in different locations, using weighted distance from sample space as a metric. In the next post of this series I’ll share the method I’m using to expand our sample space and get a model that can produce useful predictions over a much wider area. Before then, I’m going to take a break from baobabs and write up some other, smaller experiments I’ve been doing. See you then!\n*I’m using ‘type’ instead of ‘species’ here, because while the genetics are contentious, it is fairly clear that there are at least two kinds of baobabs here.\n[1] - Douie, C., Whitaker, J. and Grundy, I., 2015. Verifying the presence of the newly discovered African baobab, Adansonia kilima, in Zimbabwe through morphological analysis. South African Journal of Botany, 100, pp.164-168."
  },
  {
    "objectID": "dsc/2019-06-11-zindi-competition-1-making-art.html",
    "href": "dsc/2019-06-11-zindi-competition-1-making-art.html",
    "title": "Zindi Competition 1 - Making Art!",
    "section": "",
    "text": "I’m going to try entering some Zindi competitions this week. First up is the ‘AI Art’ contest. I have many crazy plans, but my nascent tensorflow skills mean everything takes time. For now, let me present my first attempt:\n\n‘Bridge over Rainbow Water’ - J Whitaker, 2019\nThis is made with a technique called Style Transfer. For more information and an easy way to try it out yourself, see the example on Google Colab. The general idea is to use a neural network to generate images that are similar to a ‘content image’ but that have the style of a separate ‘style image’. The way the style difference is quantified is by using a network trained for image recognition - the early layers in these networks tend to measure style attributes.\nNow for the specifics of this piece:\n- The general practice is to start from the content image, and slowly morph to an image that stylistically matches the style image. I turned this around, beginning with the style image and watching the structure slowly emerge.\n- I tweaked the learning rate and other parameters, trying to maintain the curving, flowing nature of the style image even as the straight lines of the bridge come forward.\n- Most styles are picked from famous artists. Since this is a co-creation with my laptop, the style image is a microscope image of my screen, which was itself displaying the microscope feed. The screen’s sub-pixels are the source of the rainbow colours.\nSome attempts that didn’t make the cut:\n\n\n\n\n\n\n\nAs you might suspect, I’ve been playing with introducing distortion into the process. Just as we perceive a work in progress through the lens of our eyes (from different angles, with non-uniform lighting), I’d like the algorithm to only see a distorted view of it’s output. This could be a blur or transform, but ultimately I’d like to try using a webcam and some wavy glass to create a means of perception for my co-artist.\nStay tuned for more attempts at music and art!"
  },
  {
    "objectID": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html",
    "href": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html",
    "title": "Christmas games - simulation and lazy stats",
    "section": "",
    "text": "This Christmas, I was introduced to several new games by my new extended family. Much fun was had making up new games and rediscovering old, but one game annoyed me slightly. A dice game that involved rolling handfuls of dice for different scores and racing to 10000 points - known to the family as ‘Farkle’ but with rules that made it closer to ‘Dice 10000’, also called ‘Zilch’. What bothered me was the fact that, despite much talk of techniques and riskiness, most players tended to follow the same basic strategy, leaving the outcome to chance. As you’ll see, the rules are just complex enough that basic stats thinking / human intuition aren’t always able to give a quick answer as to which choice is best. Anyway, having lost badly on Christmas Day I went home, thought about it for a bit and then spent an hour or two on Boxing Day coding a simulator to test some of my hypotheses. This post documents my experiments.\nHere is a basic description of the rules, adapted from http://zilch.playr.co.uk/rules.php"
  },
  {
    "objectID": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html#risky-play-as-an-advantage",
    "href": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html#risky-play-as-an-advantage",
    "title": "Christmas games - simulation and lazy stats",
    "section": "Risky play as an advantage",
    "text": "Risky play as an advantage\nI tried some other random changes, but at this point, the best average score seemed to be around 808, re-rolling with three dice if score < 500 and only keeping 5s when necessary. But, as mentioned earlier, I had a suspicion that risky play might work out when playing with larger numbers of players.\nLet’s examine just one type of risky play to investigate this. When 5 scoring dice are rolled, a player may choose to roll the single remaining dice. Since the only ways to score with one dice are 1 and 5, there is a 33% chance of success. But success means another roll with all six dice, and potentially even higher scores! So, the player is taking a chance in order to get a higher score 1/3 of the time.\nI coded up a player with this behaviour. It includes a threshold - for scores over this threshold, it won’t risk it (neither would you). Initially, this threshold was set at 500. Since it’s relatively rare to get less than 500 points while using all but one dice, the risky play doesn’t hurt the average score much - it drops to ~806. But this is where things get interesting: with three players (one baseline, one playing the best strategy found so far and one playing with this added risky behaviour), the risky player wins slightly less games then the top ‘best’ player. As one might expect given the slightly lower score. But the difference in win percentage is only 0.5%. And when we add more players, a different result emerges.\nWith 6 players playing the ‘best’ strategy and one taking risks (risking a single dice roll with scores < 700), the risky player still has a lower average score (only 803) BUT it wins more than 1/7 of the time. In other words, the risky behaviour pays off in larger groups. Here are the total wins after each player has had 3 million turns:\nwins = {‘dump5s1’: 65559, ‘dump5s2’: 64978, ‘dump5s3’: 65293, ‘dump5s4’: 65080, ‘dump5s5’: 65238, ‘dump5s6’: 65160, ‘risks1’: 66318}\nAnd the average scores:\ndump5s1 807.679317\ndump5s2 806.118700\ndump5s3 806.327633\ndump5s4 806.029383\ndump5s5 806.765667\ndump5s6 807.170067\nrisks1 802.735333\nSo, a strategy that wins in two-player mode (dump5s1 beats risks1 50.4% of the time) might not be best in larger groups."
  },
  {
    "objectID": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html#conclusion",
    "href": "dsc/2019-01-22-christmas-games-simulation-and-lazy-stats.html#conclusion",
    "title": "Christmas games - simulation and lazy stats",
    "section": "Conclusion",
    "text": "Conclusion\nI hope you’ve enjoyed this little experiment. Game theory is complex, but I hope I’ve shown how with a little bit of programming knowledge and a simple enough game you can start testing ideas and playing around in a very short amount of time.\nI scratched my itch, and the day after boxing day I followed my optimum strategy diligently and lost a string of games, much to the amusement of all. But I’m happy nonetheless. An afternoon of banging out code, testing ideas and relaxing while my computer simulates billions of dice rolls counts as a win in my book :)"
  },
  {
    "objectID": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html",
    "href": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 2)",
    "section": "",
    "text": "In Part 1, I showed a quick way to get a model that predicts cropland extent, using someone else’s model as a starting point. This was a fun exercise, but in today’s post I’d like to show a more conventional approach to achieve the same goal, and then use that to track change in land cover over time within a region."
  },
  {
    "objectID": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#training-data",
    "href": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#training-data",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 2)",
    "section": "Training Data",
    "text": "Training Data\nThis time, we’ll generate training data manually. For convenience, I’m changing the goalposts slightly: in this post, we’ll be making a simple model to distinguish between open land (fields, grassland, bare earth) and woodland. In the area of interest, this pretty much covers all the bases. Collecting data is a simple but laborious process - examples of each class are outlines in Google Earth Engine and saved as two separate FeatureCollections:\n\nSome open areas (red) and woodland (green) manually outlined for training."
  },
  {
    "objectID": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#modelling",
    "href": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#modelling",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 2)",
    "section": "Modelling",
    "text": "Modelling\nWe’ve covered modelling in GEE before, so I won’t go into details here. Sentinel 2 imagery is used, and I pretty much followed the docs to create a classifier and then apply it to the input image over the whole area. The model is fairly accurate, and a quick visual double-check confirms that it’s doing a good job of making the open areas:\n\nOpen area masked (left) vs input image (right)"
  },
  {
    "objectID": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#change-over-time",
    "href": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#change-over-time",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 2)",
    "section": "Change over time",
    "text": "Change over time\nBy choosing fields and wooded areas for training that have been present for decades, we can use the same training data to build models on imagery from different years. To track change in open land area, we can make a prediction for each year and sum the area that is classified as ‘open land’ with the following code snippet:\n\nGetting the total area classified as open land over an ROI (ward 8)\nFor my ROI, the total open land trends steadily upwards. For dates earlier than 2015, I used Lnadsat 7 imagery as the input. From 2015 to 2018, Sentinel 2 Imagery was used as well as Landsat for comparison. In some years (2010 and 2018/19) there were enough cloudy images that I combined two years for the estimate. Some of the Landsat 7 imagery isn’t the best quality, and there are some issues with this approach that mean I wouldn’t trust the figures to be incredibly accurate. BUT, we’ve accomplished our goal: the figures show the change in land cover over time:"
  },
  {
    "objectID": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#conclusion",
    "href": "dsc/2019-08-07-mapping-change-in-cropland-in-zimbabwe-part-2.html#conclusion",
    "title": "Mapping Change in Cropland in Zimbabwe (Part 2)",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this inspires you to try something like this for yourself, in an area that you’re interested in. I don’t think I’ll come back to this topic, although I’ll keep working on this project to turn it into something reliable (adding more training data, properly assessing accuracy, incorporating ground-truth data to verify etc etc). This post also marks the end of the Pioneer project mentioned here. My posting schedule will likely slow down, and you can expect some more diverse posts in the near future. Stay tuned!"
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "",
    "text": "In part 1, we laid the groundwork for our Reinforcement Learning experiments by creating a simple game (Swoggle) that we’d be trying to teach out AI to play. We also created some simple Agents that followed hard-coded rules for play, to give our AI some opponents. In this post, we’ll get to the hard part - using RL to learn to play this game."
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#the-task",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#the-task",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "The Task",
    "text": "The Task\n\nReinforcement Learning (Artist’s Depiction)\nWe want to create some sort of Agent capable of looking at the state of the game and deciding on the best move. It should be able to learn the rules and how to win by playing many games. Concretely, our agent should take in an array encoding the dice roll, the positions of the players and bases etc, and it should output one of 192 possible moves (64 squares, with two special kinds of move to give 64*3 possible actions). This agent shouldn’t just be a passive actor - it must also be able to learn from past games."
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#policy-networks",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#policy-networks",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "Policy Networks",
    "text": "Policy Networks\nIn RL, a ‘policy’ is a map from game state to action. So when we talk about ‘Policy Learners’, ‘Policy Gradients’ or ‘Policy Networks’, we’re referring to something that is able to learn a good policy over time.\n\nThe network we’ll be training\nSo how would we ‘learn’ a policy? If we had a vast archive of past games, we could treat this as a supervised learning task - feed in the game state, chosen action and eventual reward for each action in the game history to a neural network or other learning algorithm and hope that it learns what ‘good’ actions look like. Sadly, we don’t have such an archive! So, we take the following approach:\n\nStart a game (an ‘episode’)\nFeed the game state through our policy network, which initially will give random output probabilities on each possible action\nPick an action, favoring those for which the network output is high\nKeep making actions and feeding the resultant game state through the network to pick the next one, until the game ends.\nCalculate the reward. If we won, +100. If we lost, -20. Maybe an extra +0.1 for each valid move made, and some negative reward for each time we tried to break the rules.\nUpdate the network, so that it (hopefully) will better predict which moves will result in positive rewards.\nStart another game and repeat, for as long as you want.\n\nHere’s a notebook where I implement this. The code borrows a little from this implementation (with associated blog post that explains it well). Some things I changed:\n\nThe initial example (like most resources you’ll find if you look around) chooses a problem with a single action - up or down, for example. I modified the network to take in 585 inputs (the Swoggle game state representation) and give out 192 outputs for the 62*3 possible actions an agent could take. I also added the final sigmoid layer since I’ll be interpreting the outputs as probabilities.\nMany implementations either take random actions (totally random) or look at the argmax of the network output. This isn’t great in our case - random actions are quite often invalid moves, but the top output of the network might also be invalid. Instead, we sample an action from the probability distribution represented by the network output. This is like the approach Andrej Karpathy takes in his classic ‘Pong from Pixels’ post (which I highly recommend).\nThis game is dice-based (which adds randomness) and not all actions are possible at all times, so I needed to add code to handle cases where the proposed move is invalid. In those cases, we add a small negative reward and try a different action.\nThe implementation I started from used a parameter epsilon to shift from exploration (making random moves) to optimal play (picking the top network output). I removed this - by sampling from the prob. distribution, we keep our agent on it’s toes, and it always has a chance of acting randomly/unpredictably. This should make it more fun to play against, while still keeping it’s ability to play well most of the time.\n\nThis whole approach takes a little bit of time to internalize, and I’m not best placed to explain it well. Check out the aforementioned ‘Pong from Pixels’ post and google for Policy Gradients to learn more."
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#success-or-cheaty-agents",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#success-or-cheaty-agents",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "Success? Or Cheaty Agents?",
    "text": "Success? Or Cheaty Agents?\nhttps://player.vimeo.com/video/355211341?autopause=0&autoplay=0&background=1&loop=1&muted=1&playsinline=1&transparent=1\nOpenAI’s glitch-finding players (source: https://openai.com/blog/emergent-tool-use/)\nEarly on, I seemed to have hit upon an excellent strategy. Within a few games, my Agent was winning nearly 50% of games against the basic game AI (for a four player game, anything above 25% is great!). Digging a little deeper, I found my mistake. If the agent proposed a move that was invalid, it stayed where it was while the other agents moved around. This let it ‘camp’ on it’s base, or wait for a good dice roll before swoggling another base. I was able to get a similar win-rate with the following algorithm:\n\nPick a random move\nIf it’s valid, make the move. If not, stay put (not always a valid action but I gave the agent control of the board!)\n\nThat’s it - that’s the ‘CheatyAgent’ algorithm :) Fortunately, I’m not the first to have flaws in my game engine exploited by RL agents - check out the clip from OpenAI above!\n\nAnother bug: See where I wrote sr.dice() instead of dice_roll? This let the network re-roll if it proposed an invalid move, which could lead to artificially high performance.\nAfter a few more sneaky attempts by the AI to get around my rules, I finally got a setup that forced the AI to play by the rules, make valid moves and generally behave like a good and proper Swoggler should."
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#winning-for-real",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#winning-for-real",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "Winning for real",
    "text": "Winning for real\n\nLearning to win!!!\nWith the bugs ironed out, I could start tweaking rewards and training the network! It took a few goes, but I was able to find a setup that let the agent learn to play in a remarkably short time. After a few thousand games, we end up with a network that can win against three BasicAgents about 40-45% of the time! I used the trained network to pick moves in 4000 games, and it won 1856 of them, confirming it’s superiority to the BasicAgents, who hung their heads in shame."
  },
  {
    "objectID": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#so-much-more-to-try",
    "href": "dsc/2020-01-24-swoggle-part-2-building-a-policy-network-with-pytorch-dealing-with-cheaty-agents-and-beating-the-game.html#so-much-more-to-try",
    "title": "Swoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game",
    "section": "So much more to try",
    "text": "So much more to try\nI’ve still got plenty to play around with. The network still tries to propose lots of invalid moves. Tweaking the rewards can change this (note the orange curve below that tracks ratio of valid:invalid moves) but at the cost of diverting the network from the true goal: winning games!\n\nLearning to make valid moves, but at the cost of winning.\nThat said, I’m happy enough with the current state of things to share this blog. Give it a go yourself! I’ll probably keep playing with this, but unless I find something super interesting, there probably won’t be a part 3 in this series. Thanks for coming along on my RL journey :)"
  },
  {
    "objectID": "dsc/2022-03-29-sketchy-unet.html",
    "href": "dsc/2022-03-29-sketchy-unet.html",
    "title": "Sketchy Unet",
    "section": "",
    "text": "The model demo running on Huggingface Spaces\nI wanted a fast way to go from an image to something like a rough charcoal sketch. This would be the first step in a longer pipeline that would later add detail and colour, so all it has to do is give a starting point with the right sort of proportions."
  },
  {
    "objectID": "dsc/2022-03-29-sketchy-unet.html#finding-a-dataset",
    "href": "dsc/2022-03-29-sketchy-unet.html#finding-a-dataset",
    "title": "Sketchy Unet",
    "section": "Finding a dataset",
    "text": "Finding a dataset\nI found a small dataset that seemed like a good starting point (originally created in ‘APDrawingGAN: Generating Artistic Portrait Drawings From Face Photos With Hierarchical GANs’ by Ran Yi, Yong-Jin Liu, Yu-Kun Lai, Paul L. Rosin). It’s quick to download, and (with a little datablock wrangling) easy enough to load with fastai. See the notebook for details."
  },
  {
    "objectID": "dsc/2022-03-29-sketchy-unet.html#training-the-model",
    "href": "dsc/2022-03-29-sketchy-unet.html#training-the-model",
    "title": "Sketchy Unet",
    "section": "Training the model",
    "text": "Training the model\nI chose to model this as an image-to-image task, and used fastai’s unet_learner function to create a U-net style network based on a Resnet34 backbone. Starting with 128px images and then moving up to 224px, the model is trained to minimise the MSE between the output and the reference sketch. In about 3 minutes (!!) we end up with a model that is doing pretty much exactly what I want:\n\nImages (left), artist’s sketch (center), model outputs (right)"
  },
  {
    "objectID": "dsc/2022-03-29-sketchy-unet.html#sharing-a-demo",
    "href": "dsc/2022-03-29-sketchy-unet.html#sharing-a-demo",
    "title": "Sketchy Unet",
    "section": "Sharing a Demo",
    "text": "Sharing a Demo\nI’ve been playing around with HuggingFace Spaces recently, and this model was a great candidate for a simple demo that should run reasonably fast even on a CPU (like those provided by Spaces). At the end of the training notebook you can see the gradio interface code. Very user-friendly for these quick demos! The trained model was uploaded to huggingface as well, and they somehow detected that my code was downloading it because it shows up as a ‘linked model’ from the space.\nIt’s neat that I can so easily share everything related to a mini-project like this for others to follow along. The colab notebook provides a free cloud environment to replicate training, the model is hosted by someone with lots of bandwidth and is easy to download, and the demo needs no technical skills and lets anyone try it out in seconds. Hooray for fastai, gradio, huggingface and so many others who work so hard to make our lives easy :)"
  },
  {
    "objectID": "dsc/2022-03-29-sketchy-unet.html#update-whats-this-for",
    "href": "dsc/2022-03-29-sketchy-unet.html#update-whats-this-for",
    "title": "Sketchy Unet",
    "section": "Update: What’s this for?",
    "text": "Update: What’s this for?\n\nWaterface demo: https://huggingface.co/spaces/johnowhitaker/waterface\nI used this model to ‘sketchify’ images before loading them into an imstack and optimising that to match a CLOOB prompt like ‘A charcoal and watercolor sketch of a person’. After a few steps the result looks pretty OR more likely a little creepy. Ah, the power of AI :) Try it out here."
  },
  {
    "objectID": "dsc/2020-08-12-personal-metrics.html",
    "href": "dsc/2020-08-12-personal-metrics.html",
    "title": "Personal Metrics",
    "section": "",
    "text": "This is just a quick post following on from some recent conversations in this area. tldr: Tracking some data about yourself is a great exercise, and I highly recommend it. In this post I’ll share a few of the tools I use, and dig around in my own data to see if there are any interesting insights…."
  },
  {
    "objectID": "dsc/2020-08-12-personal-metrics.html#time-tracker-toggl",
    "href": "dsc/2020-08-12-personal-metrics.html#time-tracker-toggl",
    "title": "Personal Metrics",
    "section": "Time Tracker: Toggl",
    "text": "Time Tracker: Toggl\nThe first source of data is my time tracker: toggl. It’s simple to use, and has a web app as well as a good android app. As a consultant, this is useful for billing etc, but it has also just become a general habit to log what I’m working on. It’s good motivation not to context-switch, and it’s a great way to keep track of what I’m up to. A good day of work can sometimes mean 4 hours on the clock, since I tend not to log small tasks or admin, but it’s still good enough that I’ll bill clients based on the hours logged. Toggle let you do some reporting within the app, but you can also export the data to CSV for later analysis. Here’s my last two years, total seconds per month:\n\nTime logged per month (as of August 12)\nAs you can see, I’ve been busier than normal the past few months - one of the reasons this blog hasn’t had any new posts for a while!"
  },
  {
    "objectID": "dsc/2020-08-12-personal-metrics.html#daily-mood-and-activities-daylio",
    "href": "dsc/2020-08-12-personal-metrics.html#daily-mood-and-activities-daylio",
    "title": "Personal Metrics",
    "section": "Daily mood and activities: daylio",
    "text": "Daily mood and activities: daylio\nDaylio is a smartphone app that asks ‘How was your day?’ every day, and optionally let’s you log activities for the day. I’ve made it a habit, although tracking stopped for a few months at the start of the pandemic :/ One thing I like about this (and the previous thing I used, https://year-in-pixels.glitch.me/) is that it forces you to evaluate how you’re feeling. Was today great, or merely good? Why was it ‘Meh’? And by quantifying something less concrete than simply hours worked, it let’s me see what I can do to optimize for generally better days.\n\nTime worked on days marked as Average, Good or Great\nMondays are my lowest day, followed by Wednesdays. Being outdoors bumps my rating from ~4 (good) to nearly 4.5 (5 being ‘great’). As you can see in the image above, lots of work tends to mean not-so-great days. Around 3 hours per day logged (4-6 hours work) is where I start properly having fun, and if I can fit in activities like birding or something creative then it’s even closer to optimum. I’m in a pretty good place now despite the busyness - the average score (~4.3) is much higher than when I was still in uni trying to balance work and assignments (3.3). It’s nice to see this - on tougher days it’s amazing to look back and see how many good or great ones there are, and how lovely life is overall."
  },
  {
    "objectID": "dsc/2020-08-12-personal-metrics.html#moar-data-ulogme",
    "href": "dsc/2020-08-12-personal-metrics.html#moar-data-ulogme",
    "title": "Personal Metrics",
    "section": "Moar data: uLogMe",
    "text": "Moar data: uLogMe\nI recently found a project called uLogMe (by Karpathy oif all people), and after reading his post about it I decided to give it a go. If you’re keen to try it, look for a fork on HitHub as the original project is deprecated. I only use the logging scripts, which keep track of active window title and number of keystrokes in each 9s window. This is really fun data, as you can identify different activities, find patterns, see trends in when you’re most active… As one example, look at a fairly typical day from last month:\n\nKeystroke intensity over time\nYou can see me start a little late, since it’s winter. After an initial burst of work I went on a long walk looking for insects (there was a bioblitz on) before hacking away during my 10am meeting. There are spikes of activity and periods of very little (meetings) or no (breaks) activity. 6-8pm is my class time, so I’m tapping away in demos as I teach, especially in the second half of the lesson.\nCheck out Karpathy’s post to see what else it’s possible to do with this data."
  },
  {
    "objectID": "dsc/2020-08-12-personal-metrics.html#putting-it-all-together",
    "href": "dsc/2020-08-12-personal-metrics.html#putting-it-all-together",
    "title": "Personal Metrics",
    "section": "Putting it all together",
    "text": "Putting it all together\nI can’t wait to get a fitness tracker to add sleep tracking, exercise and heart rate. But even without those, I have some really great data to be playing with. I can see relationships between external factors (travel, activities, work) and my mood, explore how much time goes into different projects, graph the number of characters being typed in different applications (spoiler: I use Jupyter a LOT) and generally put some hard numbers behind my intuition around how I’m spending my time and how that’s affecting me.\n\nA small subset of the data now waiting to be analysed\nI hope that this post marks a return to this blog for me (hours are trending downwards now that some courses I teach are wrapping up) and that it inspires you to find some personal data to track! If you still aren’t convinced, here’s a TED talk that might push you over the edge. Happy hacking :)"
  },
  {
    "objectID": "dsc/2019-07-08-data-glimpse-nighttime-lights.html",
    "href": "dsc/2019-07-08-data-glimpse-nighttime-lights.html",
    "title": "Data Glimpse: Nighttime Lights",
    "section": "",
    "text": "This ‘Data Glimpse’ post will look at the Global Radiance-Calibrated Nighttime Lights dataset [1], available through Google Earth Engine. However, the method shown here can be used with any Raster data source. To avoid repetition, I’ll refer back to this post any time I aggregate raster data over a shapefile.\n\nThe Data\nThe dataset is a collection of images from different years showing nighttime lights all over the globe. This information can be used to see where people are [2] and estimate measures such as economic activity in an area [3]. They have been used in some great research estimating the Global Human Footprint and highlighting the last wild places on earth [4].\n\nNighttime lights displayed in GEE\nEach image contains two bands: ‘avg_vis’, which is the measure of illumination, and ‘cf_cvg’ describing cloud cover (used as a data quality metric).\n\n\nAggregating the Data by Region\nInstead of a large raster image, we might want to aggregate the data by region. For example, we might want to look at how the amount of light visible at night in National Parks has changed over time. To get the data in the form that we want, we first need to define the regions that we’re interested in. This script that I made to illustrate the idea uses a landuse map of Zimbabwe as an example, but one could just as easily use Country outlines or draw a region with the ‘Draw a shape’ tool in GEE.\nWith the input region(s) defined, the key step is to use the reduceRegions function to add properties to each feature (area) that summarize the underlying raster. For example, with an image of nighttime illumination in the year 2000 called ‘lights_2000’ and the landuses map, we can add the mean illumination in each area with var landuse_with_lights = lights_2000.reduceRegions(landuses, ee.Reducer.mean());. The result can be exported as a shapefile or CSV file (see the script for details) and displayed or analyses in whatever software you like.\n\nAverage nighttime illumination over Zimbabwe\n\n\nChange over Time\nOne of the nice things about this dataset is that it contains values for several different years. I took a look at the data from 2000 and 2010, with the goal of seeing if protected areas (forest lands, national parks etc) had seen an increase in nighttime lights (an indicator that people are moving into these areas). Most protected areas in Zimbabwe had almost no nighttime lights recorded, and those that did show (on average) a drop in the amount of nighttime lights (2010 values are ~20% lower than those for 2000). In the few places where lights had increased, the increase seems to be due to safari camps rather than encroachment from neighboring districts. The data can’t tell the whole story, and poor coverage plus the relative dimness of firelight might mean that some encroachment is missed, but it was encouraging to see that the wilderness areas are still largely dark and empty - just the way they should be.\n\n\nReferences\n[1] - https://developers.google.com/earth-engine/datasets/catalog/NOAA_DMSP-OLS_CALIBRATED_LIGHTS_V4\n[2] - Elvidge, C.D., Imhoff, M.L., Baugh, K.E., Hobson, V.R., Nelson, I., Safran, J., Dietz, J.B. and Tuttle, B.T., 2001. Night-time lights of the world: 1994–1995. ISPRS Journal of Photogrammetry and Remote Sensing, 56(2), pp.81-99. [3] - Wu, J., Wang, Z., Li, W. and Peng, J., 2013. Exploring factors affecting the relationship between light consumption and GDP based on DMSP/OLS nighttime satellite imagery. Remote Sensing of Environment, 134, pp.111-119. [4] - Sanderson, E.W., Jaiteh, M., Levy, M.A., Redford, K.H., Wannebo, A.V. and Woolmer, G., 2002. The human footprint and the last of the wild: the human footprint is a global map of human influence on the land surface, which suggests that human beings are stewards of nature, whether we like it or not. BioScience, 52(10), pp.891-904."
  },
  {
    "objectID": "dsc/2019-07-07-data-glimpse-south-africas-hydrological-data.html",
    "href": "dsc/2019-07-07-data-glimpse-south-africas-hydrological-data.html",
    "title": "Data Glimpse: South Africa’s Hydrological Data",
    "section": "",
    "text": "South Africa’s Department of Water Affairs (DWA) makes all kinds of data publicly available through their data portal: http://www.dwa.gov.za/hydrology/. The download interface is a little clunky, but simple once you get the hang of it. This short post will take a look at some typical data, and list some of the ways this could be used in the future.\n\nThe DWA website, after selecting ‘Verified data’.\nMost of the data comes from monitoring stations, each of which is assigned a unique ID. The easiest way to find stations in your area of interest is via the ‘Station Catalogue’ link visible in the above screenshot. Stations are typically a depth measure in a dam or river.\nWith a station chosen, the next step is to specify the date range and type of data you’d like to download. The available dates and information are listed in the Station Catalog. I picked a station in the Pongola river system, and saved the data file generated by the website as ‘daily_flows.txt’. This is a text file with variables separated by whitespace, and can be loaded into a pandas dataframe for analysis as follows:\n\nLoading the data.\nWith the data thus loaded, it’s fairly easy to pot the flow over a given year, or calculate monthly averages. Here’s a plot showing the daily flow rate out of Jozini dam in 2018. Note that the graph has many flat areas - this is because this is a managed flow, with the amount of water released from the dam regulated by local authorities (somewhat badly, in this case [2]).\n\nA plot of the daily flow rate.\nA notebook showing more plots and an example of re-sampling for yearly averages is available here.\nSo what can you do with this data? Here are some ideas (let me know if you’d like to see any as future posts):\n- Get dam level data for dams all over South Africa and and animate the levels over time, to illustrate the recent drought and the (alarming) longer trend.\n- Use the data to learn hydrodynamic modelling (see [1])\n- Combine with rain data to see how watershed capture has changed with agriculture and land use change\n- Look for the change in river flows after new projects (dams, diversions and so on)\nI hope you’ve enjoyed this brief glimpse at some fun data. Please let me know if you do something with this, or if you have some data that you’d like featured.\nReferences:\n[1] - Birkhead, A.L., Brown, C.A., Joubert, A.R., Singh, A. and Tlou, T., 2018. The Pongola Floodplain, South Africa–Part 1: Two-dimensional hydrodynamic modelling in support of an environmental flows assessment. Water SA, 44(4), pp.731-745.\n[2] - Lanyi, Shira. 2018. “Paradise Lost: The Struggle to Preserve the Pongola River and its Inhabitants.” Open Rivers: Rethinking Water, Place & Community, no. 11. http://editions.lib.umn.edu/openrivers/article/paradise-lost/."
  },
  {
    "objectID": "dsc/2019-01-22-curious-correlations.html",
    "href": "dsc/2019-01-22-curious-correlations.html",
    "title": "Curious correlations",
    "section": "",
    "text": "I wanted to write this up to show how easy it is becoming to test ideas and find interesting trends in data. Please don’t draw too many conclusions from the numbers here - pinch of salt and all that.\nYesterday I came across the Wellcome Trust Data Re-Use Prize: Maleria. They have made tons of data available, and invited participants to generate a new insight, tool or health application from that data. Incredible to see such great initiatives.\nBrowsing through the data, one map in particular drew my attention - the ‘Residual Means’. These “show the remaining (residual) transmission that has not been accounted for by the covariates already in the model.” Doesn’t that smell juicy?\nExplaining this unattributed transmission is one of the example questions provided. It would be neat to see if we can figure out why malaria infection rates are higher than expected in some areas, and lower in others.\nI was looking at all this as I procrastinated some work I’m doing mapping baobab trees. It occurred to me that it wouldn’t be completely absurd to see if there is any relation between the two subjects. Now you’ll just have to take my word on this for now, but rest assured that I have a decently accurate map of baobab tree density for Zimbabwe and surrounds. I quickly downloaded the residuals maps and fired up QGIS to take a look.\n\nThis isn’t the density map I used, but it is similar and looks prettier\nEstimating correlation by looking at two maps and saying “there seems to be some patterns here” is not an established scientific practice, but it is fun to see the brains pattern-matching functions get abused. After a few minutes the fun wore off and I got down to the serious business. I want to see if there is a correlation between baobab density (or rather, access to baobab fruit) and malerial transmission/infection.\nStackoverflow “get raster value at point” since it’s been a while. Wow - I don’t even have the gdal toolbox on this laptop yet! Technical hurdles out of the way, I threw together some code:\n\nFull code listing on Github \nCreating regularly spaced points over the area of interest (i.e. the area I have baobab densities for), I use the above code to sample the baobab density and the transmission residual at each point. Next, we check to see if they’re correlated:\nscipy.stats.pearsonr(densities, maleria_residuals) yields a correlation coefficient of -0.1226401351031383, p=0. That is, places with more baobabs have less unattributed transmission than places without. To show this visually, let’s look at a scatter plot of the two variables:\n\nScatter plot - unattributed transmission vs baobab density\nPlaces with high baobab density have inexplicably low transmission rates, in general. In fact, 86% of locations with estimated baobab density >10 trees/hectare had a negative ‘unattributed transmission’ value.\nAt this point, my half-hour break should have ended, but I was interested. I had mainly done asked the question as an exercise in seeing how easy it was to play with the data. But there was a correlation (note: correlation != causation). Now it could well be that baobab trees and malaria transmission are both dependent on some of the same environmental factors, some of which might not have been taken into account by the model. But could it be the case that this wonderful tree (I’m a little biased) might be doing some good?\nBaobab fruit is good for you [1]. It’s got lots of minerals and vitamins, and my initial hunch was that maybe, just maybe, it could be boosting the health of any community who lives close to the trees. Another angle came up when I looked for sources for [1] and found references to the use of baobab in traditional medicine as a treatment for malaria [2, 3]. Now curious, I looked around and found a study [4] suggesting “that Adansonia digitata protects against Plasmodium berghei induced-malaria, and that administration of the extract after established infection reduced malaria progression.” (in mice - from the [4]).\nTo sum up, we’ve looked at the malaria data and found that there are some variations in the transmission rates that the current models can’t explain. We’ve then examined the relationship between baobab tree density and malaria transmission residuals and noted that there is a small negative correlation. We’ve seen that areas with baobabs present tend to have lower transmission rates than expected, and presented the idea that this could be due to the health benefits of the fruit or the anti-malarial properties of the bark, which is often used in traditional medicine. All done, thank you very much, can I has my PhD yet?\nScience isn’t quite that easy. I share this story to show how rapidly you can start generating hypotheses and playing with data. But to give a rigorous answer will take a little more than an hour coding and an hour smugly writing a blog post. I can think of a few reasons why the results here should be taken with a large pinch of salt, and I leave it as an exercise for the reader to list a whole bunch more. Hopefully soon I’ll have time for a follow-up, doing it properly and explaining how one should actually go about it.\nFor now, cheers\nReferences\n[1] - I had some sources, but it’s more entertaining if you google ‘baobab superfruit’ and then ignore the most enthusiastic 90% of results. But see [2] for some good info (available online at https://www.sciencedirect.com/science/article/pii/S222116911530174X#bib4)\n[2] - Rahul, J., Jain, M.K., Singh, S.P., Kamal, R.K., Naz, A., Gupta, A.K. and Mrityunjay, S.K., 2015. Adansonia digitata L.(baobab): a review of traditional information and taxonomic description. Asian Pacific Journal of Tropical Biomedicine, 5(1), pp.79-84.\n[3] - Kamatou, G.P.P., Vermaak, I. and Viljoen, A.M., 2011. An updated review of Adansonia digitata: A commercially important African tree. South African Journal of Botany, 77(4), pp.908-919.\n[4] - Adeoye, A.O. and Bewaji, C.O., 2018. Chemopreventive and remediation effect of Adansonia digitata L. Baobab (Bombacaceae) stem bark extracts in mouse model malaria. Journal of ethnopharmacology, 210, pp.31-38."
  },
  {
    "objectID": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html",
    "href": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html",
    "title": "How Predictable: Evaluating Song Lyrics with Language Models",
    "section": "",
    "text": "I was briefly nerd-sniped this morning by the following tweet:\nhttps://twitter.com/unixpickle/status/1584761450979299329?s=20&t=TTgENBNO4pb7c1Ar2R7AJg\nCan we quantify how ‘predictable’ a set of lyrics are?"
  },
  {
    "objectID": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#language-models-and-token-probabilities",
    "href": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#language-models-and-token-probabilities",
    "title": "How Predictable: Evaluating Song Lyrics with Language Models",
    "section": "Language Models and Token Probabilities",
    "text": "Language Models and Token Probabilities\nA language model is a neural network trained to predict the next token in a sequence. Specifically, given an input sequence it outputs a probability for each token in its vocabulary. So, given the phrase “Today is a nice” the model outputs one value for every token, and we can look up the probability associated with the token for “day” - which will likely be fairly high (~0.5 in my tests).\nWe can look at the probabilities predicted for each successive word in a set of lyrics, and take the average as a measure of ‘predictability’. Here’s the full code I used:\nimport torch\nfrom transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\ngpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\", return_dict_in_generate=True)\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\nlyrics = \"\"\"\n    And my thoughts stay runnin', runnin' (Runnin')\n    The heartbreaks keep comin', comin' (Comin')\n    Oh, somebody tell me that I'll be okay\n\"\"\"\ninput_ids = tokenizer(lyrics, return_tensors=\"pt\").input_ids\nword_probs = []\nmin_length = 5 # How much do we give to start with\n\nfor i in range(min_length, len(input_ids[0])-1):\n    ids = input_ids[:,:i]\n    with torch.no_grad():\n        generated_outputs = gpt2.generate(ids[:,:-1], do_sample=True, output_scores=True,\n                                          max_new_tokens=1,\n                                          pad_token_id=tokenizer.eos_token_id)\n    scores = generated_outputs.scores[0]\n    probs = scores.softmax(-1)\n    word_probs.append(probs[0][ids[0][-1]])\n\ntorch.mean(torch.tensor(word_probs))\nMy starting point was this post by Patrick Von Platen showing how to generate probabilities per token with GPT-2."
  },
  {
    "objectID": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#results",
    "href": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#results",
    "title": "How Predictable: Evaluating Song Lyrics with Language Models",
    "section": "Results",
    "text": "Results\nThe first test: ‘Remind Me’ by Megan Trainor. The mean probability given by the model for the next word given the lyrics up to that point: 0.58!\nTrying a few other songs I could think of with less repetitive lyrics:\n\n‘Levitate’ (21 Pilots): 0.34\n‘Mom’s Spaghetti’ (MNM): 0.35\nThe code example above: 0.45\nI’m Gonna Be (500 Miles)’ (The Proclaimers): 0.59\n\nThere is a caveat worth making which is that anything written before 2019 might be in the model’s training data, and so it might ‘know’ the lyrics already making the measure less informative."
  },
  {
    "objectID": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#historical-trends",
    "href": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#historical-trends",
    "title": "How Predictable: Evaluating Song Lyrics with Language Models",
    "section": "Historical Trends",
    "text": "Historical Trends\nEDIT: Someone (me) didn’t preview their data well enough, the lyrics I used for this were either badly scraped or very processed, so these scores won’t compare well to the previous section and I need to re-do this with a proper dataset before we can say anything concrete about trends!\n\nPlotting the median estimated predictability per decade for a random sample of ~6k songs\nI downloaded a bunch of song lyrics via this dataset and sampled some from different years (1950 - 2019). For each, I estimated the predictability as described above. I found very little correlation (correlation coefficient 0.037 EDIT: 0.06 with a larger sample size) between predictability and year released, but there does seem to be a slight uptick in median predictability over time, especially going into the 2010s, which I’m sure will validate those grumbling about ‘music these days’…"
  },
  {
    "objectID": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#conclusion",
    "href": "dsc/2022-10-25-how-predictable-evaluating-song-lyrics-with-language-models.html#conclusion",
    "title": "How Predictable: Evaluating Song Lyrics with Language Models",
    "section": "Conclusion",
    "text": "Conclusion\nThis was fun! Go play with the code and see if your least favourite song is actually as predictable as you think it is. Or perhaps run it over the top 100 current hits and see which is best. I should get back to work now, but I hope you’ve enjoyed this little diversion :)"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "",
    "text": "Last week a guy called Evan Miller tweeted out a blog post claiming to have discovered a flaw in the attention mechanism used by transformers today:\nhttps://twitter.com/EvMill/status/1683508861762695168?s=20\nThe phrasing was sensationalist, and many people were dismissive of the idea. Evan hadn’t run any experiments, and it turned out that his proposed fix was already implemented in PyTorch as a (typically unused) option in the standard Multi-Headed Attention implementation. Surely this was something that would already be in use if it was actually useful? But, since the suggested change was pretty simple, I figured I’d try it out for myself. And that in turn led to a fun little research adventure, in which some internet randos may just have found something impactful :) Let me explain…"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#the-problem",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#the-problem",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "The Problem",
    "text": "The Problem\nNeural Networks like transformers are stored as big piles of numbers (parameters) that are applied in different mathematical calculations to process some input. Each parameter is represented inside the computer by some number of 1s and 0s. If you use more of these bits per parameter (say, 32) you can represent the numbers with a lot of precision. But if you use fewer (say, 8) the model takes up less storage space, more parameters can be kept in RAM, and the calculations could potentially be faster. So, using fewer bits per number - quantization - is a hot topic at the moment for anyone concerned with running big models as cheaply as possible.\nThe problem arises when you try to go from a high-precision 32-bit neural network to an 8-bit one. With 8 bits you can only represent 2^8 (256) different numbers. If most of your numbers are small, then you can use those 256 numbers to represent, say, a range of values from -1 to 1 and map your 32-bit floating point numbers to the nearest 8-bit approximation without too much loss in accuracy. However, if there is an occasional *outlier* in your set of numbers then you may need to represent a much larger range (say, -100 to 100) which in turn leaves far fewer options for all those small values close to 0, and results in much lower accuracy.\n\nFigure from Time Dettmer’s blog post showing the drop in performance with quantization after outliers emerge\nTim’s blog post explains this extremely well. And in bad news for quantization fans, it turns out that outliers do indeed occur in these models especially as you scale up, leading to major drops in performance after quantization unless you do lots of extra work to address the issue. For example, you can identify groups of parameters that contain most of the outliers and keep these in higher precision (say, 16-bit) while still quantizing the other 99.9% of the network parameters down to 8 bit or less. Still, this is extra work and imposes a performance penalty. If only there were ways to avoid these outliers from occurring…"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#existing-fixes",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#existing-fixes",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "Existing Fixes",
    "text": "Existing Fixes\nSome researchers at Qualcom (who have a keen interest in making LLMs runnable at the edge) explored this problem in depth and proposed two clever solutions. It was their paper that sparked this whole thing. To summarize their findings:\n\nMany outliers occur as attention heads try to learn a ‘no-op’ where they don’t modify the residual. They do this by creating a larger and larger input to the softmax, pushing the rest of the values closer to 0 (but thanks to the softmax formulation they never get all the way to 0).\nOne fix is to scale and clip the softmax output such that it can saturate and go completely to 0, blocking gradients and preventing the values from growing further. This is the method they call clipped softmax.\nAnother option is to add some additional parameters for a learnable gating function, which can control whether the attention output is added in or not. This lets the network learn another way to achieve the so-called ‘no-op’. They call this gated attention.\nBoth of their approaches do dramatically reduce the presence of outliers (which they show by measuring the max magnitude of the activations as well as the ‘kurtosis’) and the resulting transformers perform almost as well quantized as they do in full precision, unlike the baseline without their proposed fixes.\n\n\nTable 2 from the paper showing results. The max activation size (inf. norm) and kurtosis are much lower with their fixes, and the performance after quantization (W8A8 column, i.e. weights and activations are both 8-bit) is close to that of the model before quantization, unlike the baseline (vanilla) case.\nThis paper is nice in that it gives a very concrete way to think about the problem and to measure how well a particular solution solves it. If your model has less outliers (measured via inf norm and kurtosis) and still performs well after quantization, you’re on the right track!"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#evans-suggestion",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#evans-suggestion",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "Evan’s suggestion",
    "text": "Evan’s suggestion\nBoth of the methods above have potential downsides. Clipping can mess with gradients, and gating requires additional parameters. Evan, with his Physics and Economics background, believes there is a simpler fix: modify the softmax operation itself. Softmax is often used in a ‘choose between options’ scenario, and gets thrown in a lot whenever ML people want a convenient function whose outputs sum to 1 (great when you want probabilities). In the case of attention, this is not necessarily a desirable property!\n\nEvan’s proposed ‘Softmax1’ formula\nEvan’s suggestion is to effectively add an extra logit that is always set to 0, which means that if the rest go negative the final outputs can all approach zero without the sum needing to be 1. In most cases this function will behave very close to vanilla softmax, but it gives the network an out for when it doesn’t want to have any high outputs. In practice this is implemented by adding 1 to the denominator in the softmax formula. See his post for more details."
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#testing-it-out",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#testing-it-out",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "Testing it out",
    "text": "Testing it out\nSince the change is so small, I cast about for a transformer implementation I could quickly adapt to try this. Karpathy’s nanoGPT is perfect for this kind of thing. Written to facilitate easy learning, a single model.py file to mess with, it was the perfect starting point for experimentation. It turns out I was not the only one to think this, and soon I was in touch with Thomas Capelle (who was using the llama2.c repository that tweaks the nanoGPT code to implement the more recent LlaMa architecture) and with ‘HypnoPump17’ on Discord who trained two gpt2-small sized models using nanoGPT which were larger than the mini ones I’d been messing with and formed a good starting point for trying to measure the effects of interest.\n\nVisualizing the activations from an attention layer (notebook). You can see that some dimensions (the 768 dims are spread along the x axis) have outliers present.\nMy first experiments were on very small models (15M parameters). It’s usually good to start small so you can iterate quickly, but in this case this backfired and my results were inconclusive. As noted in Tim’s blog and subsequent work, outliers only start to emerge above 100M parameters and reach some sort of critical threshold only in even larger models (5b+). Luckily, swapping in the 125M parameter models from HypnoPump was and easy change and my quick exploratory notebook showed a marked improvement in inf. norm and kurtosis for the modified softmax, in line with what the Qualcomm authors had observed with their fixes."
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#the-magic-of-enthusiastic-collaborators-and-outsider-insight",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#the-magic-of-enthusiastic-collaborators-and-outsider-insight",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "The magic of enthusiastic collaborators and outsider insight",
    "text": "The magic of enthusiastic collaborators and outsider insight\nWhile it remains to be seen how impactful this is (see next section), the thing that has really stood out to me so far in this little experiment is how great community research can be. One person with a unique educational background spots a potential new idea, another implements it, a third runs some training runs, a forth analyses the weights, a fifth sets up instrumentation with W&B to track stats during training, someone else starts organising time on a cluster to run some bigger experiments… the productivity of a Discord channel full of enthusiastic hackers is quite something to behold!"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#where-we-are-and-what-comes-next",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#where-we-are-and-what-comes-next",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "Where we are and what comes next?",
    "text": "Where we are and what comes next?\nAmid all this fun the paper authors have been in touch, and have run their own tests of the softmax1 approach, finding that it seems to work about as well as their other proposed fixes. Of course, there’s a lot of work to be done between ‘this maybe works in a quick test’ and something being accepted by the larger community as a technique worth adopting. I expect the next stage involves some larger training runs and more thorough evaluation, hopefully resulting in a paper that presents enough evidence to show the teams currently working on the next generation of LLMs that this is worth paying attention to ;)"
  },
  {
    "objectID": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#conclusions",
    "href": "dsc/2023-08-04-exploring-softmax1-or-community-research-for-the-win.html#conclusions",
    "title": "Exploring Softmax1, or “Community Research For The Win!”",
    "section": "Conclusions",
    "text": "Conclusions\nThis blog post isn’t about the results - we’ll have reports and papers and all that from other people soon enough. At the moment this still has a good chance of ending up in the large and ever-growing bucket of “proposed changes to transformers that never ended up going anywhere”. The reason I’ve written this is instead to share and praise the process, in which open science (teams sharing research on Arxiv), diverse perspectives (Evan writing his post, misc twitter experts chiming in), great tools (Karpathy’s amazing didactic repos, easy experiment tracking and weight sharing) and an active community of hobby researchers all come together to deepen our collective understanding of this magical technology."
  },
  {
    "objectID": "tils.html",
    "href": "tils.html",
    "title": "TILs",
    "section": "",
    "text": "Exporting from Wordpress into a Quarto Blog\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "DataScienceCastnet",
    "section": "",
    "text": "Exploring Softmax1, or “Community Research For The Win!”\n\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy and how I’m shifting focus to LLMs\n\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nA Recipe for Training Good Generative Models\n\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nDistilHN: Summarizing News Articles with Transformers\n\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nHow Predictable: Evaluating Song Lyrics with Language Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2022\n\n\n\n\n\n\n  \n\n\n\n\nUpdate Time\n\n\n\n\n\n\n\n\n\n\n\n\nMay 27, 2022\n\n\n\n\n\n\n  \n\n\n\n\nFine-tuning a CLOOB-Conditioned Latent Diffusion Model on WikiArt\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\nSketchy Unet\n\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nTurtle Recall: A Contrastive Learning Approach\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2022\n\n\n\n\n\n\n  \n\n\n\n\nAIAIART Course Retrospective\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2021\n\n\n\n\n\n\n  \n\n\n\n\nPlaying with Tweet Sentiment Analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\nWhistleGen: Generating Traditional Irish music with ML\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nIn Brief: Playing with Class Imbalance\n\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2021\n\n\n\n\n\n\n  \n\n\n\n\nBirdClef Entry: Bird Call Classification with FastAI\n\n\n\n\n\n\n\n\n\n\n\n\nApr 22, 2021\n\n\n\n\n\n\n  \n\n\n\n\nLanguage Models for Protein Sequence Classification\n\n\n\n\n\n\n\n\n\n\n\n\nOct 30, 2020\n\n\n\n\n\n\n  \n\n\n\n\nData Glimpse: Predicted Historical Air Quality for African Cities\n\n\n\n\n\n\n\n\n\n\n\n\nSep 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\nPersonal Metrics\n\n\n\n\n\n\n\n\n\n\n\n\nAug 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdate: Why the pause?\n\n\n\n\n\n\n\n\n\n\n\n\nJun 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSelf-Supervised Learning with Image网\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2020\n\n\n\n\n\n\n  \n\n\n\n\nMeta ‘Data Glimpse’ - Google Dataset Search\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSwoggle Part 2 - Building a Policy Network with PyTorch, dealing with Cheaty Agents and ‘Beating’ the Game\n\n\n\n\n\n\n\n\n\n\n\n\nJan 24, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSwoggle Part 1- RL Environments and Literate Programming with NBDev\n\n\n\n\n\n\n\n\n\n\n\n\nJan 20, 2020\n\n\n\n\n\n\n  \n\n\n\n\nBehind the scenes of a Zindi Contest\n\n\n\n\n\n\n\n\n\n\n\n\nJan 16, 2020\n\n\n\n\n\n\n  \n\n\n\n\nSnapshot Serengeti - Working with Large Image Datasets\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2019\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning + Remote Sensing - Using NNs to turn imagery into meaningful features\n\n\n\n\n\n\n\n\n\n\n\n\nNov 12, 2019\n\n\n\n\n\n\n  \n\n\n\n\nZindi UberCT Part 3: Uber Movement\n\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2019\n\n\n\n\n\n\n  \n\n\n\n\nZindi UberCT Part 2: Stepping Up\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2019\n\n\n\n\n\n\n  \n\n\n\n\nZindi UberCT Part 1: Getting started\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2019\n\n\n\n\n\n\n  \n\n\n\n\nPackaging a classification model as a web app\n\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2019\n\n\n\n\n\n\n  \n\n\n\n\nPothole Detection (aka Johno tries fastai)\n\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2019\n\n\n\n\n\n\n  \n\n\n\n\nTrying Automated ML\n\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMapping Change in Cropland in Zimbabwe (Part 2)\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMapping Change in Cropland in Zimbabwe (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nJul 14, 2019\n\n\n\n\n\n\n  \n\n\n\n\nData Glimpse: Cropland and Settlement maps from QED.AI\n\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2019\n\n\n\n\n\n\n  \n\n\n\n\nData Glimpse: Nighttime Lights\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2019\n\n\n\n\n\n\n  \n\n\n\n\nData Glimpse: South Africa’s Hydrological Data\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nData Glimpse: Visualizing Economic Activity with the G-Econ Project data\n\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2019\n\n\n\n\n\n\n  \n\n\n\n\nTutorial: Improving Crop Type Predictions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2019\n\n\n\n\n\n\n  \n\n\n\n\nTutorial: Predicting Crop Types with GEE\n\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPioneer Tournament has Begun!\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2019\n\n\n\n\n\n\n  \n\n\n\n\nNew Database: Forest Change in Different Regions\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2019\n\n\n\n\n\n\n  \n\n\n\n\nZindi Competition 2 - Trying CatBoost on the Traffic Jam Challenge\n\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n  \n\n\n\n\nZindi Competition 1 - Making Art!\n\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2019\n\n\n\n\n\n\n  \n\n\n\n\nLooking at traffic/congestion vs air quality AKA a quest for data\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nML and IR Tomography\n\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMapping Baobabs, Part 3 - Model Applicability\n\n\n\n\n\n\n\n\n\n\n\n\nMar 12, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMapping Baobabs, Part 2 - Qualifying Model Performance and More Complex Models\n\n\n\n\n\n\n\n\n\n\n\n\nMar 7, 2019\n\n\n\n\n\n\n  \n\n\n\n\nMapping Baobabs, Part 1 - Modelling the Density of Baobab Trees in Zimbabwe with a Linear Model in GEE\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2019\n\n\n\n\n\n\n  \n\n\n\n\nChristmas games - simulation and lazy stats\n\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\n\n\n\n\n  \n\n\n\n\nCurious correlations\n\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninit(self): What is this blog\n\n\n\n\n\n\n\n\n\n\n\n\nJan 11, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tils/2023-08-11-convert_wordpress_to_quarto.html",
    "href": "tils/2023-08-11-convert_wordpress_to_quarto.html",
    "title": "johnowhitaker.dev",
    "section": "",
    "text": "Exporting from Wordpress into a Quarto Blog\nI have been blogging for years on a free Wordpress account, but figured I should finally migrate to a custom setup. I’m using Quarto, and in this TIL I document how to export your blogs from Wordpress to Quarto. The whole process took about 20 minutes, thanks to a note I left for myself last time I tried the process :) The steps are:\n\nExport XML from wordpress. I used the standard process, Tools -> Export -> Export All (https://wordpress.org/support/article/tools-export-screen/) to get an XML file that contains all my posts etc.\nConvert the XML export to markdown. I used https://github.com/lonekorean/wordpress-export-to-markdown. I ran ‘npx wordpress-export-to-markdown’ in the folder containing the export, following the prompts to create files with the right date format. I chose not to place them in separate folders.\nCopy-paste the files into a folder in your quarto blog\n\nTo actually have them show up in the blog you need to edit a couple of other files. Here’s what blog.qmd looks like:\n---\ntitle: \"DataScienceCastnet\"\nlisting:\n  contents: dsc\n  sort: \"date desc\"\n  type: default\n  categories: False\n---\nAnd then in the main _quarto.yml we add it like so:\nwebsite:\n  title: \"johnowhitaker.dev\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - href: blog.qmd\n        text: Data Science Castnet Blog\n        \nThat’s it - hooray for the TIL format that means this counts as a post :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jonathan Whitaker",
    "section": "",
    "text": "Johno’s Personal Website (under construction)\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]